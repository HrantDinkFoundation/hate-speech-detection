{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wFXTGNTSP-K0"},"outputs":[],"source":["from google.colab import files\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","root_path = 'gdrive/My Drive/'\n","\n","import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","! nvcc --version\n","!nvidia-smi -L\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAAU3hinHKRQ"},"outputs":[],"source":["!pip install -U transformers\n","#!pip install transformers==4.28.0\n","!pip install transformers==4.45.2\n","!pip install datasets\n","!pip install snscrape\n","!pip install --upgrade accelerate\n","!pip install datasets evaluate\n","!pip install accelerate -U\n","\n","from scipy.spatial.distance import cosine\n","from transformers import AutoModel, AutoTokenizer\n","import numpy as np\n","import random\n","import csv\n","import pandas as pd\n","from sklearn.model_selection import StratifiedKFold\n","import tensorflow as tf\n","import datasets\n","from datasets import Dataset, DatasetDict\n","import evaluate\n","from sklearn.metrics import classification_report, f1_score\n","\n","import sys, os\n","import torch\n","import json\n","import re\n"]},{"cell_type":"code","source":["# preprocess functions\n","\n","def remove_url(text):\n","    return ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\",\" \", text).split())\n","\n","def remove_username(text):\n","    return ' '.join(re.sub(\"([@][A-Za-z0-9_]+)\",\" \", text).split())\n","\n","def remove_at_mark(text):\n","    return re.sub(r'[@]', ' ', text)\n","\n","def remove_tag_mark(text):\n","    return re.sub(r'[#]', ' ', text)\n","\n","def replace_hashtags_with_segments (text):\n","   regex = \"#(\\w+)\"\n","   text_split = text.split()\n","   sent= ''\n","   strip_word =' '\n","   for word in text_split:\n","     hashtag= re.findall(regex, word)\n","     #print(hashtag)\n","     #print(strip_word)\n","\n","     if (len(hashtag)) == 0 :\n","        sent = sent + ' ' + word\n","     if (len(hashtag))!=0:\n","        strip_word = word.replace(('#'+hashtag[0]), ' ')\n","        #print (hashtag[0])\n","        #print(strip_word)\n","        try :\n","           key = '#'+ hashtag[0]\n","           segmented = segmented_dict [key]\n","           if type(segmented) == str:\n","                    #segmented = re.sub(r'[|]', ' ', segmented)\n","                    #segmented = re.sub(r'[,]', ' ', segmented)\n","                    sent= sent + ' ' + segmented\n","           if type(segmented) == list:\n","                    for k in segmented:\n","                      sent = sent +' ' + k\n","        except:\n","           sent = sent + ' ' +  hashtag[0]\n","   return sent + ' ' + strip_word\n","\n","def control_tag_mark(text):\n","     s = ' '\n","     for word in text.split():\n","            word = re.sub(r'[#]', ' #', word)\n","            s = s + ' ' + word\n","     return s\n","\n","def remove_punctuation_marks(text):\n","   punc = '''!()-[]{};:'\"\\,<>./?$%^&*~'''\n","   for i in text:\n","       if i in punc:\n","         text = text.replace(i, \" \")\n","   return text\n","\n","\n","\n","def remove_underline(text):\n","  punc = '''_'''\n","  for i in text:\n","       if i in punc:\n","         text = text.replace(i, \"\")\n","  return text\n","\n","def remove_hashtag(text):\n","    return ' '.join(re.sub(\"([#][A-Za-z0-9_]+)\",\" \", text).split())\n","\n","def remove_turkish_hashtag(text):\n","    return ' '.join(re.sub(\"([#][\\w+_]+)\",\" \", text).split())\n","\n","def find_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    return re.findall(emoji_pattern, text) # no emoji\n","\n","def remove_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    return (emoji_pattern.sub(r'', text)) # no emoji\n","\n","def remove_arabic_hashtags(text):\n","   return ' '.join(re.sub('([#][\\u0600-\\u06ff_]+)', \" \", text).split())\n","\n","def remove_emoji_duplicate (text):\n","      emoji =[]\n","      rest = ''\n","      main_text = []\n","      for w in text :\n","         x = find_emoji(w)\n","         if len (x) != 0 :\n","            emoji.append (x[0])\n","      #print(emoji)\n","      rest = remove_emoji(text)\n","      emoji = list(set(emoji))\n","      for e in emoji :\n","        try:\n","          rest = rest + \" \" + e\n","        except:\n","          rest = rest\n","      main_text.append(rest)\n","      return main_text[0]\n","\n","def lower_case(text):\n","    text = text.replace(\"I\", \"Ä±\")\n","    text = text.replace(\"Ä°\", \"i\")\n","    return text.lower()"],"metadata":{"id":"9vyLbRzuVICq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def seed_everything(seed):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","seed_everything(0)"],"metadata":{"id":"pAAVod3AVjod"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define data path\n","# Please replace the addresses with the address of dataset on your local drive\n","\n","path_turkish_data     =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/df_turkish_cats_targets_degrees_final_with_translated_all.csv'\n","path_arabic_data      =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/df_arabic_cats_targets_degrees_final.csv'\n","path_synthetic_data   =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/synthetic_data_all_new.xlsx'\n","\n","path_all_emoji =         '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/all_emoji_dict.json'\n","path_hashtag_segment  =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/all_hashtag_segmented_tr.json'"],"metadata":{"id":"MXbwhXVtk8M2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Laod emoji_dict and hashtag_dict\n","\n","f = open(path_all_emoji)\n","data = json.load(f)\n","emoji_list = data['emojis']\n","\n","f = open(path_hashtag_segment)\n","segmented_dict = json.load(f)"],"metadata":{"id":"Gn-0qMMmVyjS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select which preprocess steps must be apply on data\n","\n","def pre_process_tweets(text_list):\n","   cleaned_text =[]\n","   for s in text_list:\n","       #s = lower_case(s)  #Umut\n","       s = s.lower()\n","       s = remove_username(s)\n","       s = remove_at_mark(s)\n","       s = remove_url(s)\n","       s = remove_tag_mark(s)\n","       #s = control_tag_mark(s)  #\n","       s = remove_punctuation_marks(s)\n","       s = remove_underline(s)  #\n","       #s = replace_hashtags_with_segments(s) #\n","       #s = remove_tag_mark(s)\n","       cleaned_text.append(s)\n","   return cleaned_text"],"metadata":{"id":"GbINt_v7XOKE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Load synthetic data\n","\n","df_syn = pd.read_excel(path_synthetic_data)\n","df_syn = df_syn.iloc[:,[4, 10, 9,]]\n","\n","print(df_syn['DegreeMean'].value_counts())\n","df_syn= df_syn.sample(frac = 1)\n","\n","df_syn"],"metadata":{"id":"3ZlH3j_avBwC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load Turkish data\n","\n","df_turkish = pd.read_csv(path_turkish_data)\n","df_turkish = df_turkish.iloc[:,[2, 37,38, 39]]\n","\n","df_turkish"],"metadata":{"id":"y3HBA7XJvwx4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Load Arabic data\n","\n","df_arabic = pd.read_csv(path_arabic_data)\n","df_arabic = df_arabic.iloc[:,[2,37,38]]\n","\n","df_arabic"],"metadata":{"id":"qOSLlHxvw1J1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# select number of classes\n","# select k-fold for spliting train and validation data\n","\n","num_class = 1\n","k_fold = 11\n"],"metadata":{"id":"N0-_R31GqByY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select strength column\n","# Define the name and the address of trained model on HuggingFace\n","\n","if num_class == 1:\n","     df_turkish.dropna(subset=['DegreeMean'], inplace=True)\n","     df_arabic.dropna(subset=['DegreeMean'], inplace=True)\n","     hub_id = 'HrantDinkFoundation/'\n","     model_id = 'hs-degree-prediction'\n","     column_select = 'DegreeMean'"],"metadata":{"id":"XUT08gwueOnI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# select which data used in training\n","\n","# 'only-tr' : use only Turkish data\n","# 'only-ar' : use only arabic data\n","# 'tr-syn'  : use Turkish and synthetic data\n","# 'ar-syn'  : use arabic and synthetic data\n","# 'ar-translated-syn'  : use arabic, synthetic and the translated column of Turkish data\n","\n","mode_train = ['only-tr', 'only-ar', 'tr-syn' , 'ar-syn', 'ar-translated-syn']\n","mode = mode_train [2]"],"metadata":{"id":"RV_pZFVMm6M1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if mode =='only-tr':\n","    model_lang = 'tr'\n","    model_name = 'dbmdz/bert-base-turkish-uncased'\n","    df_train = df_turkish[(df_turkish.Div=='train')]\n","    df_test  = df_turkish[(df_turkish.Div=='test')]\n","\n","if mode =='only-ar' :\n","     model_lang = 'ar'\n","     model_name = 'aubmindlab/bert-base-arabert'\n","     df_train = df_arabic[(df_arabic.Div=='train')]\n","     df_test  = df_arabic[(df_arabic.Div=='test')]\n","\n","\n","if mode == 'tr-syn':\n","    model_lang = 'tr'\n","    hub_id = hub_id + 'turkish-' + model_id\n","    model_name = 'dbmdz/bert-base-turkish-uncased'\n","    df_train = df_turkish[(df_turkish.Div=='train')]\n","    df_test  = df_turkish[(df_turkish.Div=='test')]\n","\n","    df_train = pd.concat([df_train, df_syn], ignore_index=True)\n","\n","\n","if mode == 'ar-syn':\n","    model_lang = 'ar'\n","    hub_id = hub_id + 'arabic-' + model_id\n","    model_name = 'aubmindlab/bert-base-arabert'\n","    df_train = df_arabic[(df_arabic.Div=='train')]\n","    df_test  = df_arabic[(df_arabic.Div=='test')]\n","\n","    df_syn = df_syn.drop (columns=['Text'])\n","    df_syn.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","    df_train = pd.concat([df_train, df_syn], ignore_index=True)\n","\n","\n","if mode == 'ar-translated-syn':\n","    model_lang = 'ar'\n","    model_name = 'aubmindlab/bert-base-arabert'\n","    hub_id = hub_id + 'arabic-' + model_id\n","\n","    df_turkish = df_turkish.drop (columns=['Text'])\n","    df_turkish.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","\n","    df_train_arabic = df_arabic[(df_arabic.Div=='train')]\n","    df_test  = df_arabic[(df_arabic.Div=='test')]\n","\n","    df_syn = df_syn.drop (columns=['Text'])\n","    df_syn.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","\n","    df_train = pd.concat([df_train_arabic, df_turkish, df_syn], ignore_index=True)\n"],"metadata":{"id":"Z-pbvvDE1ZeJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train\n"],"metadata":{"id":"5OtGrwB0qgsx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test"],"metadata":{"id":"_hAfQvHmneZv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pgeiz9nGnKQB"},"outputs":[],"source":["# preprocess train data\n","\n","import ast\n","\n","text = df_train['Text'].tolist()\n","label = df_train[column_select].tolist()\n","\n","text = pre_process_tweets(text)\n","train_data = pd.DataFrame(list(zip(text, label)), columns =['text', 'label'])\n","\n","train_data= train_data.astype({'label':'float'})\n","\n","train_data\n"]},{"cell_type":"code","source":["# preprocess test data\n","\n","text = df_test['Text'].tolist()\n","label = df_test[column_select].tolist()\n","\n","text = pre_process_tweets(text)\n","test_data = pd.DataFrame(list(zip(text, label)), columns =['text', 'label'])\n","test_data= test_data.astype({'label':'float'})\n","\n","\n","test_data\n"],"metadata":{"id":"yoX6nmbsj0wB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Laod BERT model\n","\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from transformers import ElectraForSequenceClassification,  ElectraTokenizerFast\n","from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaTokenizerFast, RobertaForSequenceClassification\n","from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments, GPT2Config\n","\n","pretrained_model_name = model_name\n","tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, do_lower_case=True, force_download=True)\n","model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name, num_labels = num_class, force_download=True ).to(device)   # id2label=id2label, label2id=label2id\n","model.resize_token_embeddings(len(tokenizer))\n","for item in emoji_list :\n","        tokenizer.add_tokens(item)\n","model.resize_token_embeddings(len(tokenizer))\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"],"metadata":{"id":"Mjkcw1fDWToG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vh9FzUR427JW"},"outputs":[],"source":["def preprocess_function(examples):\n","    preprocessed = examples[\"text\"]\n","    return tokenizer(preprocessed, truncation=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZfw_lai35A4"},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import r2_score\n","\n","def compute_metrics(eval_pred):\n","\n","    metric4 = evaluate.load(\"accuracy\")\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    accuracy = metric4.compute(predictions=predictions, references=labels)[\"accuracy\"]\n","    macro_f1 = (f1_score(labels, predictions, average =\"macro\"))\n","\n","    return {\"accuracy\":accuracy, \"macro_f1\":macro_f1}\n","\n","\n","def compute_metrics(eval_pred):\n","    metric1 = evaluate.load(\"mse\")\n","    logits, labels = eval_pred\n","    mse = metric1.compute(predictions=logits, references=labels)[\"mse\"]\n","    return {\"mse\": mse}\n","\n","\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    labels = labels.reshape(-1, 1)\n","\n","    mse = mean_squared_error(labels, logits)\n","    mae = mean_absolute_error(labels, logits)\n","    r2 = r2_score(labels, logits)\n","    single_squared_errors = ((logits - labels).flatten()**2).tolist()\n","\n","    # Compute accuracy\n","    # Based on the fact that the rounded score = true score only if |single_squared_errors| < 0.5\n","    accuracy = sum([1 for e in single_squared_errors if e < 0.25]) / len(single_squared_errors)\n","\n","    return {\"mse\": mse, \"mae\": mae, \"r2\": r2, \"accuracy\": accuracy}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ntZERAEqvNwt"},"outputs":[],"source":["skf = StratifiedKFold(n_splits= k_fold, random_state = 31, shuffle=True)"]},{"cell_type":"code","source":["from torch import nn\n","from transformers import Trainer\n","\n","class CustomTrainer(Trainer):\n","    def save_model(self, output_dir=None, _internal_call=False):\n","        if output_dir is None:\n","            output_dir = self.args.output_dir\n","\n","        self.model = self.model.to('cuda')\n","\n","        for param in self.model.parameters():\n","            if not param.is_contiguous():\n","                param.data = param.data.contiguous()\n","\n","        super().save_model(output_dir, _internal_call)\n","\n","\n","\n","class RegressionTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.get('logits')\n","        loss_fct = nn.MSELoss()\n","        loss = loss_fct(logits.squeeze(), labels.squeeze())\n","        return (loss, outputs) if return_outputs else loss"],"metadata":{"id":"pdYaTy8NoQi4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define training and validation data\n","val_ratio = 0.09\n","\n","inds = np.random.permutation(train_data.shape[0])\n","ind_split = int(train_data.shape[0]*val_ratio)\n","\n","val_data = train_data.loc[inds[:ind_split]].sort_index().copy()\n","train_data2 = train_data.loc[inds[ind_split:]].sort_index().copy()\n","\n","assert val_data.shape[0] + train_data2.shape[0] == train_data.shape[0]\n","train_data2.head(20)\n"],"metadata":{"id":"LriV06xhyP6u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's start training model\n","\n","val = Dataset.from_pandas(val_data.set_index(\"text\"))\n","train = Dataset.from_pandas(train_data2.set_index(\"text\"))\n","\n","dataset_dict = DatasetDict({\"val\": val, \"train\": train})\n","preprocessed_dataset_dict = dataset_dict.map(preprocess_function, batched=True)\n","\n","training_args = TrainingArguments(\n","        \"Hate Speech Strength Prediction\",\n","        learning_rate=5e-6,\n","        per_device_train_batch_size=16,\n","        per_device_eval_batch_size=20,\n","        num_train_epochs=10,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"steps\",\n","        metric_for_best_model = 'accuracy',\n","        logging_steps = 100,\n","        load_best_model_at_end=True,\n","        greater_is_better=True,\n","        #hub_strategy=\"end\",           # These parameter used when you want saved mmodels directly to HuggingFace,\n","        #push_to_hub=True,             # If you want to save model on huggingFace you must select \"True\"\n","        #hub_model_id= hub_id,         # Write your model address on HuggingFace\n","        #hub_private_repo= False,      # Determine whether the model on Hugging Face should be private or public.\n","        #hub_token= \"...........\",     # To save model into HuggingFace you need \"hub_token\" which can take from your own HuggingFace account\n",")\n","\n","trainer = RegressionTrainer(\n","        model = model,\n","        args = training_args,\n","        train_dataset=preprocessed_dataset_dict[\"train\"],\n","        eval_dataset=preprocessed_dataset_dict[\"val\"],\n","        tokenizer = tokenizer,\n","        data_collator = data_collator,\n","        compute_metrics = compute_metrics,\n","\n",")\n","trainer.train()\n","\n","#trainer.save_model()       # If you want save your trained model on HuggingFace"],"metadata":{"id":"giMXzUWDyWQb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSHmt9lFDzt9"},"outputs":[],"source":["# Let's start testing model\n","\n","test_ds = Dataset.from_pandas(test_data).map(preprocess_function, batched=True)\n","test_ds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2O_oLx5c-vc-"},"outputs":[],"source":["predictions = trainer.predict(test_ds).predictions\n","predictions"]},{"cell_type":"code","source":["test_data['Predicted-HS-Severity'] = predictions.reshape(-1)\n","test_data"],"metadata":{"id":"W-1rYQpawEdR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data.loc[test_data['Predicted-HS-Severity'] < 0, 'Predicted-HS-Severity'] = 0\n","test_data.loc[test_data['Predicted-HS-Severity'] < 0]\n","\n","test_data['Rounded-HS-Severity'] = test_data['Predicted-HS-Severity'].round()\n","\n","test_data"],"metadata":{"id":"zxBXeDpxwSHS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import mean_squared_error\n","from math import sqrt\n","from scipy.stats import pearsonr, spearmanr\n","\n","\n","MSE_simple =  mean_squared_error(test_data['label'].tolist(), test_data['Predicted-HS-Severity'].tolist())\n","MSE_rounded = mean_squared_error(test_data['label'].tolist(), test_data['Rounded-HS-Severity'].tolist())\n","\n","\n","RMSE_simple = sqrt(mean_squared_error(test_data['label'].tolist(), test_data['Predicted-HS-Severity'].tolist()))\n","RMSE_rounded = sqrt(mean_squared_error(test_data['label'].tolist(), test_data['Rounded-HS-Severity'].tolist()))\n","\n","correlation = test_data['label'].corr(test_data['Predicted-HS-Severity'])\n","\n","\n","print(\"MSE_simple:\", MSE_simple )\n","print(\"MSE_rounded:\", MSE_rounded )\n","print(\" \")\n","print(\"RMSE_simple:\", RMSE_simple )\n","print(\"RMSE_rounded:\", RMSE_rounded )\n","print(\" \")\n","print(\"Pearson correlation coefficient:\", correlation)"],"metadata":{"id":"tFSqE7PIwSuf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save trained model on local drive\n","# Please replace the pathes with your local address path\n","\n","\n","if model_lang == 'tr':\n","          path_save_model = '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/trained_models/hs-tr-strength/'\n","\n","if model_lang == 'ar':\n","          path_save_model = '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/trained_models/hs-ar-strength/'\n","\n","\n","trainer.save_model(path_save_model)"],"metadata":{"id":"QbcM6TVepXtn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load trained model from local drive\n","\n","from transformers import pipeline\n","\n","path_trained_model = path_save_model\n","model_pipline = pipeline(task = \"text-classification\", model = path_trained_model, function_to_apply=\"none\" , device=\"cuda\")\n"],"metadata":{"id":"2iMsNqe0bxzI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# predict the strength of example teewts using trained model\n","\n","def predict_strength_tweet(review):\n","        prediction = []\n","        prediction = (model_pipline (review))\n","        prediction = prediction[0][\"score\"]\n","        return prediction"],"metadata":{"id":"dD_qKQROhBJV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ðŸ”´ â–¶ **Disclaimer:**\n","\n","This study (project) includes examples of offensive language, hate speech, and profanity due to the nature of the research. These examples do not represent the authorsâ€™ views (or developers' views). The goal of this work is to contribute to the detection and prevention of harmful content and violence directed toward refugees, LGBTI+ individuals, religious groups, nationalities, and other vulnerable communities."],"metadata":{"id":"G0Brsr5Jjo-z"}},{"cell_type":"code","source":["AR_TWEETS_EXAMPLES = [\n","    \"ÙƒÙ„ Ø§Ù„Ù„Ø§Ø¬Ø¦ÙŠÙ† Ù„ØµÙˆØµ\",\n","    \"Ø§Ø±Ø­Ù„ÙˆØ§ Ù…Ù† Ø¨Ù„Ø¯Ù†Ø§\",\n","    \"Ù…ÙƒØ§Ù† Ø§Ù„Ù…Ø±Ø£Ø© Ù‡Ùˆ Ø§Ù„Ù…Ø·Ø¨Ø®\",\n","    \"Ø§Ù„ÙŠÙˆÙ†Ø§Ù† ÙŠØ¬Ø¨ Ø£Ù† ØªØ¹Ø±Ù Ø­Ø¯ÙˆØ¯Ù‡Ø§ØŒ ÙˆØ¥Ù„Ø§ Ù‚Ø¯ Ù†Ø£ØªÙŠ ÙØ¬Ø£Ø© ÙÙŠ Ù„ÙŠÙ„Ø©\",\n","    \"Ù„Ø§ ÙŠÙ†Ø¨ØºÙŠ Ù„ØºÙŠØ± Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† Ø£Ù† ÙŠØµØ¨Ø­ÙˆØ§ Ù†ÙˆØ§Ø¨Ù‹Ø§.\",\n","    \"ÙŠØ¬Ø¨ Ø£Ù„Ø§ ÙŠÙØ³Ù…Ø­ Ù„Ù„Ø§Ø¬Ø¦ÙŠÙ† Ø¨Ø§Ù„ØªØµÙˆÙŠØª.\",\n","    \"ÙŠØ¬Ø¨ Ø£Ù„Ø§ ÙŠØ³ØªÙÙŠØ¯ Ø§Ù„Ù„Ø§Ø¬Ø¦ÙˆÙ† Ù…Ù† Ø§Ù„ØªØ£Ù…ÙŠÙ† Ø§Ù„ØµØ­ÙŠ.\",\n","    \"Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ±Ø¯ÙŠØ© ØºÙŠØ± Ù…Ù‚Ø¨ÙˆÙ„.\",\n","    \"Ù„Ù„Ø§Ø¬Ø¦ÙŠÙ† Ø£ÙŠØ¶Ù‹Ø§ Ø§Ù„Ø­Ù‚ ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø©.\",\n","    \"ÙŠØªÙ… Ø§ØªØ®Ø§Ø° ØªØ¯Ø§Ø¨ÙŠØ± Ø´Ø§Ù…Ù„Ø© Ø¶Ù…Ù† Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù„Ù…Ù†Ø¹ Ø£Ù†Ø´Ø·Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ…Ø§Øª Ø§Ù„Ø¥Ø±Ù‡Ø§Ø¨ÙŠØ©.\",\n","    \"ØªÙ… Ø§Ù„Ø¥Ø¹Ù„Ø§Ù† Ø¹Ù† ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„ÙØ¹Ø§Ù„ÙŠØ§Øª Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ© Ø§Ù„ØªÙŠ Ø³ØªÙÙ‚Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ø¹Ø§Ù… ÙÙŠ Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„.\",\n","    \"ØªÙØªØªØ­ Ø±Ø§Ø¦Ø¯Ø§Øª Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ ØµØ§Ù„ÙˆÙ†Ø§Øª ØªØ¬Ù…ÙŠÙ„ ØªØ³ØªÙ‡Ø¯Ù Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ù…Ù† Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† ÙˆØºÙŠØ± Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ†.\",\n","    \"ØªÙØ¬Ø±Ù‰ Ø§Ù„Ø§Ø³ØªØ¹Ø¯Ø§Ø¯Ø§Øª ÙÙŠ Ø§Ù„Ø£Ø³ÙˆØ§Ù‚ ÙÙŠ Ø³ÙˆØ±ÙŠØ§ Ù‚Ø¨Ù„ Ø§Ù„Ø£Ø¹ÙŠØ§Ø¯ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© ÙˆØ§Ù„Ù…Ø³ÙŠØ­ÙŠØ©.\",\n","    \"Ù‡Ù„ ÙƒÙˆÙ† Ø¨Ø¹Ø¶ Ø§Ù„ÙƒÙ„Ø§Ø¨ Ù…Ø³Ø¹ÙˆØ±Ø© Ø£Ùˆ Ø¹Ø¯ÙˆØ§Ù†ÙŠØ© ÙŠØ¨Ø±Ø± Ù‚ØªÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ø§Ø¨ØŸ Ø§Ø³Ø­Ø¨ÙˆØ§ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†.\",\n","     \"ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø³Ø§Ø¡ Ø¯Ø¹Ù… Ø¨Ø¹Ø¶Ù‡Ù† Ø§Ù„Ø¨Ø¹Ø¶\",\n","    \"Ø§Ø´ØªØ±ÙŠØª ØªÙØ§Ø­Ù‹Ø§ Ù…Ù† Ø§Ù„Ø³ÙˆÙ‚.\",\n","    \"Ø·Ø¹Ø§Ù… Ø§Ù„Ø³ÙˆØ±ÙŠÙŠÙ† Ù„Ø°ÙŠØ° Ø¬Ø¯Ù‹Ø§.\",\n","    \"Ø§Ù„Ø·Ø¹Ø§Ù… Ø§Ù„Ø³ÙˆØ±ÙŠ Ø´Ù‡ÙŠ Ù„Ù„ØºØ§ÙŠØ©.\",\n","    \"Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ø³ÙˆØ±ÙŠÙŠÙ† ÙƒØ«ÙŠØ±Ù‹Ø§ØŒ Ø¥Ù†Ù‡Ù… Ø£Ø´Ø®Ø§Øµ Ù„Ø·ÙŠÙÙˆÙ† Ø¬Ø¯Ù‹Ø§.\",\n","    \"Ø§Ù„Ø³Ù†ØºØ§ÙÙˆØ±ÙŠÙˆÙ† ÙŠØ¹Ù…Ù„ÙˆÙ† Ø¨Ø¬Ø¯.\",\n","    \"Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠÙŠÙ† ÙƒØ«ÙŠØ±Ù‹Ø§\",\n","    \"Ø£Ù†Ø§ Ù„Ø§ Ø£Ø­Ø¨ Ø§Ù„Ù…Ù„Ø­Ø¯ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚\",\n","    \"Ø§Ù„Ù…Ø³ÙŠØ­ÙŠÙŠÙ† Ø§Ù„Ø£Ø±Ø«ÙˆØ°ÙƒØ³\",\n","    \"Ø³Ù†ØºØ§ÙÙˆØ±Ø© Ø¨Ù„Ø¯ Ø¬Ù…ÙŠÙ„ Ø¬Ø¯Ù‹Ø§\"   ,\n","    \"Ø°Ù‡Ø¨Øª Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ Ø£Ù…Ø³ØŒ Ø«Ù… ØªÙ†Ø§ÙˆÙ„Øª ÙˆØ¬Ø¨Ø© Ø·Ø¹Ø§Ù… Ù„Ø°ÙŠØ°Ø©ØŒ Ø«Ù… Ø¹Ø¯Øª Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù†Ø²Ù„ØŒ ÙˆÙ„Ø¹Ø¨Øª Ù…Ø¹ Ø§Ø¨Ù†ØªÙŠØŒ ÙˆØ´Ø§Ù‡Ø¯Øª Ù…Ø³Ù„Ø³Ù„Ù‹Ø§ Ø£Ù…Ø±ÙŠÙƒÙŠÙ‹Ø§\",\n","    \"Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙƒØ«ÙŠØ±Ù‹Ø§\"\n","]\n","\n","TR_TWEETS_EXAMPLES = [\n","    \"mÃ¼ltecilerin hepsi hÄ±rsÄ±zdÄ±r\",\n","    \"defolun bizim Ã¼lkemizden\",\n","    \"kadÄ±nÄ±n yeri mutfaktÄ±r\",\n","    \"Yunanistan haddini bilecek yoksa bir gece ansizin gelebiliriz\",\n","    \"GayrimÃ¼slimler milletvekili olmasÄ±n.\",\n","    \"MÃ¼lteciler oy kullanmasÄ±n.\",\n","    \"MÃ¼ltecilere oy hakki verilmemeli\",\n","    \"MÃ¼lteciler sgk'dan yararlanmasÄ±n.\",\n","    \"KÃ¼rtÃ§e eÄŸitim kabul edilemez.\",\n","    \"MÃ¼ltecilerin de yaÅŸamaya hakkÄ± var.\",\n","    \"TerÃ¶r Ã¶rgÃ¼tlerinin faaliyetlerini Ã¶nlemek iÃ§in hukuki Ã§erÃ§evede kapsamlÄ± Ã¶nlemler alÄ±nmaktadÄ±r.\",\n","    \"Ä°srail'de bu yÄ±l dÃ¼zenlenecek kÃ¼ltÃ¼rel etkinliklerin tarihleri aÃ§Ä±klandÄ±\",\n","    \"KadÄ±n giriÅŸimciler, hem MÃ¼slÃ¼man hem de diÄŸer topluluklardan mÃ¼ÅŸterilere hitap eden gÃ¼zellik salonlarÄ± aÃ§Ä±yor.\",\n","    \"Suriyeâ€™deki pazarlarda MÃ¼slÃ¼man ve Hristiyan bayramlarÄ± Ã¶ncesinde hazÄ±rlÄ±klar yapÄ±lÄ±yor.\",\n","    \"BazÄ± kÃ¶peklerin kuduz ya da saldÄ±rgan olmasÄ±, tÃ¼m kÃ¶peklerin Ã¶ldÃ¼rÃ¼lmesi iÃ§in haklÄ± bir sebep midir? YasayÄ± geri Ã§ek.\",\n","    \"KadÄ±nlar birbirlerine destek olmalÄ±.\",\n","    \"Pazardan elma aldim.\",\n","    \"Suriyelilerin yemekleri Ã§ok gÃ¼zel.\",\n","    \"Suriye yemekleri Ã§ok lezzetli.\",\n","    \"SÃ¼riyelileri Ã§ok seviyorum, Ã§ok sempatik insanlar.\",\n","    \"Singapurlular Ã§ok Ã§alÄ±ÅŸkan\",\n","    \"AmerikalÄ±larÄ± Ã§ok Seviyorum\",\n","    \"Ateistleri hiÃ§ sevmiyorum\",\n","    \"Ortodoksalar hrÄ±stiyanlar\",\n","    \"Singapur Ã§ok gÃ¼zel bir Ã¼lke dir\",\n","    \"DÃ¼n hastaneye gittim sonra da bir gÃ¼zel yemek yedim, eve dÃ¶ndÃ¼m, kÄ±zÄ±mla oyun oynadÄ±m, Amerikan dizi izledim\",\n","    \"hayvanlari Ã§ok seviyorum\"\n","]\n","\n","\n","if model_lang == 'ar':\n","      TWEETS_EXAMPLES = AR_TWEETS_EXAMPLES\n","\n","if model_lang == 'tr':\n","      TWEETS_EXAMPLES = TR_TWEETS_EXAMPLES\n","\n","for tweet in TWEETS_EXAMPLES:\n","    strength = predict_strength_tweet(remove_punctuation_marks(tweet.lower()))\n","    strength = round (strength)\n","    print(f\"Tweet:    {tweet}\")\n","    print(f\"Strength: {strength}\")\n","    print(\"-\" * 60)\n"],"metadata":{"id":"w9xJCvj7b_ey"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyNimMTmG506fR5JdOuuvC/+"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}