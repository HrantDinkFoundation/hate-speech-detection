{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wFXTGNTSP-K0"},"outputs":[],"source":["from google.colab import files\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","root_path = 'gdrive/My Drive/'\n","\n","import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","! nvcc --version\n","!nvidia-smi -L\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAAU3hinHKRQ"},"outputs":[],"source":["!pip install -U transformers\n","#!pip install transformers==4.28.0\n","!pip install datasets\n","!pip install snscrape\n","!pip install --upgrade accelerate\n","!pip install datasets evaluate\n","!pip install accelerate -U\n","\n","from scipy.spatial.distance import cosine\n","from transformers import AutoModel, AutoTokenizer\n","import numpy as np\n","import random\n","import csv\n","import pandas as pd\n","from sklearn.model_selection import StratifiedKFold\n","\n","import tensorflow as tf\n","import datasets\n","from datasets import Dataset, DatasetDict\n","import evaluate\n","from sklearn.metrics import classification_report, f1_score\n","\n","import sys, os\n","import torch\n","import json\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9vyLbRzuVICq"},"outputs":[],"source":["# preprocess functions\n","\n","def remove_url(text):\n","    return ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\",\" \", text).split())\n","\n","def remove_username(text):\n","    return ' '.join(re.sub(\"([@][A-Za-z0-9_]+)\",\" \", text).split())\n","\n","def remove_at_mark(text):\n","    return re.sub(r'[@]', ' ', text)\n","\n","def remove_tag_mark(text):\n","    return re.sub(r'[#]', ' ', text)\n","\n","def replace_hashtags_with_segments (text):\n","   regex = \"#(\\w+)\"\n","   text_split = text.split()\n","   sent= ''\n","   strip_word =' '\n","   for word in text_split:\n","     hashtag= re.findall(regex, word)\n","     #print(hashtag)\n","     #print(strip_word)\n","\n","     if (len(hashtag)) == 0 :\n","        sent = sent + ' ' + word\n","     if (len(hashtag))!=0:\n","        strip_word = word.replace(('#'+hashtag[0]), ' ')\n","        #print (hashtag[0])\n","        #print(strip_word)\n","        try :\n","           key = '#'+ hashtag[0]\n","           segmented = segmented_dict [key]\n","           if type(segmented) == str:\n","                    #segmented = re.sub(r'[|]', ' ', segmented)\n","                    #segmented = re.sub(r'[,]', ' ', segmented)\n","                    sent= sent + ' ' + segmented\n","           if type(segmented) == list:\n","                    for k in segmented:\n","                      sent = sent +' ' + k\n","        except:\n","           sent = sent + ' ' +  hashtag[0]\n","   return sent + ' ' + strip_word\n","\n","def control_tag_mark(text):\n","     s = ' '\n","     for word in text.split():\n","            word = re.sub(r'[#]', ' #', word)\n","            s = s + ' ' + word\n","     return s\n","\n","def remove_punctuation_marks(text):\n","   punc = '''!()-[]{};:'\"\\,<>./?$%^&*~'''\n","   for i in text:\n","       if i in punc:\n","         text = text.replace(i, \" \")\n","   return text\n","\n","def remove_underline(text):\n","  punc = '''_'''\n","  for i in text:\n","       if i in punc:\n","         text = text.replace(i, \"\")\n","  return text\n","\n","def remove_hashtag(text):\n","    return ' '.join(re.sub(\"([#][A-Za-z0-9_]+)\",\" \", text).split())\n","\n","def remove_turkish_hashtag(text):\n","    return ' '.join(re.sub(\"([#][\\w+_]+)\",\" \", text).split())\n","\n","def find_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    return re.findall(emoji_pattern, text) # no emoji\n","\n","def remove_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    return (emoji_pattern.sub(r'', text)) # no emoji\n","\n","def remove_arabic_hashtags(text):\n","   return ' '.join(re.sub('([#][\\u0600-\\u06ff_]+)', \" \", text).split())\n","\n","def remove_emoji_duplicate (text):\n","      emoji =[]\n","      rest = ''\n","      main_text = []\n","      for w in text :\n","         x = find_emoji(w)\n","         if len (x) != 0 :\n","            emoji.append (x[0])\n","      #print(emoji)\n","      rest = remove_emoji(text)\n","      emoji = list(set(emoji))\n","      for e in emoji :\n","        try:\n","          rest = rest + \" \" + e\n","        except:\n","          rest = rest\n","      main_text.append(rest)\n","      return main_text[0]\n","\n","def lower_case(text):\n","    text = text.replace(\"I\", \"Ä±\")\n","    text = text.replace(\"Ä°\", \"i\")\n","    return text.lower()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAAVod3AVjod"},"outputs":[],"source":["def seed_everything(seed):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","seed_everything(0)"]},{"cell_type":"code","source":["# Define data path\n","# Please replace the addresses with the address of dataset on your local drive\n","\n","path_turkish_data     =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/df_turkish_cats_targets_degrees_final_with_translated_all.csv'\n","path_arabic_data      =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/df_arabic_cats_targets_degrees_final.csv'\n","path_synthetic_data   =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/synthetic_data_all_new.xlsx'\n","\n","path_all_emoji =         '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/all_emoji_dict.json'\n","path_hashtag_segment  =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/all_hashtag_segmented_tr.json'"],"metadata":{"id":"ji-JuOQJvvw3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gn-0qMMmVyjS"},"outputs":[],"source":["# Laod emoji_dict and hashtag_dict\n","\n","f = open(path_all_emoji)\n","data = json.load(f)\n","emoji_list = data['emojis']\n","\n","f = open(path_hashtag_segment)\n","segmented_dict = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbINt_v7XOKE"},"outputs":[],"source":["# Select which preprocess steps must be apply on data\n","\n","def pre_process_tweets(text_list):\n","   cleaned_text =[]\n","   for s in text_list:\n","       #s = lower_case(s)\n","       s = remove_username(s)\n","       s = remove_at_mark(s)\n","       s = remove_url(s)\n","       s = remove_tag_mark(s)\n","       #s = control_tag_mark(s)\n","       s = remove_punctuation_marks(s)\n","       s = remove_underline(s)\n","       #s = replace_hashtags_with_segments(s)\n","       cleaned_text.append(s)\n","   return cleaned_text"]},{"cell_type":"code","source":["#  Main target groups in the dataset\n","\n","main_targets = {'Hedef grup belirgin deÄŸil veya yok.': 0,\n","                'Ãœlke/Milliyet': 1,\n","                'Irk/Etnik KÃ¶ken': 2,\n","                'Din': 3,\n","                'Cinsiyet': 4,\n","                'Cinsel YÃ¶nelim': 5,\n","                'Belli GÃ¶rÃ¼ÅŸ/StatÃ¼/Uygulama; Mesleki Pozisyon Grubu': 6,\n","                'Hedef grup birden fazla.': 7}"],"metadata":{"id":"ffTlz8SeCJUF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Labels after merging :\n","\n","# 'Ãœlke/Milliyet' and 'Irk/Etnik KÃ¶ken'  merged\n","# 'Cinsiyet' and 'Cinsel YÃ¶nelim' merged\n","\n","labels = ['Target group not specified or not present' ,\n","          'Country/Nationality/Race/Ethnicity' ,\n","          'Religion',\n","          'Gender/Sexual Orientation',\n","          'Specific Viewpoint/Status/Practice; Occupational Position Group']\n","\n","\n","map_7_to_5 = { 0:0, 1:1, 2:1, 3:2, 4:3, 5:3 , 6:4}\n","def extract_5class(inp):\n","    out = set()\n","    for i in inp:\n","      out.add(map_7_to_5[i])\n","\n","    return out\n","\n","\n","# remove 0 class (no-target), when there is other targets\n","\n","def conflicting_target_0(inp):\n","    out = inp.copy()\n","    if 0 in inp and len(inp) > 1:\n","        out.remove(0)\n","    return out"],"metadata":{"id":"r0dAP6ebTic_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wUp63TQOo8Ke"},"outputs":[],"source":["#Load synthetic data\n","\n","df_syn = pd.read_excel(path_synthetic_data)\n","df_syn = df_syn.iloc[:,[4,8,9]]\n","\n","\n","df_syn = df_syn[df_syn.TargetAll != set()].copy()\n","df_syn['TargetAll']= df_syn.TargetAll.apply(eval)\n","\n","print(df_syn['TargetAll'].value_counts())\n","\n","df_syn['TargetAll'] = df_syn.TargetAll.apply(extract_5class)\n","df_syn['TargetAll'] = df_syn.TargetAll.apply(conflicting_target_0)\n","\n","print(df_syn['TargetAll'].value_counts())\n","\n","df_syn= df_syn.sample(frac = 1)\n","df_syn\n"]},{"cell_type":"code","source":["#Load synthetic data\n","\n","df_turkish = pd.read_csv(path_turkish_data)\n","\n","df_turkish = df_turkish.iloc[:,[2, 36,38, 39]]\n","\n","label_all = df_turkish['TargetAll'].tolist()\n","label_all = [str({item}) for item in label_all]\n","\n","df_turkish = df_turkish[df_turkish.TargetAll != set()].copy()\n","df_turkish['TargetAll']= df_turkish.TargetAll.apply(eval)\n","\n","df_turkish['TargetAll'] = df_turkish.TargetAll.apply(extract_5class)\n","df_turkish['TargetAll'] = df_turkish['TargetAll'].apply(lambda x: x.discard(4) or x)    # ignor this class: 'Specific Viewpoint/Status/Practice; Occupational Position Group'\n","df_turkish['TargetAll'] = df_turkish.TargetAll.apply(conflicting_target_0)\n","df_turkish = df_turkish[df_turkish.TargetAll != set()].copy()\n","\n","print(df_turkish['TargetAll'].value_counts())\n","\n","\n","#df_all = df_all.reset_index()\n","df_turkish= df_turkish.sample(frac = 1)\n","\n","df_turkish"],"metadata":{"id":"MHfbcRLgxCTc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","df_arabic = pd.read_csv(path_arabic_data)\n","\n","df_arabic = df_arabic.iloc[:,[2,36,38]]\n","\n","\n","df_arabic = df_arabic[df_arabic.TargetAll != set()].copy()\n","df_arabic['TargetAll']= df_arabic.TargetAll.apply(eval)\n","\n","df_arabic['TargetAll'] = df_arabic.TargetAll.apply(extract_5class)\n","df_arabic['TargetAll'] = df_arabic['TargetAll'].apply(lambda x: x.discard(4) or x)  # ignor this class: 'Specific Viewpoint/Status/Practice; Occupational Position Group'\n","df_arabic['TargetAll'] = df_arabic.TargetAll.apply(conflicting_target_0)\n","df_arabic = df_arabic[df_arabic.TargetAll != set()].copy()\n","\n","print(df_arabic['TargetAll'].value_counts())\n","df_arabic= df_arabic.sample(frac = 1)\n","\n","df_arabic\n","\n"],"metadata":{"id":"iiP4O0k0xfYD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define the name and the adress of trained model on HuggingFace\n","\n","hub_id = 'HrantDinkFoundation/'\n","model_id = 'hs-target-prediction'"],"metadata":{"id":"ahU5jLPq-2UP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# select which data used in training\n","\n","# 'only-tr' : use only Turkish data\n","# 'only-ar' : use only arabic data\n","# 'tr-syn'  : use Turkish and synthetic data\n","# 'ar-syn'  : use arabic and synthetic data\n","# 'ar-translated-syn'  : use arabic, synthetic and the translated column of Turkish data\n","\n","mode_train = ['only-tr', 'only-ar', 'tr-syn', 'ar-syn', 'ar-translated-syn']\n","mode = mode_train[2]"],"metadata":{"id":"4NL7OpXAwbw5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if mode =='only-tr':\n","    model_lang = 'tr'\n","    model_name = 'dbmdz/bert-base-turkish-uncased'\n","    df_train = df_turkish[(df_turkish.Div=='train')]\n","    df_test  = df_turkish[(df_turkish.Div=='test')]\n","\n","\n","if mode =='only-ar' :\n","     model_lang = 'ar'\n","     model_name = 'aubmindlab/bert-base-arabert'\n","     df_train = df_arabic[(df_arabic.Div=='train')]\n","     df_test  = df_arabic[(df_arabic.Div=='test')]\n","\n","\n","if mode == 'tr-syn':\n","    model_lang = 'tr'\n","    model_name = 'dbmdz/bert-base-turkish-uncased'\n","    hub_id = hub_id + 'turkish-' + model_id\n","    df_train = df_turkish[(df_turkish.Div=='train')]\n","    df_test  = df_turkish[(df_turkish.Div=='test')]\n","\n","    df_train = pd.concat([df_train, df_syn], ignore_index=True)\n","\n","\n","if mode == 'ar-syn':\n","    model_lang = 'ar'\n","    model_name = 'aubmindlab/bert-base-arabert'\n","    df_train = df_arabic[(df_arabic.Div=='train')]\n","    df_test  = df_arabic[(df_arabic.Div=='test')]\n","\n","    df_syn = df_syn.drop (columns=['Text'])\n","    df_syn.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","    df_train = pd.concat([df_train, df_syn], ignore_index=True)\n","\n","\n","if mode == 'ar-translated-syn' :\n","     model_lang = 'ar'\n","     model_name = 'aubmindlab/bert-base-arabert'\n","     hub_id = hub_id + 'arabic-' + model_id\n","     df_turkish = df_turkish.drop (columns=['Text'])\n","     df_turkish.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","\n","     df_syn = df_syn.drop (columns=['Text'])\n","     df_syn.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","\n","     df_train = df_arabic[(df_arabic.Div=='train')]\n","     df_test  = df_arabic[(df_arabic.Div=='test')]\n","\n","     df_train = pd.concat([df_train, df_turkish, df_syn], ignore_index=True)\n","\n"],"metadata":{"id":"sF2M8ZN2-wtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train"],"metadata":{"id":"E06oD192YYL3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test\n"],"metadata":{"id":"msR_sbt1BSkF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_class = 4\n","column_select = 'TargetAll'\n","k_fold = 10\n"],"metadata":{"id":"H3qB2gQ_BFG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","df_train = df_train[df_train.TargetAll != set()].copy()\n","\n","text = df_train['Text'].tolist()\n","label = df_train[column_select].tolist()\n","label = [str(x) for x in label]\n","text = pre_process_tweets(text)\n","train_data = pd.DataFrame(list(zip(text, label)), columns =['text', 'label'])\n","\n","print(train_data['label'].value_counts())\n","train_data\n"],"metadata":{"id":"gU9qGrSUBYmv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import ast\n","\n","df_test = df_test[df_test.TargetAll != set()].copy()\n","print(df_test['TargetAll'].value_counts())\n","\n","text = df_test['Text'].tolist()\n","label = df_test[column_select].tolist()\n","label = [list(s) for s in label]\n","label = list(map(lambda l: list(np.where(np.isin(np.arange(num_class), l), float(1), float(0))), label))\n","\n","text = pre_process_tweets(text)\n","test_data = pd.DataFrame(list(zip(text, label)), columns =['text', 'label'])\n","#test_data= test_data.astype({'label':'int'})\n","\n","print(test_data['label'].value_counts())\n","test_data"],"metadata":{"id":"TdNbSju9C9uk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mjkcw1fDWToG"},"outputs":[],"source":["# Load BERT model\n","\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from transformers import ElectraForSequenceClassification,  ElectraTokenizerFast\n","from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaTokenizerFast, RobertaForSequenceClassification\n","from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments, GPT2Config\n","\n","labels = ['Target group not specified or not present' ,\n","          'Country/Nationality/Race/Ethnicity' ,\n","          'Religion',\n","          'Gender/Sexual Orientation']\n","\n","id2label = {idx:label for idx, label in enumerate(labels)}\n","label2id = {label:idx for idx, label in enumerate(labels)}\n","\n","pretrained_model_name = model_name\n","print(model_name)\n","\n","tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, do_lower_case=True, force_download=True)\n","model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name, num_labels = num_class, problem_type=\"multi_label_classification\",\n","                                                           force_download=True, id2label=id2label, label2id=label2id).to(device)\n","\n","model.resize_token_embeddings(len(tokenizer))\n","for item in emoji_list :\n","        tokenizer.add_tokens(item)\n","model.resize_token_embeddings(len(tokenizer))\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vh9FzUR427JW"},"outputs":[],"source":["def preprocess_function(examples):\n","    preprocessed = examples[\"text\"]\n","    return tokenizer(preprocessed, truncation=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZfw_lai35A4"},"outputs":[],"source":["from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n","from transformers import EvalPrediction\n","import torch\n","\n","# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n","def multi_label_metrics(predictions, labels, threshold=0.5):\n","    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n","    sigmoid = torch.nn.Sigmoid()\n","    probs = sigmoid(torch.Tensor(predictions))\n","    # next, use threshold to turn them into integer predictions\n","    y_pred = np.zeros(probs.shape)\n","    y_pred[np.where(probs >= threshold)] = 1\n","    # finally, compute metrics\n","    y_true = labels\n","    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n","    try:\n","      roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n","    except ValueError:\n","      pass\n","    accuracy = accuracy_score(y_true, y_pred)\n","    # return as dictionary\n","    metrics = {'f1': f1_micro_average,\n","               'roc_auc': roc_auc,\n","               'accuracy': accuracy}\n","    return metrics\n","\n","def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions[0] if isinstance(p.predictions,\n","            tuple) else p.predictions\n","    result = multi_label_metrics(\n","        predictions=preds,\n","        labels=p.label_ids)\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ntZERAEqvNwt"},"outputs":[],"source":["k_fold = 8\n","skf = StratifiedKFold(n_splits= k_fold, random_state = 31, shuffle=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pdYaTy8NoQi4"},"outputs":[],"source":["class CustomTrainer(Trainer):\n","    def save_model(self, output_dir=None, _internal_call=False):\n","        if output_dir is None:\n","            output_dir = self.args.output_dir\n","\n","        self.model = self.model.to('cuda')\n","\n","        for param in self.model.parameters():\n","            if not param.is_contiguous():\n","                param.data = param.data.contiguous()\n","\n","        super().save_model(output_dir, _internal_call)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c8vqUCVTvSQ0"},"outputs":[],"source":["# Let's start training model\n","\n","for i, (train_index, test_index) in enumerate(skf.split(train_data.text , train_data.label)):\n","\n","    print(\"number of samples in train folds: \", len(train_index))\n","    print(\"number of samples in test fold  : \", len(test_index))\n","\n","    test = train_data.iloc[test_index].set_index(\"text\")\n","    multi_label_test = test['label'].tolist()\n","    multi_label_test = [list(ast.literal_eval(item)) for item in multi_label_test]\n","    label_test = list(map(lambda l: list(np.where(np.isin(np.arange(num_class), l), float(1), float(0))), multi_label_test))\n","    test = test.drop (columns=['label'])\n","    test['label'] = label_test\n","    print(label_test)\n","    test = Dataset.from_pandas(test)\n","\n","\n","    train = train_data.iloc[train_index].set_index(\"text\")\n","    multi_label_train = train['label'].tolist()\n","    multi_label_train = [list(ast.literal_eval(item)) for item in multi_label_train]\n","    label_train = list(map(lambda l: list(np.where(np.isin(np.arange(num_class), l), float(1), float(0))), multi_label_train))\n","    trian = train.drop (columns=['label'])\n","    train['label'] = label_train\n","    train = Dataset.from_pandas(train)\n","\n","\n","    dataset_dict = DatasetDict({\"test\": test, \"train\": train})\n","    preprocessed_dataset_dict = dataset_dict.map(preprocess_function, batched=True)\n","\n","    training_args = TrainingArguments(\n","        output_dir= \"My Hate Speech Detection\",\n","        overwrite_output_dir=True,\n","        #report_to=\"none\",\n","        learning_rate=5e-6,\n","        per_device_train_batch_size=32,\n","        per_device_eval_batch_size=20,\n","        num_train_epochs=10,\n","        weight_decay=0.01,\n","        eval_strategy=\"steps\",\n","        metric_for_best_model = 'f1',\n","        logging_steps = 100,\n","        load_best_model_at_end=True,\n","        greater_is_better=True,\n","        #hub_strategy=\"end\",          # These parameter used when you want saved mmodels directly to HuggingFace,\n","        #push_to_hub=True,            # If you want to save model on huggingFace you must select \"True\"\n","        #hub_model_id= hub_id,        # Write your model address on HuggingFace\n","        #hub_private_repo= False,     # Determine whether the model on Hugging Face should be private or public.\n","        #hub_token= \"...........\",    # To save model into HuggingFace you need \"hub_token\" which can take from your own HuggingFace account\n","    )\n","\n","    trainer = CustomTrainer(\n","        model = model,\n","        args = training_args,\n","        train_dataset=preprocessed_dataset_dict[\"train\"],\n","        eval_dataset=preprocessed_dataset_dict[\"test\"],\n","        tokenizer = tokenizer,\n","        data_collator = data_collator,\n","        compute_metrics = compute_metrics,\n","\n","    )\n","    trainer.train()\n","    break\n","\n","#trainer.save_model()      # If you want save your trained model on HuggingFace\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSHmt9lFDzt9"},"outputs":[],"source":["#Let's start testing model\n","\n","test_ds = Dataset.from_pandas(test_data).map(preprocess_function, batched=True)\n","test_ds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fd_JrO8P4lkc"},"outputs":[],"source":["#Let's start testing model\n","\n","id2label = {idx:label for idx, label in enumerate(labels)}\n","label2id = {label:idx for idx, label in enumerate(labels)}\n","\n","model.eval()\n","\n","y_test = test_data['label'].tolist()\n","y_test = np.array(y_test)\n","y_pred =[]\n","\n","for t in test_data['text'].tolist() :\n","   encoding = tokenizer(t, return_tensors=\"pt\")\n","   encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n","\n","   outputs = trainer.model(**encoding)\n","   logits = outputs.logits\n","   logits.shape\n","   sigmoid = torch.nn.Sigmoid()\n","   probs = sigmoid(logits.squeeze().cpu())\n","   predictions = np.zeros(probs.shape)\n","   predictions[np.where(probs >= 0.3)] = 1\n","   y_pred.append(predictions)\n"]},{"cell_type":"code","source":["label_mapping = {0:'Target group not specified or not present' ,\n","                 1: 'Country/Nationality/Race/Ethnicity' ,\n","                 2: 'Religion',\n","                 3: 'Gender/Sexual Orientation'}"],"metadata":{"id":"uF61JDBQDa2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-fDE4iEj5b2U"},"outputs":[],"source":["from sklearn.metrics import classification_report, f1_score\n","from sklearn.metrics import confusion_matrix\n","\n","print ('-------------------------------------------------------')\n","print(classification_report(y_test, y_pred, digits=num_class, zero_division=0))\n"]},{"cell_type":"code","source":["from sklearn.metrics import classification_report, f1_score\n","from sklearn.metrics import confusion_matrix\n","\n","print ('-------------------------------------------------------')\n","print(classification_report(y_test, y_pred, target_names=labels, zero_division=0))\n"],"metadata":{"id":"W-9d161z1r4q"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGV9C4TRFs_g"},"outputs":[],"source":["# Saving trained model on local drive\n","# Please replace the pathes with your local address path\n","\n","\n","if model_lang == 'tr':\n","          path_save_model = '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/trained_models/hs-tr-target/'\n","\n","if model_lang == 'ar':\n","          path_save_model = '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/trained_models/hs-ar-strength/'\n","\n","trainer.save_model(path_save_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1z-apwd41ER"},"outputs":[],"source":["# Load trained model from local drive\n","# Please add the address where the models are saved\n","\n","from transformers import pipeline\n","\n","path_trained_model = path_save_model\n","model_pipline = pipeline(task = \"text-classification\", model = path_trained_model, top_k=None, return_all_scores=True, device=\"cuda\")"]},{"cell_type":"code","source":["def predict_target_tweet(review):\n","        prediction = []\n","        result = []\n","        predictions = model_pipline (review)\n","        prediction_result = predictions[0]\n","        result = []\n","        # Filter predictions based on the sigmoid threshold\n","        filtered_predictions = [pred for pred in prediction_result if pred['score'] > 0.2]\n","\n","        # Print all labels that meet the threshold\n","        if filtered_predictions:\n","            for item in filtered_predictions:\n","                   result.append ((item['label'], 'score:', item['score']))\n","            #else:\n","                   #result.append (\"No predictions meet the threshold.\")\n","\n","        return result\n"],"metadata":{"id":"BJVReiz4W0cb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ğŸ”´ â–¶ **Disclaimer:**\n","\n","This study (project) includes examples of offensive language, hate speech, and profanity due to the nature of the research. These examples do not represent the authorsâ€™ views (or developers' views). The goal of this work is to contribute to the detection and prevention of harmful content and violence directed toward refugees, LGBTI+ individuals, religious groups, nationalities, and other vulnerable communities."],"metadata":{"id":"VeDHKb20g8lj"}},{"cell_type":"code","source":["AR_TWEETS_EXAMPLES = [\n","    \"ÙƒÙ„ Ø§Ù„Ù„Ø§Ø¬Ø¦ÙŠÙ† Ù„ØµÙˆØµ\",\n","    \"Ø§Ø±Ø­Ù„ÙˆØ§ Ù…Ù† Ø¨Ù„Ø¯Ù†Ø§\",\n","    \"Ù…ÙƒØ§Ù† Ø§Ù„Ù…Ø±Ø£Ø© Ù‡Ùˆ Ø§Ù„Ù…Ø·Ø¨Ø®\",\n","    \"Ø§Ù„ÙŠÙˆÙ†Ø§Ù† ÙŠØ¬Ø¨ Ø£Ù† ØªØ¹Ø±Ù Ø­Ø¯ÙˆØ¯Ù‡Ø§ØŒ ÙˆØ¥Ù„Ø§ Ù‚Ø¯ Ù†Ø£ØªÙŠ ÙØ¬Ø£Ø© ÙÙŠ Ù„ÙŠÙ„Ø©\",\n","    \"Ù„Ø§ ÙŠÙ†Ø¨ØºÙŠ Ù„ØºÙŠØ± Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† Ø£Ù† ÙŠØµØ¨Ø­ÙˆØ§ Ù†ÙˆØ§Ø¨Ù‹Ø§.\",\n","    \"ÙŠØ¬Ø¨ Ø£Ù„Ø§ ÙŠÙØ³Ù…Ø­ Ù„Ù„Ø§Ø¬Ø¦ÙŠÙ† Ø¨Ø§Ù„ØªØµÙˆÙŠØª.\",\n","    \"ÙŠØ¬Ø¨ Ø£Ù„Ø§ ÙŠØ³ØªÙÙŠØ¯ Ø§Ù„Ù„Ø§Ø¬Ø¦ÙˆÙ† Ù…Ù† Ø§Ù„ØªØ£Ù…ÙŠÙ† Ø§Ù„ØµØ­ÙŠ.\",\n","    \"Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ±Ø¯ÙŠØ© ØºÙŠØ± Ù…Ù‚Ø¨ÙˆÙ„.\",\n","    \"Ù„Ù„Ø§Ø¬Ø¦ÙŠÙ† Ø£ÙŠØ¶Ù‹Ø§ Ø§Ù„Ø­Ù‚ ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø©.\",\n","    \"ÙŠØªÙ… Ø§ØªØ®Ø§Ø° ØªØ¯Ø§Ø¨ÙŠØ± Ø´Ø§Ù…Ù„Ø© Ø¶Ù…Ù† Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù„Ù…Ù†Ø¹ Ø£Ù†Ø´Ø·Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ…Ø§Øª Ø§Ù„Ø¥Ø±Ù‡Ø§Ø¨ÙŠØ©.\",\n","    \"ØªÙ… Ø§Ù„Ø¥Ø¹Ù„Ø§Ù† Ø¹Ù† ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„ÙØ¹Ø§Ù„ÙŠØ§Øª Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ© Ø§Ù„ØªÙŠ Ø³ØªÙÙ‚Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ø¹Ø§Ù… ÙÙŠ Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„.\",\n","    \"ØªÙØªØªØ­ Ø±Ø§Ø¦Ø¯Ø§Øª Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ ØµØ§Ù„ÙˆÙ†Ø§Øª ØªØ¬Ù…ÙŠÙ„ ØªØ³ØªÙ‡Ø¯Ù Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ù…Ù† Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† ÙˆØºÙŠØ± Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ†.\",\n","    \"ØªÙØ¬Ø±Ù‰ Ø§Ù„Ø§Ø³ØªØ¹Ø¯Ø§Ø¯Ø§Øª ÙÙŠ Ø§Ù„Ø£Ø³ÙˆØ§Ù‚ ÙÙŠ Ø³ÙˆØ±ÙŠØ§ Ù‚Ø¨Ù„ Ø§Ù„Ø£Ø¹ÙŠØ§Ø¯ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© ÙˆØ§Ù„Ù…Ø³ÙŠØ­ÙŠØ©.\",\n","    \"Ù‡Ù„ ÙƒÙˆÙ† Ø¨Ø¹Ø¶ Ø§Ù„ÙƒÙ„Ø§Ø¨ Ù…Ø³Ø¹ÙˆØ±Ø© Ø£Ùˆ Ø¹Ø¯ÙˆØ§Ù†ÙŠØ© ÙŠØ¨Ø±Ø± Ù‚ØªÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ø§Ø¨ØŸ Ø§Ø³Ø­Ø¨ÙˆØ§ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†.\",\n","     \"ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø³Ø§Ø¡ Ø¯Ø¹Ù… Ø¨Ø¹Ø¶Ù‡Ù† Ø§Ù„Ø¨Ø¹Ø¶\",\n","    \"Ø§Ø´ØªØ±ÙŠØª ØªÙØ§Ø­Ù‹Ø§ Ù…Ù† Ø§Ù„Ø³ÙˆÙ‚.\",\n","    \"Ø·Ø¹Ø§Ù… Ø§Ù„Ø³ÙˆØ±ÙŠÙŠÙ† Ù„Ø°ÙŠØ° Ø¬Ø¯Ù‹Ø§.\",\n","    \"Ø§Ù„Ø·Ø¹Ø§Ù… Ø§Ù„Ø³ÙˆØ±ÙŠ Ø´Ù‡ÙŠ Ù„Ù„ØºØ§ÙŠØ©.\",\n","    \"Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ø³ÙˆØ±ÙŠÙŠÙ† ÙƒØ«ÙŠØ±Ù‹Ø§ØŒ Ø¥Ù†Ù‡Ù… Ø£Ø´Ø®Ø§Øµ Ù„Ø·ÙŠÙÙˆÙ† Ø¬Ø¯Ù‹Ø§.\",\n","    \"Ø§Ù„Ø³Ù†ØºØ§ÙÙˆØ±ÙŠÙˆÙ† ÙŠØ¹Ù…Ù„ÙˆÙ† Ø¨Ø¬Ø¯.\",\n","    \"Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠÙŠÙ† ÙƒØ«ÙŠØ±Ù‹Ø§\",\n","    \"Ø£Ù†Ø§ Ù„Ø§ Ø£Ø­Ø¨ Ø§Ù„Ù…Ù„Ø­Ø¯ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚\",\n","    \"Ø§Ù„Ù…Ø³ÙŠØ­ÙŠÙŠÙ† Ø§Ù„Ø£Ø±Ø«ÙˆØ°ÙƒØ³\",\n","    \"Ø³Ù†ØºØ§ÙÙˆØ±Ø© Ø¨Ù„Ø¯ Ø¬Ù…ÙŠÙ„ Ø¬Ø¯Ù‹Ø§\"   ,\n","    \"Ø°Ù‡Ø¨Øª Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ Ø£Ù…Ø³ØŒ Ø«Ù… ØªÙ†Ø§ÙˆÙ„Øª ÙˆØ¬Ø¨Ø© Ø·Ø¹Ø§Ù… Ù„Ø°ÙŠØ°Ø©ØŒ Ø«Ù… Ø¹Ø¯Øª Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù†Ø²Ù„ØŒ ÙˆÙ„Ø¹Ø¨Øª Ù…Ø¹ Ø§Ø¨Ù†ØªÙŠØŒ ÙˆØ´Ø§Ù‡Ø¯Øª Ù…Ø³Ù„Ø³Ù„Ù‹Ø§ Ø£Ù…Ø±ÙŠÙƒÙŠÙ‹Ø§\",\n","    \"Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙƒØ«ÙŠØ±Ù‹Ø§\"\n","]\n","\n","TR_TWEETS_EXAMPLES = [\n","    \"mÃ¼ltecilerin hepsi hÄ±rsÄ±zdÄ±r\",\n","    \"defolun bizim Ã¼lkemizden\",\n","    \"kadÄ±nÄ±n yeri mutfaktÄ±r\",\n","    \"Yunanistan haddini bilecek yoksa bir gece ansizin gelebiliriz\",\n","    \"GayrimÃ¼slimler milletvekili olmasÄ±n.\",\n","    \"MÃ¼lteciler oy kullanmasÄ±n.\",\n","    \"MÃ¼ltecilere oy hakki verilmemeli\",\n","    \"MÃ¼lteciler sgk'dan yararlanmasÄ±n.\",\n","    \"KÃ¼rtÃ§e eÄŸitim kabul edilemez.\",\n","    \"MÃ¼ltecilerin de yaÅŸamaya hakkÄ± var.\",\n","    \"TerÃ¶r Ã¶rgÃ¼tlerinin faaliyetlerini Ã¶nlemek iÃ§in hukuki Ã§erÃ§evede kapsamlÄ± Ã¶nlemler alÄ±nmaktadÄ±r.\",\n","    \"Ä°srail'de bu yÄ±l dÃ¼zenlenecek kÃ¼ltÃ¼rel etkinliklerin tarihleri aÃ§Ä±klandÄ±\",\n","    \"KadÄ±n giriÅŸimciler, hem MÃ¼slÃ¼man hem de diÄŸer topluluklardan mÃ¼ÅŸterilere hitap eden gÃ¼zellik salonlarÄ± aÃ§Ä±yor.\",\n","    \"Suriyeâ€™deki pazarlarda MÃ¼slÃ¼man ve Hristiyan bayramlarÄ± Ã¶ncesinde hazÄ±rlÄ±klar yapÄ±lÄ±yor.\",\n","    \"BazÄ± kÃ¶peklerin kuduz ya da saldÄ±rgan olmasÄ±, tÃ¼m kÃ¶peklerin Ã¶ldÃ¼rÃ¼lmesi iÃ§in haklÄ± bir sebep midir? YasayÄ± geri Ã§ek.\",\n","    \"KadÄ±nlar birbirlerine destek olmalÄ±.\",\n","    \"Pazardan elma aldim.\",\n","    \"Suriyelilerin yemekleri Ã§ok gÃ¼zel.\",\n","    \"Suriye yemekleri Ã§ok lezzetli.\",\n","    \"SÃ¼riyelileri Ã§ok seviyorum, Ã§ok sempatik insanlar.\",\n","    \"Singapurlular Ã§ok Ã§alÄ±ÅŸkan\",\n","    \"AmerikalÄ±larÄ± Ã§ok Seviyorum\",\n","    \"Ateistleri hiÃ§ sevmiyorum\",\n","    \"Ortodoksalar hrÄ±stiyanlar\",\n","    \"Singapur Ã§ok gÃ¼zel bir Ã¼lke dir\",\n","    \"DÃ¼n hastaneye gittim sonra da bir gÃ¼zel yemek yedim, eve dÃ¶ndÃ¼m, kÄ±zÄ±mla oyun oynadÄ±m, Amerikan dizi izledim\",\n","    \"hayvanlari Ã§ok seviyorum\"\n","]\n","\n","if model_lang == 'ar':\n","      TWEETS_EXAMPLES = AR_TWEETS_EXAMPLES\n","\n","if model_lang == 'tr':\n","      TWEETS_EXAMPLES = TR_TWEETS_EXAMPLES\n","\n","\n","for tweet in TWEETS_EXAMPLES:\n","    target = predict_target_tweet(remove_punctuation_marks(tweet.lower()))\n","    print(f\"Tweet:    {tweet}\")\n","    print(f\"Target: {target}\")\n","    print(\"-\" * 60)\n"],"metadata":{"id":"wH0rs_PIWZJs"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyN4JNs6VEI02/1UlXpu5Fgi"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}