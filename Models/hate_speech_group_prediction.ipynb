{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNROifaUmsk7pUVFOO5GAoU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-NvmRLR2tvqJ"},"outputs":[],"source":["from google.colab import files\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","root_path = 'gdrive/My Drive/'\n","\n","import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","! nvcc --version\n","!nvidia-smi -L\n","device"]},{"cell_type":"code","source":["!pip install -U transformers\n","#!pip install transformers==4.28.0\n","!pip install datasets\n","!pip install snscrape\n","!pip install --upgrade accelerate\n","!pip install datasets evaluate\n","!pip install accelerate -U\n","\n","from scipy.spatial.distance import cosine\n","from transformers import AutoModel, AutoTokenizer\n","import numpy as np\n","import random\n","import csv\n","import pandas as pd\n","from sklearn.model_selection import StratifiedKFold\n","import tensorflow as tf\n","import datasets\n","from datasets import Dataset, DatasetDict\n","import evaluate\n","from sklearn.metrics import classification_report, f1_score\n","\n","import sys, os\n","import torch\n","import json\n","import re"],"metadata":{"id":"rvcFz_got2c7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# preprocess functions\n","\n","def remove_url(text):\n","    return ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\",\" \", text).split())\n","\n","def remove_username(text):\n","    return ' '.join(re.sub(\"([@][A-Za-z0-9_]+)\",\" \", text).split())\n","\n","def remove_at_mark(text):\n","    return re.sub(r'[@]', ' ', text)\n","\n","def remove_tag_mark(text):\n","    return re.sub(r'[#]', ' ', text)\n","\n","def replace_hashtags_with_segments (text):\n","   regex = \"#(\\w+)\"\n","   text_split = text.split()\n","   sent= ''\n","   strip_word =' '\n","   for word in text_split:\n","     hashtag= re.findall(regex, word)\n","     #print(hashtag)\n","     #print(strip_word)\n","\n","     if (len(hashtag)) == 0 :\n","        sent = sent + ' ' + word\n","     if (len(hashtag))!=0:\n","        strip_word = word.replace(('#'+hashtag[0]), ' ')\n","        #print (hashtag[0])\n","        #print(strip_word)\n","        try :\n","           key = '#'+ hashtag[0]\n","           segmented = segmented_dict [key]\n","           if type(segmented) == str:\n","                    #segmented = re.sub(r'[|]', ' ', segmented)\n","                    #segmented = re.sub(r'[,]', ' ', segmented)\n","                    sent= sent + ' ' + segmented\n","           if type(segmented) == list:\n","                    for k in segmented:\n","                      sent = sent +' ' + k\n","        except:\n","           sent = sent + ' ' +  hashtag[0]\n","   return sent + ' ' + strip_word\n","\n","def control_tag_mark(text):\n","     s = ' '\n","     for word in text.split():\n","            word = re.sub(r'[#]', ' #', word)\n","            s = s + ' ' + word\n","     return s\n","\n","def remove_punctuation_marks(text):\n","   punc = '''!()-[]{};:'\"\\,<>./?$%^&*~'''\n","   for i in text:\n","       if i in punc:\n","         text = text.replace(i, \" \")\n","   return text\n","\n","\n","\n","def remove_underline(text):\n","  punc = '''_'''\n","  for i in text:\n","       if i in punc:\n","         text = text.replace(i, \"\")\n","  return text\n","\n","def remove_hashtag(text):\n","    return ' '.join(re.sub(\"([#][A-Za-z0-9_]+)\",\" \", text).split())\n","\n","def remove_turkish_hashtag(text):\n","    return ' '.join(re.sub(\"([#][\\w+_]+)\",\" \", text).split())\n","\n","def find_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    return re.findall(emoji_pattern, text) # no emoji\n","\n","def remove_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    return (emoji_pattern.sub(r'', text)) # no emoji\n","\n","def remove_arabic_hashtags(text):\n","   return ' '.join(re.sub('([#][\\u0600-\\u06ff_]+)', \" \", text).split())\n","\n","def remove_emoji_duplicate (text):\n","      emoji =[]\n","      rest = ''\n","      main_text = []\n","      for w in text :\n","         x = find_emoji(w)\n","         if len (x) != 0 :\n","            emoji.append (x[0])\n","      #print(emoji)\n","      rest = remove_emoji(text)\n","      emoji = list(set(emoji))\n","      for e in emoji :\n","        try:\n","          rest = rest + \" \" + e\n","        except:\n","          rest = rest\n","      main_text.append(rest)\n","      return main_text[0]\n","\n","def lower_case(text):\n","    text = text.replace(\"I\", \"Ä±\")\n","    text = text.replace(\"Ä°\", \"i\")\n","    return text.lower()"],"metadata":{"id":"GmwJIRixuBDc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def seed_everything(seed):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","seed_everything(0)"],"metadata":{"id":"nxL6VBYOuBFm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define data path\n","# Please replace the addresses with the address of dataset on your local drive\n","\n","path_turkish_data     =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/df_turkish_cats_targets_degrees_final_with_translated_all.csv'\n","path_arabic_data      =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/df_arabic_cats_targets_degrees_final.csv'\n","path_synthetic_data   =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/synthetic_data_all_new.xlsx'\n","\n","path_all_emoji =         '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/all_emoji_dict.json'\n","path_hashtag_segment  =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/all_hashtag_segmented_tr.json'"],"metadata":{"id":"oQwMXxin08oe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Laod emoji_dict and hashtag_dict\n","\n","f = open(path_all_emoji)\n","data = json.load(f)\n","emoji_list = data['emojis']\n","\n","f = open(path_hashtag_segment)\n","segmented_dict = json.load(f)"],"metadata":{"id":"79D1SFy2uBJR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select which preprocess steps must be apply on data\n","\n","def pre_process_tweets(text_list):\n","   cleaned_text =[]\n","   for s in text_list:\n","       #s = lower_case(s)\n","       s = s.lower()\n","       s = remove_username(s)\n","       s = remove_at_mark(s)\n","       s = remove_url(s)\n","       s = remove_tag_mark(s)\n","       #s = control_tag_mark(s)\n","       s = remove_punctuation_marks(s)\n","       s = remove_underline(s)\n","       #s = replace_hashtags_with_segments(s)\n","       #s = remove_tag_mark(s)\n","       cleaned_text.append(s)\n","   return cleaned_text"],"metadata":{"id":"nZrS0AFJt2e7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load synthetic data\n","\n","df_syn = pd.read_excel(path_synthetic_data)\n","\n","df_syn = df_syn.iloc[:,[4 , 9, 2]]\n","\n","df_syn= df_syn.sample(frac = 1)\n","\n","print(df_syn['Group'].value_counts())\n","\n","\n","df_syn\n"],"metadata":{"id":"ydHs8oe7utny"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load training data\n","\n","df_turkish = pd.read_csv(path_turkish_data)\n","df_turkish = df_turkish.iloc[:,[2, 1, 38,39]]\n","\n","print(df_turkish['Group'].value_counts())\n","\n","\n","df_turkish"],"metadata":{"id":"oqG2z09ayjgt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load testing data\n","\n","df_arabic = pd.read_csv(path_arabic_data)\n","\n","# Becasue for arabic dataset the group is \"mÃ¼lteci karÅŸÄ±tÄ±\"\n","\n","df_arabic.rename(columns={'Group': 'keyword'}, inplace=True)\n","hedef_group = [\"Refugees\"] * len(df_arabic)\n","df_arabic['Group'] = hedef_group\n","\n","df_arabic = df_arabic.iloc[:,[2, 39, 38]]\n","print(df_arabic['Group'].value_counts())\n","\n","df_arabic"],"metadata":{"id":"OwWF2-_uy-4B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# select the number of classes\n","# select k-fold for spliting train and validation data\n","\n","num_class = 11\n","k_fold = 10"],"metadata":{"id":"wZfLq6AMBT_Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_mapping = { 'LABEL_0': 'No-group',\n","                  'LABEL_1': 'Refugees',\n","                  'LABEL_2': 'Israil-Yahudi',\n","                  'LABEL_3': 'Greeks',\n","                  'LABEL_4': 'Armenian',\n","                  'LABEL_5': 'Alevi',\n","                  'LABEL_6': 'Kurdish',\n","                  'LABEL_7': 'Arabian',\n","                  'LABEL_8': 'LGBTI+',\n","                  'LABEL_9': 'Women',\n","                  'LABEL_10': 'Other groups'\n","\n","                 }\n","\n","labels = [ 'No-group',\n","           'Refugees',\n","           'Jews',\n","           'Greeks',\n","           'Armenians',\n","           'Alevis',\n","           'Kurds',\n","           'Arabs',\n","           'LGBTI+s',\n","           'Women',\n","           'Other-groups'\n","                  ]\n","\n"],"metadata":{"id":"swO9JNgv2OAC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the name and the address of trained model on HuggingFace\n","\n","hub_id = 'HrantDinkFoundation/'\n","model_id = 'hs-group-prediction'"],"metadata":{"id":"qy93o-1GCl8X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# select which data used in training\n","\n","# 'only-tr' : use only Turkish data\n","# 'only-ar' : use only arabic data\n","# 'tr-syn'  : use Turkish and synthetic data\n","# 'ar-syn'  : use arabic and synthetic data\n","# 'ar-translated-syn'  : use arabic, synthetic and the translated column of Turkish data\n","\n","mode_train = ['only-tr' , 'only-ar' , 'tr-syn',  'ar-syn', 'ar-translated-syn']\n","mode = mode_train [2]"],"metadata":{"id":"urRqL8N1CDbQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if mode == 'only-tr':\n","    model_lang = 'tr'\n","    model_name = 'dbmdz/bert-base-turkish-uncased'\n","    hub_id = hub_id + 'turkish-' + model_id\n","\n","    df_train = df_turkish[(df_turkish.Div=='train')]\n","    df_test  = df_turkish[(df_turkish.Div=='test')]\n","\n","\n","if mode == 'only-ar':\n","    model_lang = 'ar'\n","    model_name = 'aubmindlab/bert-base-arabert'\n","    hub_id = hub_id + 'arabic-' + model_id\n","\n","    df_test  = df_arabic[(df_arabic.Div=='test')]\n","    df_train = df_arabic[(df_arabic.Div=='train')]\n","\n","\n","if mode == 'tr-syn':\n","    model_lang = 'tr'\n","    model_name = 'dbmdz/bert-base-turkish-uncased'\n","    hub_id = hub_id + 'turkish-' + model_id\n","    df_train = df_turkish[(df_turkish.Div=='train')]\n","    df_test  = df_turkish[(df_turkish.Div=='test')]\n","\n","    df_train = pd.concat([df_train, df_syn], ignore_index=True)\n","\n","\n","if mode == 'ar-syn':\n","    model_lang = 'ar'\n","    model_name = 'aubmindlab/bert-base-arabert'\n","    hub_id = hub_id + 'arabic-' + model_id\n","\n","\n","    df_syn = df_syn.drop (columns=['Text'])\n","    df_syn.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","\n","    df_test  = df_arabic[(df_arabic.Div=='test')]\n","    df_train = df_arabic[(df_arabic.Div=='train')]\n","\n","    df_train = pd.concat([df_train, df_syn], ignore_index=True)\n","\n","\n","if mode == 'ar-translated-syn' :\n","     model_lang = 'ar'\n","     model_name = 'aubmindlab/bert-base-arabert'\n","     hub_id = hub_id + 'arabic-' + model_id\n","\n","     df_turkish = df_turkish.drop (columns=['Text'])\n","     df_turkish.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","\n","     df_syn = df_syn.drop (columns=['Text'])\n","     df_syn.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","\n","     df_train = df_arabic[(df_arabic.Div=='train')]\n","     df_test  = df_arabic[(df_arabic.Div=='test')]\n","\n","     df_train = pd.concat([df_train, df_turkish, df_syn], ignore_index=True)\n","\n","\n","\n","df_train= df_train.iloc[:,[0, 1, 2]]\n","df_test= df_test.iloc[:,[0, 1, 2]]\n"],"metadata":{"id":"ajYHBvYE2OfK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train"],"metadata":{"id":"m2kofP4xQ95D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test"],"metadata":{"id":"Y5jxHSr6RAo4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# different labeling processes create different naming for same groups, so here we convert them to the uniqe integer class\n","\n","group_mapping = {'No-group': 0 ,\n","                 'mÃ¼lteci' : 1 , 'refugee':1 , 'Refugee' : 1, 'Refugees' : 1,\n","                 'Isr-Pal':2, 'Isr-pal':2, 'Jews':2,\n","                 'Tr-Gr': 3 , 'tr-gr': 3, 'Greeks': 3,\n","                 'Armenian': 4, 'Armeni':4,\n","                 'Alevi': 5,\n","                 'Kurdish': 6, 'Kurd': 6,\n","                 'Arabian':7 ,\n","                 'LGBTI+': 8 ,\n","                 'Women':9 ,\n","                 'Other groups': 10 }"],"metadata":{"id":"iEyYdlie9Knl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_group_class(inp):\n","    return(group_mapping[inp])\n","\n","df_train['label'] = df_train.Group.apply(extract_group_class)\n","\n","df_test['label'] = df_test.Group.apply(extract_group_class)\n","\n","\n","print(df_train['label'].value_counts())\n","print(df_test['label'].value_counts())\n","\n","df_train"],"metadata":{"id":"KjEqLuBP-vz2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# preprocess train data\n","import ast\n","\n","text = df_train['Text'].tolist()\n","label = df_train['label'].tolist()\n","\n","\n","text = pre_process_tweets(text)\n","train_data = pd.DataFrame(list(zip(text, label)), columns =['text', 'label'])\n","train_data= train_data.astype({'label':'int'})\n","\n","\n","print(train_data['label'].value_counts())\n","train_data\n"],"metadata":{"id":"uE1zXZjjBx9I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# preprocess test data\n","\n","text = df_test['Text'].tolist()\n","label = df_test['label'].tolist()\n","\n","text = pre_process_tweets(text)\n","test_data = pd.DataFrame(list(zip(text, label)), columns =['text', 'label'])\n","test_data= test_data.astype({'label':'int'})\n","\n","\n","\n","print(test_data['label'].value_counts())\n","test_data\n"],"metadata":{"id":"zmYzw60UB86W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Laod BERT model\n","\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from transformers import ElectraForSequenceClassification,  ElectraTokenizerFast\n","from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaTokenizerFast, RobertaForSequenceClassification\n","from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments, GPT2Config\n","\n","id2label = {idx:label for idx, label in enumerate(labels)}\n","label2id = {label:idx for idx, label in enumerate(labels)}\n","\n","pretrained_model_name = model_name\n","tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, do_lower_case=True, force_download=True)\n","model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name, num_labels = num_class, force_download=True , id2label=id2label, label2id=label2id).to(device)   #\n","model.resize_token_embeddings(len(tokenizer))\n","for item in emoji_list :\n","        tokenizer.add_tokens(item)\n","model.resize_token_embeddings(len(tokenizer))\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"],"metadata":{"id":"4AF07FBbCD7z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_function(examples):\n","    preprocessed = examples[\"text\"]\n","    return tokenizer(preprocessed, truncation=True)"],"metadata":{"id":"Xa58Rw0mCLWr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_metrics(eval_pred):\n","\n","    metric4 = evaluate.load(\"accuracy\")\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    accuracy = metric4.compute(predictions=predictions, references=labels)[\"accuracy\"]\n","    macro_f1 = (f1_score(labels, predictions, average =\"macro\"))\n","\n","    return {\"accuracy\":accuracy, \"macro_f1\":macro_f1}\n","\n"],"metadata":{"id":"Q_v-LhuZCcFx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["skf = StratifiedKFold(n_splits= k_fold, random_state = 31, shuffle=True)"],"metadata":{"id":"NrcJ1kDjCcIH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomTrainer(Trainer):\n","    def save_model(self, output_dir=None, _internal_call=False):\n","        if output_dir is None:\n","            output_dir = self.args.output_dir\n","\n","        self.model = self.model.to('cuda')\n","\n","        for param in self.model.parameters():\n","            if not param.is_contiguous():\n","                param.data = param.data.contiguous()\n","\n","        super().save_model(output_dir, _internal_call)"],"metadata":{"id":"6odU382jCcMH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's start training model\n","\n","for i, (train_index, test_index) in enumerate(skf.split(train_data.text , train_data.label)):\n","\n","    print(\"number of samples in train folds: \", len(train_index))\n","    print(\"number of samples in test fold  : \", len(test_index))\n","\n","    test = Dataset.from_pandas(train_data.iloc[test_index].set_index(\"text\"))\n","    train = Dataset.from_pandas(train_data.iloc[train_index].set_index(\"text\"))\n","    dataset_dict = DatasetDict({\"test\": test, \"train\": train})\n","    preprocessed_dataset_dict = dataset_dict.map(preprocess_function, batched=True)\n","\n","    training_args = TrainingArguments(\n","        \"Target Group Detection\",\n","        learning_rate=5e-6,\n","        per_device_train_batch_size=16,\n","        per_device_eval_batch_size=20,\n","        num_train_epochs=10,\n","        weight_decay=0.01,\n","        eval_strategy=\"steps\",\n","        metric_for_best_model = 'accuracy',\n","        logging_steps = 100,\n","        load_best_model_at_end=True,\n","        greater_is_better=True,\n","        #hub_strategy=\"end\",             # These parameter used when you want saved mmodels directly to HuggingFace,\n","        #push_to_hub=True,               # If you want to save model on huggingFace you must select \"True\"\n","        #hub_model_id= hub_id,           # Write your model address on HuggingFace\n","        #hub_private_repo= False,        # Determine whether the model on Hugging Face should be private or public.\n","        #hub_token= \"...........\",       # To save model into HuggingFace you need \"hub_token\" which can take from your own HuggingFace account\n","    )\n","\n","    trainer = CustomTrainer(\n","        model = model,\n","        args = training_args,\n","        train_dataset=preprocessed_dataset_dict[\"train\"],\n","        eval_dataset=preprocessed_dataset_dict[\"test\"],\n","        tokenizer = tokenizer,\n","        data_collator = data_collator,\n","        compute_metrics = compute_metrics,\n","\n","    )\n","    trainer.train()\n","    break\n","\n","#  trainer.save_model()    # If you want save your trained model on HuggingFace\n"],"metadata":{"id":"3CpmTzu5CcOj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's start testing model\n","\n","test_ds = Dataset.from_pandas(test_data).map(preprocess_function, batched=True)\n","test_ds"],"metadata":{"id":"Z8IQsYl5CcSc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = trainer.predict(test_ds).predictions\n","predictions"],"metadata":{"id":"yJ4878CpCcUv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","\n","y_pred = tf.argmax(predictions, axis=1).numpy()\n","y_test= np.array(test_data['label'])\n","\n","#print(title)\n","print(confusion_matrix(y_test, y_pred))\n","print('-------------------------------------------------------')\n","print('macro-f1:', f1_score(y_test, y_pred, average =\"macro\"))\n","print('accuracy:', f1_score(y_test, y_pred, average =\"micro\"))"],"metadata":{"id":"pc9-kPUnCcYg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report, f1_score\n","from sklearn.metrics import confusion_matrix\n","\n","con_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred, num_classes=num_class)\n","#print(title)\n","print(con_mat)\n","print('-------------------------------------------------------')\n","print(classification_report(y_test, y_pred, digits=num_class, zero_division=0))"],"metadata":{"id":"m4zshpTRC0pJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving trained model on local drive\n","# Please replace the pathes with your local address path\n","\n","if model_lang == 'tr':\n","          path_save_model = '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/trained_models/hs-tr-group/'\n","\n","if model_lang == 'ar':\n","          path_save_model = '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/trained_models/hs-ar-group/'\n","\n","\n","trainer.save_model(path_save_model)"],"metadata":{"id":"QECNxP9-Ccbj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load trained model from local drive\n","\n","from transformers import pipeline\n","\n","path_trained_model = path_save_model\n","model_pipline = pipeline(task = \"text-classification\", model =path_trained_model, device=\"cuda\")"],"metadata":{"id":"KMTmgSFJUVlZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predict the group of  example teewts using trained model\n","\n","def predict_group_tweet(review):\n","        prediction = []\n","        prediction = (model_pipline (review))\n","        prediction = prediction[0][\"label\"]\n","        return prediction"],"metadata":{"id":"PJx7BZgwC4O4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ğŸ”´ â–¶ **Disclaimer:**\n","\n","This study (project) includes examples of offensive language, hate speech, and profanity due to the nature of the research. These examples do not represent the authorsâ€™ views (or developers' views). The goal of this work is to contribute to the detection and prevention of harmful content and violence directed toward refugees, LGBTI+ individuals, religious groups, nationalities, and other vulnerable communities."],"metadata":{"id":"C1cDLOP6jrWs"}},{"cell_type":"code","source":["AR_TWEETS_EXAMPLES = [\n","    \"ÙƒÙ„ Ø§Ù„Ù„Ø§Ø¬Ø¦ÙŠÙ† Ù„ØµÙˆØµ\",\n","    \"Ø§Ø±Ø­Ù„ÙˆØ§ Ù…Ù† Ø¨Ù„Ø¯Ù†Ø§\",\n","    \"Ù…ÙƒØ§Ù† Ø§Ù„Ù…Ø±Ø£Ø© Ù‡Ùˆ Ø§Ù„Ù…Ø·Ø¨Ø®\",\n","    \"Ø§Ù„ÙŠÙˆÙ†Ø§Ù† ÙŠØ¬Ø¨ Ø£Ù† ØªØ¹Ø±Ù Ø­Ø¯ÙˆØ¯Ù‡Ø§ØŒ ÙˆØ¥Ù„Ø§ Ù‚Ø¯ Ù†Ø£ØªÙŠ ÙØ¬Ø£Ø© ÙÙŠ Ù„ÙŠÙ„Ø©\",\n","    \"Ù„Ø§ ÙŠÙ†Ø¨ØºÙŠ Ù„ØºÙŠØ± Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† Ø£Ù† ÙŠØµØ¨Ø­ÙˆØ§ Ù†ÙˆØ§Ø¨Ù‹Ø§.\",\n","    \"ÙŠØ¬Ø¨ Ø£Ù„Ø§ ÙŠÙØ³Ù…Ø­ Ù„Ù„Ø§Ø¬Ø¦ÙŠÙ† Ø¨Ø§Ù„ØªØµÙˆÙŠØª.\",\n","    \"ÙŠØ¬Ø¨ Ø£Ù„Ø§ ÙŠØ³ØªÙÙŠØ¯ Ø§Ù„Ù„Ø§Ø¬Ø¦ÙˆÙ† Ù…Ù† Ø§Ù„ØªØ£Ù…ÙŠÙ† Ø§Ù„ØµØ­ÙŠ.\",\n","    \"Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ±Ø¯ÙŠØ© ØºÙŠØ± Ù…Ù‚Ø¨ÙˆÙ„.\",\n","    \"Ù„Ù„Ø§Ø¬Ø¦ÙŠÙ† Ø£ÙŠØ¶Ù‹Ø§ Ø§Ù„Ø­Ù‚ ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø©.\",\n","    \"ÙŠØªÙ… Ø§ØªØ®Ø§Ø° ØªØ¯Ø§Ø¨ÙŠØ± Ø´Ø§Ù…Ù„Ø© Ø¶Ù…Ù† Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù„Ù…Ù†Ø¹ Ø£Ù†Ø´Ø·Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ…Ø§Øª Ø§Ù„Ø¥Ø±Ù‡Ø§Ø¨ÙŠØ©.\",\n","    \"ØªÙ… Ø§Ù„Ø¥Ø¹Ù„Ø§Ù† Ø¹Ù† ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„ÙØ¹Ø§Ù„ÙŠØ§Øª Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ© Ø§Ù„ØªÙŠ Ø³ØªÙÙ‚Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ø¹Ø§Ù… ÙÙŠ Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„.\",\n","    \"ØªÙØªØªØ­ Ø±Ø§Ø¦Ø¯Ø§Øª Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ ØµØ§Ù„ÙˆÙ†Ø§Øª ØªØ¬Ù…ÙŠÙ„ ØªØ³ØªÙ‡Ø¯Ù Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ù…Ù† Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† ÙˆØºÙŠØ± Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ†.\",\n","    \"ØªÙØ¬Ø±Ù‰ Ø§Ù„Ø§Ø³ØªØ¹Ø¯Ø§Ø¯Ø§Øª ÙÙŠ Ø§Ù„Ø£Ø³ÙˆØ§Ù‚ ÙÙŠ Ø³ÙˆØ±ÙŠØ§ Ù‚Ø¨Ù„ Ø§Ù„Ø£Ø¹ÙŠØ§Ø¯ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© ÙˆØ§Ù„Ù…Ø³ÙŠØ­ÙŠØ©.\",\n","    \"Ù‡Ù„ ÙƒÙˆÙ† Ø¨Ø¹Ø¶ Ø§Ù„ÙƒÙ„Ø§Ø¨ Ù…Ø³Ø¹ÙˆØ±Ø© Ø£Ùˆ Ø¹Ø¯ÙˆØ§Ù†ÙŠØ© ÙŠØ¨Ø±Ø± Ù‚ØªÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ø§Ø¨ØŸ Ø§Ø³Ø­Ø¨ÙˆØ§ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†.\",\n","     \"ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø³Ø§Ø¡ Ø¯Ø¹Ù… Ø¨Ø¹Ø¶Ù‡Ù† Ø§Ù„Ø¨Ø¹Ø¶\",\n","    \"Ø§Ø´ØªØ±ÙŠØª ØªÙØ§Ø­Ù‹Ø§ Ù…Ù† Ø§Ù„Ø³ÙˆÙ‚.\",\n","    \"Ø·Ø¹Ø§Ù… Ø§Ù„Ø³ÙˆØ±ÙŠÙŠÙ† Ù„Ø°ÙŠØ° Ø¬Ø¯Ù‹Ø§.\",\n","    \"Ø§Ù„Ø·Ø¹Ø§Ù… Ø§Ù„Ø³ÙˆØ±ÙŠ Ø´Ù‡ÙŠ Ù„Ù„ØºØ§ÙŠØ©.\",\n","    \"Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ø³ÙˆØ±ÙŠÙŠÙ† ÙƒØ«ÙŠØ±Ù‹Ø§ØŒ Ø¥Ù†Ù‡Ù… Ø£Ø´Ø®Ø§Øµ Ù„Ø·ÙŠÙÙˆÙ† Ø¬Ø¯Ù‹Ø§.\",\n","    \"Ø§Ù„Ø³Ù†ØºØ§ÙÙˆØ±ÙŠÙˆÙ† ÙŠØ¹Ù…Ù„ÙˆÙ† Ø¨Ø¬Ø¯.\",\n","    \"Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠÙŠÙ† ÙƒØ«ÙŠØ±Ù‹Ø§\",\n","    \"Ø£Ù†Ø§ Ù„Ø§ Ø£Ø­Ø¨ Ø§Ù„Ù…Ù„Ø­Ø¯ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚\",\n","    \"Ø§Ù„Ù…Ø³ÙŠØ­ÙŠÙŠÙ† Ø§Ù„Ø£Ø±Ø«ÙˆØ°ÙƒØ³\",\n","    \"Ø³Ù†ØºØ§ÙÙˆØ±Ø© Ø¨Ù„Ø¯ Ø¬Ù…ÙŠÙ„ Ø¬Ø¯Ù‹Ø§\"   ,\n","    \"Ø°Ù‡Ø¨Øª Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ Ø£Ù…Ø³ØŒ Ø«Ù… ØªÙ†Ø§ÙˆÙ„Øª ÙˆØ¬Ø¨Ø© Ø·Ø¹Ø§Ù… Ù„Ø°ÙŠØ°Ø©ØŒ Ø«Ù… Ø¹Ø¯Øª Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù†Ø²Ù„ØŒ ÙˆÙ„Ø¹Ø¨Øª Ù…Ø¹ Ø§Ø¨Ù†ØªÙŠØŒ ÙˆØ´Ø§Ù‡Ø¯Øª Ù…Ø³Ù„Ø³Ù„Ù‹Ø§ Ø£Ù…Ø±ÙŠÙƒÙŠÙ‹Ø§\",\n","    \"Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙƒØ«ÙŠØ±Ù‹Ø§\"\n","]\n","\n","TR_TWEETS_EXAMPLES = [\n","    \"mÃ¼ltecilerin hepsi hÄ±rsÄ±zdÄ±r\",\n","    \"defolun bizim Ã¼lkemizden\",\n","    \"kadÄ±nÄ±n yeri mutfaktÄ±r\",\n","    \"Yunanistan haddini bilecek yoksa bir gece ansizin gelebiliriz\",\n","    \"GayrimÃ¼slimler milletvekili olmasÄ±n.\",\n","    \"MÃ¼lteciler oy kullanmasÄ±n.\",\n","    \"MÃ¼ltecilere oy hakki verilmemeli\",\n","    \"MÃ¼lteciler sgk'dan yararlanmasÄ±n.\",\n","    \"KÃ¼rtÃ§e eÄŸitim kabul edilemez.\",\n","    \"MÃ¼ltecilerin de yaÅŸamaya hakkÄ± var.\",\n","    \"TerÃ¶r Ã¶rgÃ¼tlerinin faaliyetlerini Ã¶nlemek iÃ§in hukuki Ã§erÃ§evede kapsamlÄ± Ã¶nlemler alÄ±nmaktadÄ±r.\",\n","    \"Ä°srail'de bu yÄ±l dÃ¼zenlenecek kÃ¼ltÃ¼rel etkinliklerin tarihleri aÃ§Ä±klandÄ±\",\n","    \"KadÄ±n giriÅŸimciler, hem MÃ¼slÃ¼man hem de diÄŸer topluluklardan mÃ¼ÅŸterilere hitap eden gÃ¼zellik salonlarÄ± aÃ§Ä±yor.\",\n","    \"Suriyeâ€™deki pazarlarda MÃ¼slÃ¼man ve Hristiyan bayramlarÄ± Ã¶ncesinde hazÄ±rlÄ±klar yapÄ±lÄ±yor.\",\n","    \"BazÄ± kÃ¶peklerin kuduz ya da saldÄ±rgan olmasÄ±, tÃ¼m kÃ¶peklerin Ã¶ldÃ¼rÃ¼lmesi iÃ§in haklÄ± bir sebep midir? YasayÄ± geri Ã§ek.\",\n","    \"KadÄ±nlar birbirlerine destek olmalÄ±.\",\n","    \"Pazardan elma aldim.\",\n","    \"Suriyelilerin yemekleri Ã§ok gÃ¼zel.\",\n","    \"Suriye yemekleri Ã§ok lezzetli.\",\n","    \"SÃ¼riyelileri Ã§ok seviyorum, Ã§ok sempatik insanlar.\",\n","    \"Singapurlular Ã§ok Ã§alÄ±ÅŸkan\",\n","    \"AmerikalÄ±larÄ± Ã§ok Seviyorum\",\n","    \"Ateistleri hiÃ§ sevmiyorum\",\n","    \"Ortodoksalar hrÄ±stiyanlar\",\n","    \"Singapur Ã§ok gÃ¼zel bir Ã¼lke dir\",\n","    \"DÃ¼n hastaneye gittim sonra da bir gÃ¼zel yemek yedim, eve dÃ¶ndÃ¼m, kÄ±zÄ±mla oyun oynadÄ±m, Amerikan dizi izledim\",\n","    \"hayvanlari Ã§ok seviyorum\"\n","]\n","\n","\n","if model_lang == 'ar':\n","      TWEETS_EXAMPLES = AR_TWEETS_EXAMPLES\n","\n","if model_lang == 'tr':\n","      TWEETS_EXAMPLES = TR_TWEETS_EXAMPLES\n","\n","for tweet in TWEETS_EXAMPLES:\n","    group = predict_group_tweet(remove_punctuation_marks(tweet.lower()))\n","    print(f\"Tweet:    {tweet}\")\n","    print(f\"Group: {group}\")\n","    print(\"-\" * 60)\n"],"metadata":{"id":"5bAHVwV4C_C4"},"execution_count":null,"outputs":[]}]}