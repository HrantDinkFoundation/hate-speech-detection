{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNROifaUmsk7pUVFOO5GAoU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-NvmRLR2tvqJ"},"outputs":[],"source":["from google.colab import files\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","root_path = 'gdrive/My Drive/'\n","\n","import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","! nvcc --version\n","!nvidia-smi -L\n","device"]},{"cell_type":"code","source":["!pip install -U transformers\n","#!pip install transformers==4.28.0\n","!pip install datasets\n","!pip install snscrape\n","!pip install --upgrade accelerate\n","!pip install datasets evaluate\n","!pip install accelerate -U\n","\n","from scipy.spatial.distance import cosine\n","from transformers import AutoModel, AutoTokenizer\n","import numpy as np\n","import random\n","import csv\n","import pandas as pd\n","from sklearn.model_selection import StratifiedKFold\n","import tensorflow as tf\n","import datasets\n","from datasets import Dataset, DatasetDict\n","import evaluate\n","from sklearn.metrics import classification_report, f1_score\n","\n","import sys, os\n","import torch\n","import json\n","import re"],"metadata":{"id":"rvcFz_got2c7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# preprocess functions\n","\n","def remove_url(text):\n","    return ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\",\" \", text).split())\n","\n","def remove_username(text):\n","    return ' '.join(re.sub(\"([@][A-Za-z0-9_]+)\",\" \", text).split())\n","\n","def remove_at_mark(text):\n","    return re.sub(r'[@]', ' ', text)\n","\n","def remove_tag_mark(text):\n","    return re.sub(r'[#]', ' ', text)\n","\n","def replace_hashtags_with_segments (text):\n","   regex = \"#(\\w+)\"\n","   text_split = text.split()\n","   sent= ''\n","   strip_word =' '\n","   for word in text_split:\n","     hashtag= re.findall(regex, word)\n","     #print(hashtag)\n","     #print(strip_word)\n","\n","     if (len(hashtag)) == 0 :\n","        sent = sent + ' ' + word\n","     if (len(hashtag))!=0:\n","        strip_word = word.replace(('#'+hashtag[0]), ' ')\n","        #print (hashtag[0])\n","        #print(strip_word)\n","        try :\n","           key = '#'+ hashtag[0]\n","           segmented = segmented_dict [key]\n","           if type(segmented) == str:\n","                    #segmented = re.sub(r'[|]', ' ', segmented)\n","                    #segmented = re.sub(r'[,]', ' ', segmented)\n","                    sent= sent + ' ' + segmented\n","           if type(segmented) == list:\n","                    for k in segmented:\n","                      sent = sent +' ' + k\n","        except:\n","           sent = sent + ' ' +  hashtag[0]\n","   return sent + ' ' + strip_word\n","\n","def control_tag_mark(text):\n","     s = ' '\n","     for word in text.split():\n","            word = re.sub(r'[#]', ' #', word)\n","            s = s + ' ' + word\n","     return s\n","\n","def remove_punctuation_marks(text):\n","   punc = '''!()-[]{};:'\"\\,<>./?$%^&*~'''\n","   for i in text:\n","       if i in punc:\n","         text = text.replace(i, \" \")\n","   return text\n","\n","\n","\n","def remove_underline(text):\n","  punc = '''_'''\n","  for i in text:\n","       if i in punc:\n","         text = text.replace(i, \"\")\n","  return text\n","\n","def remove_hashtag(text):\n","    return ' '.join(re.sub(\"([#][A-Za-z0-9_]+)\",\" \", text).split())\n","\n","def remove_turkish_hashtag(text):\n","    return ' '.join(re.sub(\"([#][\\w+_]+)\",\" \", text).split())\n","\n","def find_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    return re.findall(emoji_pattern, text) # no emoji\n","\n","def remove_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    return (emoji_pattern.sub(r'', text)) # no emoji\n","\n","def remove_arabic_hashtags(text):\n","   return ' '.join(re.sub('([#][\\u0600-\\u06ff_]+)', \" \", text).split())\n","\n","def remove_emoji_duplicate (text):\n","      emoji =[]\n","      rest = ''\n","      main_text = []\n","      for w in text :\n","         x = find_emoji(w)\n","         if len (x) != 0 :\n","            emoji.append (x[0])\n","      #print(emoji)\n","      rest = remove_emoji(text)\n","      emoji = list(set(emoji))\n","      for e in emoji :\n","        try:\n","          rest = rest + \" \" + e\n","        except:\n","          rest = rest\n","      main_text.append(rest)\n","      return main_text[0]\n","\n","def lower_case(text):\n","    text = text.replace(\"I\", \"ı\")\n","    text = text.replace(\"İ\", \"i\")\n","    return text.lower()"],"metadata":{"id":"GmwJIRixuBDc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def seed_everything(seed):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","seed_everything(0)"],"metadata":{"id":"nxL6VBYOuBFm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define data path\n","# Please replace the addresses with the address of dataset on your local drive\n","\n","path_turkish_data     =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/df_turkish_cats_targets_degrees_final_with_translated_all.csv'\n","path_arabic_data      =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/df_arabic_cats_targets_degrees_final.csv'\n","path_synthetic_data   =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/synthetic_data_all_new.xlsx'\n","\n","path_all_emoji =         '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/all_emoji_dict.json'\n","path_hashtag_segment  =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/all_hashtag_segmented_tr.json'"],"metadata":{"id":"oQwMXxin08oe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Laod emoji_dict and hashtag_dict\n","\n","f = open(path_all_emoji)\n","data = json.load(f)\n","emoji_list = data['emojis']\n","\n","f = open(path_hashtag_segment)\n","segmented_dict = json.load(f)"],"metadata":{"id":"79D1SFy2uBJR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select which preprocess steps must be apply on data\n","\n","def pre_process_tweets(text_list):\n","   cleaned_text =[]\n","   for s in text_list:\n","       #s = lower_case(s)\n","       s = s.lower()\n","       s = remove_username(s)\n","       s = remove_at_mark(s)\n","       s = remove_url(s)\n","       s = remove_tag_mark(s)\n","       #s = control_tag_mark(s)\n","       s = remove_punctuation_marks(s)\n","       s = remove_underline(s)\n","       #s = replace_hashtags_with_segments(s)\n","       #s = remove_tag_mark(s)\n","       cleaned_text.append(s)\n","   return cleaned_text"],"metadata":{"id":"nZrS0AFJt2e7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load synthetic data\n","\n","df_syn = pd.read_excel(path_synthetic_data)\n","\n","df_syn = df_syn.iloc[:,[4 , 9, 2]]\n","\n","df_syn= df_syn.sample(frac = 1)\n","\n","print(df_syn['Group'].value_counts())\n","\n","\n","df_syn\n"],"metadata":{"id":"ydHs8oe7utny"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load training data\n","\n","df_turkish = pd.read_csv(path_turkish_data)\n","df_turkish = df_turkish.iloc[:,[2, 1, 38,39]]\n","\n","print(df_turkish['Group'].value_counts())\n","\n","\n","df_turkish"],"metadata":{"id":"oqG2z09ayjgt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load testing data\n","\n","df_arabic = pd.read_csv(path_arabic_data)\n","\n","# Becasue for arabic dataset the group is \"mülteci karşıtı\"\n","\n","df_arabic.rename(columns={'Group': 'keyword'}, inplace=True)\n","hedef_group = [\"Refugees\"] * len(df_arabic)\n","df_arabic['Group'] = hedef_group\n","\n","df_arabic = df_arabic.iloc[:,[2, 39, 38]]\n","print(df_arabic['Group'].value_counts())\n","\n","df_arabic"],"metadata":{"id":"OwWF2-_uy-4B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# select the number of classes\n","# select k-fold for spliting train and validation data\n","\n","num_class = 11\n","k_fold = 10"],"metadata":{"id":"wZfLq6AMBT_Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_mapping = { 'LABEL_0': 'No-group',\n","                  'LABEL_1': 'Refugees',\n","                  'LABEL_2': 'Israil-Yahudi',\n","                  'LABEL_3': 'Greeks',\n","                  'LABEL_4': 'Armenian',\n","                  'LABEL_5': 'Alevi',\n","                  'LABEL_6': 'Kurdish',\n","                  'LABEL_7': 'Arabian',\n","                  'LABEL_8': 'LGBTI+',\n","                  'LABEL_9': 'Women',\n","                  'LABEL_10': 'Other groups'\n","\n","                 }\n","\n","labels = [ 'No-group',\n","           'Refugees',\n","           'Jews',\n","           'Greeks',\n","           'Armenians',\n","           'Alevis',\n","           'Kurds',\n","           'Arabs',\n","           'LGBTI+s',\n","           'Women',\n","           'Other-groups'\n","                  ]\n","\n"],"metadata":{"id":"swO9JNgv2OAC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the name and the address of trained model on HuggingFace\n","\n","hub_id = 'HrantDinkFoundation/'\n","model_id = 'hs-group-prediction'"],"metadata":{"id":"qy93o-1GCl8X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# select which data used in training\n","\n","# 'only-tr' : use only Turkish data\n","# 'only-ar' : use only arabic data\n","# 'tr-syn'  : use Turkish and synthetic data\n","# 'ar-syn'  : use arabic and synthetic data\n","# 'ar-translated-syn'  : use arabic, synthetic and the translated column of Turkish data\n","\n","mode_train = ['only-tr' , 'only-ar' , 'tr-syn',  'ar-syn', 'ar-translated-syn']\n","mode = mode_train [2]"],"metadata":{"id":"urRqL8N1CDbQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if mode == 'only-tr':\n","    model_lang = 'tr'\n","    model_name = 'dbmdz/bert-base-turkish-uncased'\n","    hub_id = hub_id + 'turkish-' + model_id\n","\n","    df_train = df_turkish[(df_turkish.Div=='train')]\n","    df_test  = df_turkish[(df_turkish.Div=='test')]\n","\n","\n","if mode == 'only-ar':\n","    model_lang = 'ar'\n","    model_name = 'aubmindlab/bert-base-arabert'\n","    hub_id = hub_id + 'arabic-' + model_id\n","\n","    df_test  = df_arabic[(df_arabic.Div=='test')]\n","    df_train = df_arabic[(df_arabic.Div=='train')]\n","\n","\n","if mode == 'tr-syn':\n","    model_lang = 'tr'\n","    model_name = 'dbmdz/bert-base-turkish-uncased'\n","    hub_id = hub_id + 'turkish-' + model_id\n","    df_train = df_turkish[(df_turkish.Div=='train')]\n","    df_test  = df_turkish[(df_turkish.Div=='test')]\n","\n","    df_train = pd.concat([df_train, df_syn], ignore_index=True)\n","\n","\n","if mode == 'ar-syn':\n","    model_lang = 'ar'\n","    model_name = 'aubmindlab/bert-base-arabert'\n","    hub_id = hub_id + 'arabic-' + model_id\n","\n","\n","    df_syn = df_syn.drop (columns=['Text'])\n","    df_syn.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","\n","    df_test  = df_arabic[(df_arabic.Div=='test')]\n","    df_train = df_arabic[(df_arabic.Div=='train')]\n","\n","    df_train = pd.concat([df_train, df_syn], ignore_index=True)\n","\n","\n","if mode == 'ar-translated-syn' :\n","     model_lang = 'ar'\n","     model_name = 'aubmindlab/bert-base-arabert'\n","     hub_id = hub_id + 'arabic-' + model_id\n","\n","     df_turkish = df_turkish.drop (columns=['Text'])\n","     df_turkish.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","\n","     df_syn = df_syn.drop (columns=['Text'])\n","     df_syn.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","\n","     df_train = df_arabic[(df_arabic.Div=='train')]\n","     df_test  = df_arabic[(df_arabic.Div=='test')]\n","\n","     df_train = pd.concat([df_train, df_turkish, df_syn], ignore_index=True)\n","\n","\n","\n","df_train= df_train.iloc[:,[0, 1, 2]]\n","df_test= df_test.iloc[:,[0, 1, 2]]\n"],"metadata":{"id":"ajYHBvYE2OfK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train"],"metadata":{"id":"m2kofP4xQ95D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test"],"metadata":{"id":"Y5jxHSr6RAo4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# different labeling processes create different naming for same groups, so here we convert them to the uniqe integer class\n","\n","group_mapping = {'No-group': 0 ,\n","                 'mülteci' : 1 , 'refugee':1 , 'Refugee' : 1, 'Refugees' : 1,\n","                 'Isr-Pal':2, 'Isr-pal':2, 'Jews':2,\n","                 'Tr-Gr': 3 , 'tr-gr': 3, 'Greeks': 3,\n","                 'Armenian': 4, 'Armeni':4,\n","                 'Alevi': 5,\n","                 'Kurdish': 6, 'Kurd': 6,\n","                 'Arabian':7 ,\n","                 'LGBTI+': 8 ,\n","                 'Women':9 ,\n","                 'Other groups': 10 }"],"metadata":{"id":"iEyYdlie9Knl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_group_class(inp):\n","    return(group_mapping[inp])\n","\n","df_train['label'] = df_train.Group.apply(extract_group_class)\n","\n","df_test['label'] = df_test.Group.apply(extract_group_class)\n","\n","\n","print(df_train['label'].value_counts())\n","print(df_test['label'].value_counts())\n","\n","df_train"],"metadata":{"id":"KjEqLuBP-vz2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# preprocess train data\n","import ast\n","\n","text = df_train['Text'].tolist()\n","label = df_train['label'].tolist()\n","\n","\n","text = pre_process_tweets(text)\n","train_data = pd.DataFrame(list(zip(text, label)), columns =['text', 'label'])\n","train_data= train_data.astype({'label':'int'})\n","\n","\n","print(train_data['label'].value_counts())\n","train_data\n"],"metadata":{"id":"uE1zXZjjBx9I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# preprocess test data\n","\n","text = df_test['Text'].tolist()\n","label = df_test['label'].tolist()\n","\n","text = pre_process_tweets(text)\n","test_data = pd.DataFrame(list(zip(text, label)), columns =['text', 'label'])\n","test_data= test_data.astype({'label':'int'})\n","\n","\n","\n","print(test_data['label'].value_counts())\n","test_data\n"],"metadata":{"id":"zmYzw60UB86W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Laod BERT model\n","\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from transformers import ElectraForSequenceClassification,  ElectraTokenizerFast\n","from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaTokenizerFast, RobertaForSequenceClassification\n","from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments, GPT2Config\n","\n","id2label = {idx:label for idx, label in enumerate(labels)}\n","label2id = {label:idx for idx, label in enumerate(labels)}\n","\n","pretrained_model_name = model_name\n","tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, do_lower_case=True, force_download=True)\n","model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name, num_labels = num_class, force_download=True , id2label=id2label, label2id=label2id).to(device)   #\n","model.resize_token_embeddings(len(tokenizer))\n","for item in emoji_list :\n","        tokenizer.add_tokens(item)\n","model.resize_token_embeddings(len(tokenizer))\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"],"metadata":{"id":"4AF07FBbCD7z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_function(examples):\n","    preprocessed = examples[\"text\"]\n","    return tokenizer(preprocessed, truncation=True)"],"metadata":{"id":"Xa58Rw0mCLWr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_metrics(eval_pred):\n","\n","    metric4 = evaluate.load(\"accuracy\")\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    accuracy = metric4.compute(predictions=predictions, references=labels)[\"accuracy\"]\n","    macro_f1 = (f1_score(labels, predictions, average =\"macro\"))\n","\n","    return {\"accuracy\":accuracy, \"macro_f1\":macro_f1}\n","\n"],"metadata":{"id":"Q_v-LhuZCcFx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["skf = StratifiedKFold(n_splits= k_fold, random_state = 31, shuffle=True)"],"metadata":{"id":"NrcJ1kDjCcIH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomTrainer(Trainer):\n","    def save_model(self, output_dir=None, _internal_call=False):\n","        if output_dir is None:\n","            output_dir = self.args.output_dir\n","\n","        self.model = self.model.to('cuda')\n","\n","        for param in self.model.parameters():\n","            if not param.is_contiguous():\n","                param.data = param.data.contiguous()\n","\n","        super().save_model(output_dir, _internal_call)"],"metadata":{"id":"6odU382jCcMH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's start training model\n","\n","for i, (train_index, test_index) in enumerate(skf.split(train_data.text , train_data.label)):\n","\n","    print(\"number of samples in train folds: \", len(train_index))\n","    print(\"number of samples in test fold  : \", len(test_index))\n","\n","    test = Dataset.from_pandas(train_data.iloc[test_index].set_index(\"text\"))\n","    train = Dataset.from_pandas(train_data.iloc[train_index].set_index(\"text\"))\n","    dataset_dict = DatasetDict({\"test\": test, \"train\": train})\n","    preprocessed_dataset_dict = dataset_dict.map(preprocess_function, batched=True)\n","\n","    training_args = TrainingArguments(\n","        \"Target Group Detection\",\n","        learning_rate=5e-6,\n","        per_device_train_batch_size=16,\n","        per_device_eval_batch_size=20,\n","        num_train_epochs=10,\n","        weight_decay=0.01,\n","        eval_strategy=\"steps\",\n","        metric_for_best_model = 'accuracy',\n","        logging_steps = 100,\n","        load_best_model_at_end=True,\n","        greater_is_better=True,\n","        #hub_strategy=\"end\",             # These parameter used when you want saved mmodels directly to HuggingFace,\n","        #push_to_hub=True,               # If you want to save model on huggingFace you must select \"True\"\n","        #hub_model_id= hub_id,           # Write your model address on HuggingFace\n","        #hub_private_repo= False,        # Determine whether the model on Hugging Face should be private or public.\n","        #hub_token= \"...........\",       # To save model into HuggingFace you need \"hub_token\" which can take from your own HuggingFace account\n","    )\n","\n","    trainer = CustomTrainer(\n","        model = model,\n","        args = training_args,\n","        train_dataset=preprocessed_dataset_dict[\"train\"],\n","        eval_dataset=preprocessed_dataset_dict[\"test\"],\n","        tokenizer = tokenizer,\n","        data_collator = data_collator,\n","        compute_metrics = compute_metrics,\n","\n","    )\n","    trainer.train()\n","    break\n","\n","#  trainer.save_model()    # If you want save your trained model on HuggingFace\n"],"metadata":{"id":"3CpmTzu5CcOj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's start testing model\n","\n","test_ds = Dataset.from_pandas(test_data).map(preprocess_function, batched=True)\n","test_ds"],"metadata":{"id":"Z8IQsYl5CcSc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = trainer.predict(test_ds).predictions\n","predictions"],"metadata":{"id":"yJ4878CpCcUv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","\n","y_pred = tf.argmax(predictions, axis=1).numpy()\n","y_test= np.array(test_data['label'])\n","\n","#print(title)\n","print(confusion_matrix(y_test, y_pred))\n","print('-------------------------------------------------------')\n","print('macro-f1:', f1_score(y_test, y_pred, average =\"macro\"))\n","print('accuracy:', f1_score(y_test, y_pred, average =\"micro\"))"],"metadata":{"id":"pc9-kPUnCcYg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report, f1_score\n","from sklearn.metrics import confusion_matrix\n","\n","con_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred, num_classes=num_class)\n","#print(title)\n","print(con_mat)\n","print('-------------------------------------------------------')\n","print(classification_report(y_test, y_pred, digits=num_class, zero_division=0))"],"metadata":{"id":"m4zshpTRC0pJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving trained model on local drive\n","# Please replace the pathes with your local address path\n","\n","if model_lang == 'tr':\n","          path_save_model = '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/trained_models/hs-tr-group/'\n","\n","if model_lang == 'ar':\n","          path_save_model = '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/trained_models/hs-ar-group/'\n","\n","\n","trainer.save_model(path_save_model)"],"metadata":{"id":"QECNxP9-Ccbj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load trained model from local drive\n","\n","from transformers import pipeline\n","\n","path_trained_model = path_save_model\n","model_pipline = pipeline(task = \"text-classification\", model =path_trained_model, device=\"cuda\")"],"metadata":{"id":"KMTmgSFJUVlZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predict the group of  example teewts using trained model\n","\n","def predict_group_tweet(review):\n","        prediction = []\n","        prediction = (model_pipline (review))\n","        prediction = prediction[0][\"label\"]\n","        return prediction"],"metadata":{"id":"PJx7BZgwC4O4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["🔴 ▶ **Disclaimer:**\n","\n","This study (project) includes examples of offensive language, hate speech, and profanity due to the nature of the research. These examples do not represent the authors’ views (or developers' views). The goal of this work is to contribute to the detection and prevention of harmful content and violence directed toward refugees, LGBTI+ individuals, religious groups, nationalities, and other vulnerable communities."],"metadata":{"id":"C1cDLOP6jrWs"}},{"cell_type":"code","source":["AR_TWEETS_EXAMPLES = [\n","    \"كل اللاجئين لصوص\",\n","    \"ارحلوا من بلدنا\",\n","    \"مكان المرأة هو المطبخ\",\n","    \"اليونان يجب أن تعرف حدودها، وإلا قد نأتي فجأة في ليلة\",\n","    \"لا ينبغي لغير المسلمين أن يصبحوا نوابًا.\",\n","    \"يجب ألا يُسمح للاجئين بالتصويت.\",\n","    \"يجب ألا يستفيد اللاجئون من التأمين الصحي.\",\n","    \"التعليم باللغة الكردية غير مقبول.\",\n","    \"للاجئين أيضًا الحق في الحياة.\",\n","    \"يتم اتخاذ تدابير شاملة ضمن الإطار القانوني لمنع أنشطة التنظيمات الإرهابية.\",\n","    \"تم الإعلان عن تواريخ الفعاليات الثقافية التي ستُقام هذا العام في إسرائيل.\",\n","    \"تفتتح رائدات الأعمال صالونات تجميل تستهدف العملاء من المسلمين وغير المسلمين.\",\n","    \"تُجرى الاستعدادات في الأسواق في سوريا قبل الأعياد الإسلامية والمسيحية.\",\n","    \"هل كون بعض الكلاب مسعورة أو عدوانية يبرر قتل جميع الكلاب؟ اسحبوا القانون.\",\n","     \"يجب على النساء دعم بعضهن البعض\",\n","    \"اشتريت تفاحًا من السوق.\",\n","    \"طعام السوريين لذيذ جدًا.\",\n","    \"الطعام السوري شهي للغاية.\",\n","    \"أنا أحب السوريين كثيرًا، إنهم أشخاص لطيفون جدًا.\",\n","    \"السنغافوريون يعملون بجد.\",\n","    \"أنا أحب الأمريكيين كثيرًا\",\n","    \"أنا لا أحب الملحدين على الإطلاق\",\n","    \"المسيحيين الأرثوذكس\",\n","    \"سنغافورة بلد جميل جدًا\"   ,\n","    \"ذهبت إلى المستشفى أمس، ثم تناولت وجبة طعام لذيذة، ثم عدت إلى المنزل، ولعبت مع ابنتي، وشاهدت مسلسلًا أمريكيًا\",\n","    \"أنا أحب الحيوانات كثيرًا\"\n","]\n","\n","TR_TWEETS_EXAMPLES = [\n","    \"mültecilerin hepsi hırsızdır\",\n","    \"defolun bizim ülkemizden\",\n","    \"kadının yeri mutfaktır\",\n","    \"Yunanistan haddini bilecek yoksa bir gece ansizin gelebiliriz\",\n","    \"Gayrimüslimler milletvekili olmasın.\",\n","    \"Mülteciler oy kullanmasın.\",\n","    \"Mültecilere oy hakki verilmemeli\",\n","    \"Mülteciler sgk'dan yararlanmasın.\",\n","    \"Kürtçe eğitim kabul edilemez.\",\n","    \"Mültecilerin de yaşamaya hakkı var.\",\n","    \"Terör örgütlerinin faaliyetlerini önlemek için hukuki çerçevede kapsamlı önlemler alınmaktadır.\",\n","    \"İsrail'de bu yıl düzenlenecek kültürel etkinliklerin tarihleri açıklandı\",\n","    \"Kadın girişimciler, hem Müslüman hem de diğer topluluklardan müşterilere hitap eden güzellik salonları açıyor.\",\n","    \"Suriye’deki pazarlarda Müslüman ve Hristiyan bayramları öncesinde hazırlıklar yapılıyor.\",\n","    \"Bazı köpeklerin kuduz ya da saldırgan olması, tüm köpeklerin öldürülmesi için haklı bir sebep midir? Yasayı geri çek.\",\n","    \"Kadınlar birbirlerine destek olmalı.\",\n","    \"Pazardan elma aldim.\",\n","    \"Suriyelilerin yemekleri çok güzel.\",\n","    \"Suriye yemekleri çok lezzetli.\",\n","    \"Süriyelileri çok seviyorum, çok sempatik insanlar.\",\n","    \"Singapurlular çok çalışkan\",\n","    \"Amerikalıları çok Seviyorum\",\n","    \"Ateistleri hiç sevmiyorum\",\n","    \"Ortodoksalar hrıstiyanlar\",\n","    \"Singapur çok güzel bir ülke dir\",\n","    \"Dün hastaneye gittim sonra da bir güzel yemek yedim, eve döndüm, kızımla oyun oynadım, Amerikan dizi izledim\",\n","    \"hayvanlari çok seviyorum\"\n","]\n","\n","\n","if model_lang == 'ar':\n","      TWEETS_EXAMPLES = AR_TWEETS_EXAMPLES\n","\n","if model_lang == 'tr':\n","      TWEETS_EXAMPLES = TR_TWEETS_EXAMPLES\n","\n","for tweet in TWEETS_EXAMPLES:\n","    group = predict_group_tweet(remove_punctuation_marks(tweet.lower()))\n","    print(f\"Tweet:    {tweet}\")\n","    print(f\"Group: {group}\")\n","    print(\"-\" * 60)\n"],"metadata":{"id":"5bAHVwV4C_C4"},"execution_count":null,"outputs":[]}]}