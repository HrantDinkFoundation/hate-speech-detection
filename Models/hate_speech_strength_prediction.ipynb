{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wFXTGNTSP-K0"},"outputs":[],"source":["from google.colab import files\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","root_path = 'gdrive/My Drive/'\n","\n","import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","! nvcc --version\n","!nvidia-smi -L\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAAU3hinHKRQ"},"outputs":[],"source":["!pip install -U transformers\n","#!pip install transformers==4.28.0\n","!pip install transformers==4.45.2\n","!pip install datasets\n","!pip install snscrape\n","!pip install --upgrade accelerate\n","!pip install datasets evaluate\n","!pip install accelerate -U\n","\n","from scipy.spatial.distance import cosine\n","from transformers import AutoModel, AutoTokenizer\n","import numpy as np\n","import random\n","import csv\n","import pandas as pd\n","from sklearn.model_selection import StratifiedKFold\n","import tensorflow as tf\n","import datasets\n","from datasets import Dataset, DatasetDict\n","import evaluate\n","from sklearn.metrics import classification_report, f1_score\n","\n","import sys, os\n","import torch\n","import json\n","import re\n"]},{"cell_type":"code","source":["# preprocess functions\n","\n","def remove_url(text):\n","    return ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\",\" \", text).split())\n","\n","def remove_username(text):\n","    return ' '.join(re.sub(\"([@][A-Za-z0-9_]+)\",\" \", text).split())\n","\n","def remove_at_mark(text):\n","    return re.sub(r'[@]', ' ', text)\n","\n","def remove_tag_mark(text):\n","    return re.sub(r'[#]', ' ', text)\n","\n","def replace_hashtags_with_segments (text):\n","   regex = \"#(\\w+)\"\n","   text_split = text.split()\n","   sent= ''\n","   strip_word =' '\n","   for word in text_split:\n","     hashtag= re.findall(regex, word)\n","     #print(hashtag)\n","     #print(strip_word)\n","\n","     if (len(hashtag)) == 0 :\n","        sent = sent + ' ' + word\n","     if (len(hashtag))!=0:\n","        strip_word = word.replace(('#'+hashtag[0]), ' ')\n","        #print (hashtag[0])\n","        #print(strip_word)\n","        try :\n","           key = '#'+ hashtag[0]\n","           segmented = segmented_dict [key]\n","           if type(segmented) == str:\n","                    #segmented = re.sub(r'[|]', ' ', segmented)\n","                    #segmented = re.sub(r'[,]', ' ', segmented)\n","                    sent= sent + ' ' + segmented\n","           if type(segmented) == list:\n","                    for k in segmented:\n","                      sent = sent +' ' + k\n","        except:\n","           sent = sent + ' ' +  hashtag[0]\n","   return sent + ' ' + strip_word\n","\n","def control_tag_mark(text):\n","     s = ' '\n","     for word in text.split():\n","            word = re.sub(r'[#]', ' #', word)\n","            s = s + ' ' + word\n","     return s\n","\n","def remove_punctuation_marks(text):\n","   punc = '''!()-[]{};:'\"\\,<>./?$%^&*~'''\n","   for i in text:\n","       if i in punc:\n","         text = text.replace(i, \" \")\n","   return text\n","\n","\n","\n","def remove_underline(text):\n","  punc = '''_'''\n","  for i in text:\n","       if i in punc:\n","         text = text.replace(i, \"\")\n","  return text\n","\n","def remove_hashtag(text):\n","    return ' '.join(re.sub(\"([#][A-Za-z0-9_]+)\",\" \", text).split())\n","\n","def remove_turkish_hashtag(text):\n","    return ' '.join(re.sub(\"([#][\\w+_]+)\",\" \", text).split())\n","\n","def find_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    return re.findall(emoji_pattern, text) # no emoji\n","\n","def remove_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    return (emoji_pattern.sub(r'', text)) # no emoji\n","\n","def remove_arabic_hashtags(text):\n","   return ' '.join(re.sub('([#][\\u0600-\\u06ff_]+)', \" \", text).split())\n","\n","def remove_emoji_duplicate (text):\n","      emoji =[]\n","      rest = ''\n","      main_text = []\n","      for w in text :\n","         x = find_emoji(w)\n","         if len (x) != 0 :\n","            emoji.append (x[0])\n","      #print(emoji)\n","      rest = remove_emoji(text)\n","      emoji = list(set(emoji))\n","      for e in emoji :\n","        try:\n","          rest = rest + \" \" + e\n","        except:\n","          rest = rest\n","      main_text.append(rest)\n","      return main_text[0]\n","\n","def lower_case(text):\n","    text = text.replace(\"I\", \"ı\")\n","    text = text.replace(\"İ\", \"i\")\n","    return text.lower()"],"metadata":{"id":"9vyLbRzuVICq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def seed_everything(seed):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","seed_everything(0)"],"metadata":{"id":"pAAVod3AVjod"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define data path\n","# Please replace the addresses with the address of dataset on your local drive\n","\n","path_turkish_data     =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/df_turkish_cats_targets_degrees_final_with_translated_all.csv'\n","path_arabic_data      =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/df_arabic_cats_targets_degrees_final.csv'\n","path_synthetic_data   =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/synthetic_data_all_new.xlsx'\n","\n","path_all_emoji =         '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/all_emoji_dict.json'\n","path_hashtag_segment  =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/all_hashtag_segmented_tr.json'"],"metadata":{"id":"MXbwhXVtk8M2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Laod emoji_dict and hashtag_dict\n","\n","f = open(path_all_emoji)\n","data = json.load(f)\n","emoji_list = data['emojis']\n","\n","f = open(path_hashtag_segment)\n","segmented_dict = json.load(f)"],"metadata":{"id":"Gn-0qMMmVyjS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select which preprocess steps must be apply on data\n","\n","def pre_process_tweets(text_list):\n","   cleaned_text =[]\n","   for s in text_list:\n","       #s = lower_case(s)  #Umut\n","       s = s.lower()\n","       s = remove_username(s)\n","       s = remove_at_mark(s)\n","       s = remove_url(s)\n","       s = remove_tag_mark(s)\n","       #s = control_tag_mark(s)  #\n","       s = remove_punctuation_marks(s)\n","       s = remove_underline(s)  #\n","       #s = replace_hashtags_with_segments(s) #\n","       #s = remove_tag_mark(s)\n","       cleaned_text.append(s)\n","   return cleaned_text"],"metadata":{"id":"GbINt_v7XOKE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Load synthetic data\n","\n","df_syn = pd.read_excel(path_synthetic_data)\n","df_syn = df_syn.iloc[:,[4, 10, 9,]]\n","\n","print(df_syn['DegreeMean'].value_counts())\n","df_syn= df_syn.sample(frac = 1)\n","\n","df_syn"],"metadata":{"id":"3ZlH3j_avBwC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load Turkish data\n","\n","df_turkish = pd.read_csv(path_turkish_data)\n","df_turkish = df_turkish.iloc[:,[2, 37,38, 39]]\n","\n","df_turkish"],"metadata":{"id":"y3HBA7XJvwx4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Load Arabic data\n","\n","df_arabic = pd.read_csv(path_arabic_data)\n","df_arabic = df_arabic.iloc[:,[2,37,38]]\n","\n","df_arabic"],"metadata":{"id":"qOSLlHxvw1J1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# select number of classes\n","# select k-fold for spliting train and validation data\n","\n","num_class = 1\n","k_fold = 11\n"],"metadata":{"id":"N0-_R31GqByY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select strength column\n","# Define the name and the address of trained model on HuggingFace\n","\n","if num_class == 1:\n","     df_turkish.dropna(subset=['DegreeMean'], inplace=True)\n","     df_arabic.dropna(subset=['DegreeMean'], inplace=True)\n","     hub_id = 'HrantDinkFoundation/'\n","     model_id = 'hs-degree-prediction'\n","     column_select = 'DegreeMean'"],"metadata":{"id":"XUT08gwueOnI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# select which data used in training\n","\n","# 'only-tr' : use only Turkish data\n","# 'only-ar' : use only arabic data\n","# 'tr-syn'  : use Turkish and synthetic data\n","# 'ar-syn'  : use arabic and synthetic data\n","# 'ar-translated-syn'  : use arabic, synthetic and the translated column of Turkish data\n","\n","mode_train = ['only-tr', 'only-ar', 'tr-syn' , 'ar-syn', 'ar-translated-syn']\n","mode = mode_train [2]"],"metadata":{"id":"RV_pZFVMm6M1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if mode =='only-tr':\n","    model_lang = 'tr'\n","    model_name = 'dbmdz/bert-base-turkish-uncased'\n","    df_train = df_turkish[(df_turkish.Div=='train')]\n","    df_test  = df_turkish[(df_turkish.Div=='test')]\n","\n","if mode =='only-ar' :\n","     model_lang = 'ar'\n","     model_name = 'aubmindlab/bert-base-arabert'\n","     df_train = df_arabic[(df_arabic.Div=='train')]\n","     df_test  = df_arabic[(df_arabic.Div=='test')]\n","\n","\n","if mode == 'tr-syn':\n","    model_lang = 'tr'\n","    hub_id = hub_id + 'turkish-' + model_id\n","    model_name = 'dbmdz/bert-base-turkish-uncased'\n","    df_train = df_turkish[(df_turkish.Div=='train')]\n","    df_test  = df_turkish[(df_turkish.Div=='test')]\n","\n","    df_train = pd.concat([df_train, df_syn], ignore_index=True)\n","\n","\n","if mode == 'ar-syn':\n","    model_lang = 'ar'\n","    hub_id = hub_id + 'arabic-' + model_id\n","    model_name = 'aubmindlab/bert-base-arabert'\n","    df_train = df_arabic[(df_arabic.Div=='train')]\n","    df_test  = df_arabic[(df_arabic.Div=='test')]\n","\n","    df_syn = df_syn.drop (columns=['Text'])\n","    df_syn.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","    df_train = pd.concat([df_train, df_syn], ignore_index=True)\n","\n","\n","if mode == 'ar-translated-syn':\n","    model_lang = 'ar'\n","    model_name = 'aubmindlab/bert-base-arabert'\n","    hub_id = hub_id + 'arabic-' + model_id\n","\n","    df_turkish = df_turkish.drop (columns=['Text'])\n","    df_turkish.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","\n","    df_train_arabic = df_arabic[(df_arabic.Div=='train')]\n","    df_test  = df_arabic[(df_arabic.Div=='test')]\n","\n","    df_syn = df_syn.drop (columns=['Text'])\n","    df_syn.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","\n","    df_train = pd.concat([df_train_arabic, df_turkish, df_syn], ignore_index=True)\n"],"metadata":{"id":"Z-pbvvDE1ZeJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train\n"],"metadata":{"id":"5OtGrwB0qgsx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test"],"metadata":{"id":"_hAfQvHmneZv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pgeiz9nGnKQB"},"outputs":[],"source":["# preprocess train data\n","\n","import ast\n","\n","text = df_train['Text'].tolist()\n","label = df_train[column_select].tolist()\n","\n","text = pre_process_tweets(text)\n","train_data = pd.DataFrame(list(zip(text, label)), columns =['text', 'label'])\n","\n","train_data= train_data.astype({'label':'float'})\n","\n","train_data\n"]},{"cell_type":"code","source":["# preprocess test data\n","\n","text = df_test['Text'].tolist()\n","label = df_test[column_select].tolist()\n","\n","text = pre_process_tweets(text)\n","test_data = pd.DataFrame(list(zip(text, label)), columns =['text', 'label'])\n","test_data= test_data.astype({'label':'float'})\n","\n","\n","test_data\n"],"metadata":{"id":"yoX6nmbsj0wB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Laod BERT model\n","\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from transformers import ElectraForSequenceClassification,  ElectraTokenizerFast\n","from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaTokenizerFast, RobertaForSequenceClassification\n","from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments, GPT2Config\n","\n","pretrained_model_name = model_name\n","tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, do_lower_case=True, force_download=True)\n","model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name, num_labels = num_class, force_download=True ).to(device)   # id2label=id2label, label2id=label2id\n","model.resize_token_embeddings(len(tokenizer))\n","for item in emoji_list :\n","        tokenizer.add_tokens(item)\n","model.resize_token_embeddings(len(tokenizer))\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"],"metadata":{"id":"Mjkcw1fDWToG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vh9FzUR427JW"},"outputs":[],"source":["def preprocess_function(examples):\n","    preprocessed = examples[\"text\"]\n","    return tokenizer(preprocessed, truncation=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZfw_lai35A4"},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import r2_score\n","\n","def compute_metrics(eval_pred):\n","\n","    metric4 = evaluate.load(\"accuracy\")\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    accuracy = metric4.compute(predictions=predictions, references=labels)[\"accuracy\"]\n","    macro_f1 = (f1_score(labels, predictions, average =\"macro\"))\n","\n","    return {\"accuracy\":accuracy, \"macro_f1\":macro_f1}\n","\n","\n","def compute_metrics(eval_pred):\n","    metric1 = evaluate.load(\"mse\")\n","    logits, labels = eval_pred\n","    mse = metric1.compute(predictions=logits, references=labels)[\"mse\"]\n","    return {\"mse\": mse}\n","\n","\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    labels = labels.reshape(-1, 1)\n","\n","    mse = mean_squared_error(labels, logits)\n","    mae = mean_absolute_error(labels, logits)\n","    r2 = r2_score(labels, logits)\n","    single_squared_errors = ((logits - labels).flatten()**2).tolist()\n","\n","    # Compute accuracy\n","    # Based on the fact that the rounded score = true score only if |single_squared_errors| < 0.5\n","    accuracy = sum([1 for e in single_squared_errors if e < 0.25]) / len(single_squared_errors)\n","\n","    return {\"mse\": mse, \"mae\": mae, \"r2\": r2, \"accuracy\": accuracy}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ntZERAEqvNwt"},"outputs":[],"source":["skf = StratifiedKFold(n_splits= k_fold, random_state = 31, shuffle=True)"]},{"cell_type":"code","source":["from torch import nn\n","from transformers import Trainer\n","\n","class CustomTrainer(Trainer):\n","    def save_model(self, output_dir=None, _internal_call=False):\n","        if output_dir is None:\n","            output_dir = self.args.output_dir\n","\n","        self.model = self.model.to('cuda')\n","\n","        for param in self.model.parameters():\n","            if not param.is_contiguous():\n","                param.data = param.data.contiguous()\n","\n","        super().save_model(output_dir, _internal_call)\n","\n","\n","\n","class RegressionTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.get('logits')\n","        loss_fct = nn.MSELoss()\n","        loss = loss_fct(logits.squeeze(), labels.squeeze())\n","        return (loss, outputs) if return_outputs else loss"],"metadata":{"id":"pdYaTy8NoQi4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define training and validation data\n","val_ratio = 0.09\n","\n","inds = np.random.permutation(train_data.shape[0])\n","ind_split = int(train_data.shape[0]*val_ratio)\n","\n","val_data = train_data.loc[inds[:ind_split]].sort_index().copy()\n","train_data2 = train_data.loc[inds[ind_split:]].sort_index().copy()\n","\n","assert val_data.shape[0] + train_data2.shape[0] == train_data.shape[0]\n","train_data2.head(20)\n"],"metadata":{"id":"LriV06xhyP6u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's start training model\n","\n","val = Dataset.from_pandas(val_data.set_index(\"text\"))\n","train = Dataset.from_pandas(train_data2.set_index(\"text\"))\n","\n","dataset_dict = DatasetDict({\"val\": val, \"train\": train})\n","preprocessed_dataset_dict = dataset_dict.map(preprocess_function, batched=True)\n","\n","training_args = TrainingArguments(\n","        \"Hate Speech Strength Prediction\",\n","        learning_rate=5e-6,\n","        per_device_train_batch_size=16,\n","        per_device_eval_batch_size=20,\n","        num_train_epochs=10,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"steps\",\n","        metric_for_best_model = 'accuracy',\n","        logging_steps = 100,\n","        load_best_model_at_end=True,\n","        greater_is_better=True,\n","        #hub_strategy=\"end\",           # These parameter used when you want saved mmodels directly to HuggingFace,\n","        #push_to_hub=True,             # If you want to save model on huggingFace you must select \"True\"\n","        #hub_model_id= hub_id,         # Write your model address on HuggingFace\n","        #hub_private_repo= False,      # Determine whether the model on Hugging Face should be private or public.\n","        #hub_token= \"...........\",     # To save model into HuggingFace you need \"hub_token\" which can take from your own HuggingFace account\n",")\n","\n","trainer = RegressionTrainer(\n","        model = model,\n","        args = training_args,\n","        train_dataset=preprocessed_dataset_dict[\"train\"],\n","        eval_dataset=preprocessed_dataset_dict[\"val\"],\n","        tokenizer = tokenizer,\n","        data_collator = data_collator,\n","        compute_metrics = compute_metrics,\n","\n",")\n","trainer.train()\n","\n","#trainer.save_model()       # If you want save your trained model on HuggingFace"],"metadata":{"id":"giMXzUWDyWQb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSHmt9lFDzt9"},"outputs":[],"source":["# Let's start testing model\n","\n","test_ds = Dataset.from_pandas(test_data).map(preprocess_function, batched=True)\n","test_ds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2O_oLx5c-vc-"},"outputs":[],"source":["predictions = trainer.predict(test_ds).predictions\n","predictions"]},{"cell_type":"code","source":["test_data['Predicted-HS-Severity'] = predictions.reshape(-1)\n","test_data"],"metadata":{"id":"W-1rYQpawEdR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data.loc[test_data['Predicted-HS-Severity'] < 0, 'Predicted-HS-Severity'] = 0\n","test_data.loc[test_data['Predicted-HS-Severity'] < 0]\n","\n","test_data['Rounded-HS-Severity'] = test_data['Predicted-HS-Severity'].round()\n","\n","test_data"],"metadata":{"id":"zxBXeDpxwSHS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import mean_squared_error\n","from math import sqrt\n","from scipy.stats import pearsonr, spearmanr\n","\n","\n","MSE_simple =  mean_squared_error(test_data['label'].tolist(), test_data['Predicted-HS-Severity'].tolist())\n","MSE_rounded = mean_squared_error(test_data['label'].tolist(), test_data['Rounded-HS-Severity'].tolist())\n","\n","\n","RMSE_simple = sqrt(mean_squared_error(test_data['label'].tolist(), test_data['Predicted-HS-Severity'].tolist()))\n","RMSE_rounded = sqrt(mean_squared_error(test_data['label'].tolist(), test_data['Rounded-HS-Severity'].tolist()))\n","\n","correlation = test_data['label'].corr(test_data['Predicted-HS-Severity'])\n","\n","\n","print(\"MSE_simple:\", MSE_simple )\n","print(\"MSE_rounded:\", MSE_rounded )\n","print(\" \")\n","print(\"RMSE_simple:\", RMSE_simple )\n","print(\"RMSE_rounded:\", RMSE_rounded )\n","print(\" \")\n","print(\"Pearson correlation coefficient:\", correlation)"],"metadata":{"id":"tFSqE7PIwSuf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save trained model on local drive\n","# Please replace the pathes with your local address path\n","\n","\n","if model_lang == 'tr':\n","          path_save_model = '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/trained_models/hs-tr-strength/'\n","\n","if model_lang == 'ar':\n","          path_save_model = '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/trained_models/hs-ar-strength/'\n","\n","\n","trainer.save_model(path_save_model)"],"metadata":{"id":"QbcM6TVepXtn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load trained model from local drive\n","\n","from transformers import pipeline\n","\n","path_trained_model = path_save_model\n","model_pipline = pipeline(task = \"text-classification\", model = path_trained_model, function_to_apply=\"none\" , device=\"cuda\")\n"],"metadata":{"id":"2iMsNqe0bxzI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# predict the strength of example teewts using trained model\n","\n","def predict_strength_tweet(review):\n","        prediction = []\n","        prediction = (model_pipline (review))\n","        prediction = prediction[0][\"score\"]\n","        return prediction"],"metadata":{"id":"dD_qKQROhBJV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["🔴 ▶ **Disclaimer:**\n","\n","This study (project) includes examples of offensive language, hate speech, and profanity due to the nature of the research. These examples do not represent the authors’ views (or developers' views). The goal of this work is to contribute to the detection and prevention of harmful content and violence directed toward refugees, LGBTI+ individuals, religious groups, nationalities, and other vulnerable communities."],"metadata":{"id":"G0Brsr5Jjo-z"}},{"cell_type":"code","source":["AR_TWEETS_EXAMPLES = [\n","    \"كل اللاجئين لصوص\",\n","    \"ارحلوا من بلدنا\",\n","    \"مكان المرأة هو المطبخ\",\n","    \"اليونان يجب أن تعرف حدودها، وإلا قد نأتي فجأة في ليلة\",\n","    \"لا ينبغي لغير المسلمين أن يصبحوا نوابًا.\",\n","    \"يجب ألا يُسمح للاجئين بالتصويت.\",\n","    \"يجب ألا يستفيد اللاجئون من التأمين الصحي.\",\n","    \"التعليم باللغة الكردية غير مقبول.\",\n","    \"للاجئين أيضًا الحق في الحياة.\",\n","    \"يتم اتخاذ تدابير شاملة ضمن الإطار القانوني لمنع أنشطة التنظيمات الإرهابية.\",\n","    \"تم الإعلان عن تواريخ الفعاليات الثقافية التي ستُقام هذا العام في إسرائيل.\",\n","    \"تفتتح رائدات الأعمال صالونات تجميل تستهدف العملاء من المسلمين وغير المسلمين.\",\n","    \"تُجرى الاستعدادات في الأسواق في سوريا قبل الأعياد الإسلامية والمسيحية.\",\n","    \"هل كون بعض الكلاب مسعورة أو عدوانية يبرر قتل جميع الكلاب؟ اسحبوا القانون.\",\n","     \"يجب على النساء دعم بعضهن البعض\",\n","    \"اشتريت تفاحًا من السوق.\",\n","    \"طعام السوريين لذيذ جدًا.\",\n","    \"الطعام السوري شهي للغاية.\",\n","    \"أنا أحب السوريين كثيرًا، إنهم أشخاص لطيفون جدًا.\",\n","    \"السنغافوريون يعملون بجد.\",\n","    \"أنا أحب الأمريكيين كثيرًا\",\n","    \"أنا لا أحب الملحدين على الإطلاق\",\n","    \"المسيحيين الأرثوذكس\",\n","    \"سنغافورة بلد جميل جدًا\"   ,\n","    \"ذهبت إلى المستشفى أمس، ثم تناولت وجبة طعام لذيذة، ثم عدت إلى المنزل، ولعبت مع ابنتي، وشاهدت مسلسلًا أمريكيًا\",\n","    \"أنا أحب الحيوانات كثيرًا\"\n","]\n","\n","TR_TWEETS_EXAMPLES = [\n","    \"mültecilerin hepsi hırsızdır\",\n","    \"defolun bizim ülkemizden\",\n","    \"kadının yeri mutfaktır\",\n","    \"Yunanistan haddini bilecek yoksa bir gece ansizin gelebiliriz\",\n","    \"Gayrimüslimler milletvekili olmasın.\",\n","    \"Mülteciler oy kullanmasın.\",\n","    \"Mültecilere oy hakki verilmemeli\",\n","    \"Mülteciler sgk'dan yararlanmasın.\",\n","    \"Kürtçe eğitim kabul edilemez.\",\n","    \"Mültecilerin de yaşamaya hakkı var.\",\n","    \"Terör örgütlerinin faaliyetlerini önlemek için hukuki çerçevede kapsamlı önlemler alınmaktadır.\",\n","    \"İsrail'de bu yıl düzenlenecek kültürel etkinliklerin tarihleri açıklandı\",\n","    \"Kadın girişimciler, hem Müslüman hem de diğer topluluklardan müşterilere hitap eden güzellik salonları açıyor.\",\n","    \"Suriye’deki pazarlarda Müslüman ve Hristiyan bayramları öncesinde hazırlıklar yapılıyor.\",\n","    \"Bazı köpeklerin kuduz ya da saldırgan olması, tüm köpeklerin öldürülmesi için haklı bir sebep midir? Yasayı geri çek.\",\n","    \"Kadınlar birbirlerine destek olmalı.\",\n","    \"Pazardan elma aldim.\",\n","    \"Suriyelilerin yemekleri çok güzel.\",\n","    \"Suriye yemekleri çok lezzetli.\",\n","    \"Süriyelileri çok seviyorum, çok sempatik insanlar.\",\n","    \"Singapurlular çok çalışkan\",\n","    \"Amerikalıları çok Seviyorum\",\n","    \"Ateistleri hiç sevmiyorum\",\n","    \"Ortodoksalar hrıstiyanlar\",\n","    \"Singapur çok güzel bir ülke dir\",\n","    \"Dün hastaneye gittim sonra da bir güzel yemek yedim, eve döndüm, kızımla oyun oynadım, Amerikan dizi izledim\",\n","    \"hayvanlari çok seviyorum\"\n","]\n","\n","\n","if model_lang == 'ar':\n","      TWEETS_EXAMPLES = AR_TWEETS_EXAMPLES\n","\n","if model_lang == 'tr':\n","      TWEETS_EXAMPLES = TR_TWEETS_EXAMPLES\n","\n","for tweet in TWEETS_EXAMPLES:\n","    strength = predict_strength_tweet(remove_punctuation_marks(tweet.lower()))\n","    strength = round (strength)\n","    print(f\"Tweet:    {tweet}\")\n","    print(f\"Strength: {strength}\")\n","    print(\"-\" * 60)\n"],"metadata":{"id":"w9xJCvj7b_ey"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyNimMTmG506fR5JdOuuvC/+"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}