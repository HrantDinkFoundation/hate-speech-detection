{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HrantDinkFoundation/hate-speech-detection/blob/main/hate_speech_classification_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFXTGNTSP-K0"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/'\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "! nvcc --version\n",
        "!nvidia-smi -L\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAAU3hinHKRQ"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers\n",
        "#!pip install transformers==4.28.0\n",
        "!pip install datasets\n",
        "!pip install snscrape\n",
        "!pip install --upgrade accelerate\n",
        "!pip install datasets evaluate\n",
        "!pip install accelerate -U\n",
        "\n",
        "from scipy.spatial.distance import cosine\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import numpy as np\n",
        "import random\n",
        "import csv\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "import datasets\n",
        "from datasets import Dataset, DatasetDict\n",
        "import evaluate\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "import sys, os\n",
        "import torch\n",
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vyLbRzuVICq"
      },
      "outputs": [],
      "source": [
        "# preprocess functions\n",
        "import re\n",
        "\n",
        "def remove_url(text):\n",
        "    return ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
        "\n",
        "def remove_username(text):\n",
        "    return ' '.join(re.sub(\"([@][A-Za-z0-9_]+)\",\" \", text).split())\n",
        "\n",
        "def remove_at_mark(text):\n",
        "    return re.sub(r'[@]', ' ', text)\n",
        "\n",
        "def remove_tag_mark(text):\n",
        "    return re.sub(r'[#]', ' ', text)\n",
        "\n",
        "def replace_hashtags_with_segments (text):\n",
        "   regex = \"#(\\w+)\"\n",
        "   text_split = text.split()\n",
        "   sent= ''\n",
        "   strip_word =' '\n",
        "   for word in text_split:\n",
        "     hashtag= re.findall(regex, word)\n",
        "     #print(hashtag)\n",
        "     #print(strip_word)\n",
        "\n",
        "     if (len(hashtag)) == 0 :\n",
        "        sent = sent + ' ' + word\n",
        "     if (len(hashtag))!=0:\n",
        "        strip_word = word.replace(('#'+hashtag[0]), ' ')\n",
        "        #print (hashtag[0])\n",
        "        #print(strip_word)\n",
        "        try :\n",
        "           key = '#'+ hashtag[0]\n",
        "           segmented = segmented_dict [key]\n",
        "           if type(segmented) == str:\n",
        "                    #segmented = re.sub(r'[|]', ' ', segmented)\n",
        "                    #segmented = re.sub(r'[,]', ' ', segmented)\n",
        "                    sent= sent + ' ' + segmented\n",
        "           if type(segmented) == list:\n",
        "                    for k in segmented:\n",
        "                      sent = sent +' ' + k\n",
        "        except:\n",
        "           sent = sent + ' ' +  hashtag[0]\n",
        "   return sent + ' ' + strip_word\n",
        "\n",
        "def control_tag_mark(text):\n",
        "     s = ' '\n",
        "     for word in text.split():\n",
        "            word = re.sub(r'[#]', ' #', word)\n",
        "            s = s + ' ' + word\n",
        "     return s\n",
        "\n",
        "def remove_punctuation_marks(text):\n",
        "   punc = '''!()-[]{};:'\"\\,<>./?$%^&*~'''\n",
        "   for i in text:\n",
        "       if i in punc:\n",
        "         text = text.replace(i, \" \")\n",
        "   return text\n",
        "\n",
        "\n",
        "\n",
        "def remove_underline(text):\n",
        "  punc = '''_'''\n",
        "  for i in text:\n",
        "       if i in punc:\n",
        "         text = text.replace(i, \"\")\n",
        "  return text\n",
        "\n",
        "def remove_hashtag(text):\n",
        "    return ' '.join(re.sub(\"([#][A-Za-z0-9_]+)\",\" \", text).split())\n",
        "\n",
        "def remove_turkish_hashtag(text):\n",
        "    return ' '.join(re.sub(\"([#][\\w+_]+)\",\" \", text).split())\n",
        "\n",
        "def find_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return re.findall(emoji_pattern, text) # no emoji\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return (emoji_pattern.sub(r'', text)) # no emoji\n",
        "\n",
        "def remove_arabic_hashtags(text):\n",
        "   return ' '.join(re.sub('([#][\\u0600-\\u06ff_]+)', \" \", text).split())\n",
        "\n",
        "def remove_emoji_duplicate (text):\n",
        "      emoji =[]\n",
        "      rest = ''\n",
        "      main_text = []\n",
        "      for w in text :\n",
        "         x = find_emoji(w)\n",
        "         if len (x) != 0 :\n",
        "            emoji.append (x[0])\n",
        "      #print(emoji)\n",
        "      rest = remove_emoji(text)\n",
        "      emoji = list(set(emoji))\n",
        "      for e in emoji :\n",
        "        try:\n",
        "          rest = rest + \" \" + e\n",
        "        except:\n",
        "          rest = rest\n",
        "      main_text.append(rest)\n",
        "      return main_text[0]\n",
        "\n",
        "def lower_case(text):\n",
        "    text = text.replace(\"I\", \"ı\")\n",
        "    text = text.replace(\"İ\", \"i\")\n",
        "    return text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAAVod3AVjod"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define data path\n",
        "# Please add the adress of datasets on your local drive\n",
        "\n",
        "path_Turkish_data     =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/df_turkish_cats_targets_degrees_final_with_translated_all.csv'\n",
        "path_arabic_data      =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/df_arabic_cats_targets_degrees_final.csv'\n",
        "path_synthetic_data   =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/synthetic_data_all_new.xlsx'\n",
        "\n",
        "path_all_emoji =         '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/all_emoji_dict.json'\n",
        "path_hashtag_segment  =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/all_hashtag_segmented_tr.json'"
      ],
      "metadata": {
        "id": "KY6iREuAvDHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn-0qMMmVyjS"
      },
      "outputs": [],
      "source": [
        "# laod emoji_dict and hashtag_dict\n",
        "\n",
        "f = open(path_all_emoji)\n",
        "data = json.load(f)\n",
        "emoji_list = data['emojis']\n",
        "\n",
        "f = open(path_hashtag_segment)\n",
        "segmented_dict = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbINt_v7XOKE"
      },
      "outputs": [],
      "source": [
        "# determine which preprocess steps must be apply on data\n",
        "\n",
        "def pre_process_tweets(text_list):\n",
        "   cleaned_text =[]\n",
        "   for s in text_list:\n",
        "       #s = lower_case(s)\n",
        "       s = s.lower()\n",
        "       s = remove_username(s)\n",
        "       s = remove_at_mark(s)\n",
        "       s = remove_url(s)\n",
        "       s = remove_tag_mark(s)\n",
        "       #s = control_tag_mark(s)\n",
        "       s = remove_punctuation_marks(s)\n",
        "       s = remove_underline(s)\n",
        "       #s = replace_hashtags_with_segments(s)\n",
        "       #s = remove_tag_mark(s)\n",
        "       cleaned_text.append(s)\n",
        "   return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZlH3j_avBwC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_syn = pd.read_excel(path_synthetic_data)\n",
        "\n",
        "df_syn = df_syn.iloc[:,[4,5,6,7,9]]\n",
        "\n",
        "print(df_syn['CatComb6class'].value_counts())\n",
        "print(df_syn['CatComb4class'].value_counts())\n",
        "print(df_syn['CatComb2class'].value_counts())\n",
        "\n",
        "df_syn= df_syn.sample(frac = 1)\n",
        "df_syn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3HBA7XJvwx4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_turkish = pd.read_csv(path_Turkish_data)\n",
        "df_turkish = df_turkish.iloc[:,[2, 27, 29, 31, 32,38, 39]]\n",
        "\n",
        "print(df_turkish['CatComb6class'].value_counts())\n",
        "print(df_turkish['CatComb4class'].value_counts())\n",
        "print(df_turkish['CatComb2class'].value_counts())\n",
        "\n",
        "df_turkish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOSLlHxvw1J1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_arabic = pd.read_csv(path_arabic_data)\n",
        "df_arabic = df_arabic.iloc[:,[2, 27, 29, 31, 32, 38]]\n",
        "\n",
        "df_arabic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0-_R31GqByY"
      },
      "outputs": [],
      "source": [
        "# select numumber of classes  [2, 4, 6]\n",
        "num_class = 2\n",
        "k_fold = 11\n",
        "\n",
        "if num_class == 2:\n",
        "     column_select = 'CatComb2class'\n",
        "     labels = ['No hate Speech', 'Hate Speech']\n",
        "     sentiment_mapping =  {'No hate Speech', 'Hate Speech'}\n",
        "     hub_id = 'HrantDinkFoundation/'\n",
        "     model_id = 'hs-2class-prediction'\n",
        "\n",
        "\n",
        "\n",
        "if num_class == 4:\n",
        "     column_select = 'CatComb4class'\n",
        "\n",
        "     labels = ['No Hate Detected',\n",
        "               'Mild Level (Discriminatory Discourse)',\n",
        "               'Moderate Level (Exaggeration, Generalization, Attribution, Distortion, Symbolization)',\n",
        "               'Severe Level (Swearing, Insult, Defamation, Dehumanization, Threat of Enmity/War/Attack/Murder/Harm)']\n",
        "\n",
        "     hub_id = 'HrantDinkFoundation/'\n",
        "     model_id = 'hs-4class-prediction'\n",
        "\n",
        "     sentiment_mapping = {'No Hate Detected',\n",
        "                          'Mild Level (Discriminatory Discourse)',\n",
        "                          'Moderate Level (Exaggeration, Generalization, Attribution, Distortion, Symbolization)',\n",
        "                          'Severe Level (Swearing, Insult, Defamation, Dehumanization, Threat of Enmity/War/Attack/Murder/Harm)',\n",
        "                          }\n",
        "\n",
        "\n",
        "if num_class == 6:\n",
        "     column_select = 'CatComb6class'\n",
        "     labels = ['No Hate Detected',\n",
        "               'Mild Level (Discriminatory Discourse)',\n",
        "               'Moderate Level (Exaggeration, Generalization, Attribution, Distortion)',\n",
        "               'Elevated Level (Symbolization)',\n",
        "               'Severe Level (Swearing, Insult, Defamation, Dehumanization)',\n",
        "               'Extreme Level (Threat of Enmity/War/Attack/Murder/Harm)']\n",
        "\n",
        "     hub_id = 'HrantDinkFoundation/'\n",
        "     model_id = 'hs-6class-prediction'\n",
        "\n",
        "     sentiment_mapping = {'No Hate Detected',\n",
        "                          'Mild Level (Discriminatory Discourse)',\n",
        "                          'Moderate Level (Exaggeration, Generalization, Attribution, Distortion)',\n",
        "                          'Elevated Level (Symbolization)',\n",
        "                          'Severe Level (Swearing, Insult, Defamation, Dehumanization)',\n",
        "                          'Extreme Level (Threat of Enmity/War/Attack/Murder/Harm)',\n",
        "                          }\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select which data used in training\n",
        "\n",
        "# 'only-tr' : use only Turkish data\n",
        "# 'only-ar' : use only arabic data\n",
        "# 'tr-syn'  : use Turkish and synthetic data\n",
        "# 'ar-syn'  : use arabic and synthetic data\n",
        "# 'ar-translated-syn'  : use arabic, synthetic and the translated column of Turkish data\n",
        "\n",
        "mode_train = ['only-tr', 'only-ar', 'tr-syn' , 'ar-syn', 'ar-translated-syn']\n",
        "mode = mode_train [2]"
      ],
      "metadata": {
        "id": "iJTR8qE90beO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-pbvvDE1ZeJ"
      },
      "outputs": [],
      "source": [
        "if mode =='only-tr':\n",
        "    model_lang = 'tr'\n",
        "    model_name = 'dbmdz/bert-base-turkish-uncased'\n",
        "    hub_id = hub_id + 'turkish-' + model_id\n",
        "\n",
        "    df_train = df_turkish[(df_turkish.Div=='train')]\n",
        "    df_test  = df_turkish[(df_turkish.Div=='test')]\n",
        "\n",
        "if mode =='only-ar' :\n",
        "     model_lang = 'ar'\n",
        "     model_name = 'aubmindlab/bert-base-arabert'\n",
        "\n",
        "     hub_id = hub_id + 'arabic-' + model_id\n",
        "     df_train = df_arabic[(df_arabic.Div=='train')]\n",
        "     df_test  = df_arabic[(df_arabic.Div=='test')]\n",
        "\n",
        "\n",
        "if mode == 'tr-syn':\n",
        "    model_lang = 'tr'\n",
        "    model_name = 'dbmdz/bert-base-turkish-uncased'\n",
        "    hub_id = hub_id + 'turkish-' + model_id\n",
        "    df_train = df_turkish[(df_turkish.Div=='train')]\n",
        "    df_test  = df_turkish[(df_turkish.Div=='test')]\n",
        "\n",
        "    df_train = pd.concat([df_train, df_syn], ignore_index=True)\n",
        "\n",
        "\n",
        "if mode == 'ar-syn':\n",
        "    model_lang = 'ar'\n",
        "    model_name = 'aubmindlab/bert-base-arabert'\n",
        "    hub_id = hub_id + 'arabic-' + model_id\n",
        "\n",
        "    df_train = df_arabic[(df_arabic.Div=='train')]\n",
        "    df_test  = df_arabic[(df_arabic.Div=='test')]\n",
        "\n",
        "    df_syn = df_syn.drop (columns=['Text'])\n",
        "    df_syn.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n",
        "    df_train = pd.concat([df_train, df_syn], ignore_index=True)\n",
        "\n",
        "\n",
        "if mode == 'ar-translated-syn':\n",
        "    model_lang = 'ar'\n",
        "    model_name = 'aubmindlab/bert-base-arabert'\n",
        "    hub_id = hub_id + 'arabic-' + model_id\n",
        "\n",
        "    df_turkish = df_turkish.drop (columns=['Text'])\n",
        "    df_turkish.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n",
        "\n",
        "    df_syn = df_syn.drop (columns=['Text'])\n",
        "    df_syn.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n",
        "\n",
        "    df_train_arabic = df_arabic[(df_arabic.Div=='train')]\n",
        "    df_test  = df_arabic[(df_arabic.Div=='test')]\n",
        "\n",
        "    df_train = pd.concat([df_train_arabic , df_turkish, df_syn], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OtGrwB0qgsx"
      },
      "outputs": [],
      "source": [
        "df_train\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test"
      ],
      "metadata": {
        "id": "tuAHXCdl6nUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7v9UKUu_0SY"
      },
      "outputs": [],
      "source": [
        "# Selecet the samples of training and testing data where the majority voting is clear\n",
        "\n",
        "if num_class == 2:\n",
        "     column_select = 'CatComb2class'\n",
        "     df_train = df_train[df_train.CatComb2class != set()].copy()\n",
        "     df_test  = df_test[df_test.CatComb2class != set()].copy()\n",
        "\n",
        "     df_train = df_train[df_train.CatComb2class.apply(len)==3]\n",
        "     df_test  = df_test[df_test.CatComb2class.apply(len)==3]\n",
        "\n",
        "\n",
        "if num_class == 4:\n",
        "     column_select = 'CatComb4class'\n",
        "     df_train = df_train[df_train.CatComb4class != set()].copy()\n",
        "     df_test  = df_test[df_test.CatComb4class != set()].copy()\n",
        "\n",
        "     df_train = df_train[df_train.CatComb4class.apply(len)==3]\n",
        "     df_test  = df_test[df_test.CatComb4class.apply(len)==3]\n",
        "\n",
        "if num_class == 6:\n",
        "     column_select = 'CatComb6class'\n",
        "     df_train = df_train[df_train.CatComb6class != set()].copy()\n",
        "     df_test  = df_test[df_test.CatComb6class != set()].copy()\n",
        "\n",
        "     df_train = df_train[df_train.CatComb6class.apply(len)==3]\n",
        "     df_test  = df_test[df_test.CatComb6class.apply(len)==3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pgeiz9nGnKQB"
      },
      "outputs": [],
      "source": [
        "# preprocess train data\n",
        "\n",
        "import ast\n",
        "\n",
        "text = df_train['Text'].tolist()\n",
        "label = df_train[column_select].tolist()\n",
        "label = [int(i.strip('{}')) for i in label]\n",
        "\n",
        "text = pre_process_tweets(text)\n",
        "train_data = pd.DataFrame(list(zip(text, label)), columns =['text', 'label'])\n",
        "train_data= train_data.astype({'label':'int'})\n",
        "\n",
        "\n",
        "print(train_data['label'].value_counts())\n",
        "train_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoX6nmbsj0wB"
      },
      "outputs": [],
      "source": [
        "# preprocess test data\n",
        "\n",
        "text = df_test['Text'].tolist()\n",
        "label = df_test[column_select].tolist()\n",
        "label = [int(i.strip('{}')) for i in label]\n",
        "\n",
        "text = pre_process_tweets(text)\n",
        "test_data = pd.DataFrame(list(zip(text, label)), columns =['text', 'label'])\n",
        "test_data= test_data.astype({'label':'int'})\n",
        "\n",
        "\n",
        "\n",
        "print(test_data['label'].value_counts())\n",
        "test_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mjkcw1fDWToG"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from transformers import ElectraForSequenceClassification,  ElectraTokenizerFast\n",
        "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaTokenizerFast, RobertaForSequenceClassification\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments, GPT2Config\n",
        "\n",
        "id2label = {idx:label for idx, label in enumerate(labels)}\n",
        "label2id = {label:idx for idx, label in enumerate(labels)}\n",
        "\n",
        "pretrained_model_name = model_name\n",
        "print(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, do_lower_case=True, force_download=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name, num_labels = num_class, force_download=True, id2label=id2label, label2id=label2id ).to(device)   # id2label=id2label, label2id=label2id\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "for item in emoji_list :\n",
        "        tokenizer.add_tokens(item)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vh9FzUR427JW"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    preprocessed = examples[\"text\"]\n",
        "    return tokenizer(preprocessed, truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZfw_lai35A4"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "\n",
        "    metric4 = evaluate.load(\"accuracy\")\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = metric4.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "    macro_f1 = (f1_score(labels, predictions, average =\"macro\"))\n",
        "\n",
        "    return {\"accuracy\":accuracy, \"macro_f1\":macro_f1}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntZERAEqvNwt"
      },
      "outputs": [],
      "source": [
        "skf = StratifiedKFold(n_splits= k_fold, random_state = 31, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdYaTy8NoQi4"
      },
      "outputs": [],
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def save_model(self, output_dir=None, _internal_call=False):\n",
        "        if output_dir is None:\n",
        "            output_dir = self.args.output_dir\n",
        "\n",
        "        self.model = self.model.to('cuda')\n",
        "\n",
        "        for param in self.model.parameters():\n",
        "            if not param.is_contiguous():\n",
        "                param.data = param.data.contiguous()\n",
        "\n",
        "        super().save_model(output_dir, _internal_call)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8vqUCVTvSQ0"
      },
      "outputs": [],
      "source": [
        "# Let's start training model\n",
        "\n",
        "for i, (train_index, test_index) in enumerate(skf.split(train_data.text , train_data.label)):\n",
        "\n",
        "    print(\"number of samples in train folds: \", len(train_index))\n",
        "    print(\"number of samples in test fold  : \", len(test_index))\n",
        "\n",
        "    test = Dataset.from_pandas(train_data.iloc[test_index].set_index(\"text\"))\n",
        "    train = Dataset.from_pandas(train_data.iloc[train_index].set_index(\"text\"))\n",
        "    dataset_dict = DatasetDict({\"test\": test, \"train\": train})\n",
        "    preprocessed_dataset_dict = dataset_dict.map(preprocess_function, batched=True)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        \"My Hate Speech Detection\",\n",
        "        overwrite_output_dir=True,\n",
        "        learning_rate=5e-6,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=20,\n",
        "        num_train_epochs=10,\n",
        "        weight_decay=0.01,\n",
        "        eval_strategy=\"steps\",\n",
        "        metric_for_best_model = 'accuracy',\n",
        "        logging_steps = 100,\n",
        "        load_best_model_at_end=True,\n",
        "        greater_is_better=True,\n",
        "        #hub_strategy=\"end\",        # These parameter used when you want saved mmodels directly to HuggingFace,\n",
        "        #push_to_hub=True,          # If you want to save model on huggingFace you must select \"True\"\n",
        "        #hub_model_id= hub_id,      # Write your model address on HuggingFace\n",
        "        #hub_private_repo= False,   # Determine whether the model on Hugging Face should be private or public.\n",
        "        #hub_token= \"......\",       # To save model into HuggingFace you need \"hub_token\" which can take from your own HuggingFace account\n",
        "    )\n",
        "\n",
        "    trainer = CustomTrainer(\n",
        "        model = model,\n",
        "        args = training_args,\n",
        "        train_dataset=preprocessed_dataset_dict[\"train\"],\n",
        "        eval_dataset=preprocessed_dataset_dict[\"test\"],\n",
        "        tokenizer = tokenizer,\n",
        "        data_collator = data_collator,\n",
        "        compute_metrics = compute_metrics,\n",
        "\n",
        "    )\n",
        "    trainer.train()\n",
        "    break\n",
        "\n",
        "#trainer.save_model()              # If you want save your trained model on HuggingFace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSHmt9lFDzt9"
      },
      "outputs": [],
      "source": [
        "# Let's start testing model\n",
        "\n",
        "test_ds = Dataset.from_pandas(test_data).map(preprocess_function, batched=True)\n",
        "test_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2O_oLx5c-vc-"
      },
      "outputs": [],
      "source": [
        "predictions = trainer.predict(test_ds).predictions\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGV9C4TRFs_g"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = tf.argmax(predictions, axis=1).numpy()\n",
        "y_test= np.array(test_data['label'])\n",
        "\n",
        "#print(title)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print('-------------------------------------------------------')\n",
        "print('macro-f1:', f1_score(y_test, y_pred, average =\"macro\"))\n",
        "print('accuracy:', f1_score(y_test, y_pred, average =\"micro\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "con_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred, num_classes=num_class)\n",
        "#print(title)\n",
        "print(con_mat)\n",
        "\n",
        "print('-------------------------------------------------------')\n",
        "print(classification_report(y_test, y_pred, digits=num_class, zero_division=0))"
      ],
      "metadata": {
        "id": "AQHOFgde8azg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1z-apwd41ER"
      },
      "outputs": [],
      "source": [
        "# save trained model on local drive\n",
        "\n",
        "if model_lang == 'tr':\n",
        "          path_save_model = '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/trained_models/hs-tr-' + str(num_class) + 'class/'\n",
        "\n",
        "if model_lang == 'ar':\n",
        "          path_save_model = '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/trained_models/hs-ar-' + str(num_class) + 'class/'\n",
        "\n",
        "\n",
        "trainer.save_model(path_save_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iMsNqe0bxzI"
      },
      "outputs": [],
      "source": [
        "# load trained model from local drive\n",
        "#Please add the address where the models are saved.\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "path_trained_model = path_save_model\n",
        "\n",
        "def classify_tweet(text):\n",
        "        prediction = []\n",
        "        model_pipline = pipeline(task = \"text-classification\", model = path_trained_model, device=\"cuda\")\n",
        "        prediction = (model_pipline (text))\n",
        "        prediction = prediction[0][\"label\"]\n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9xJCvj7b_ey"
      },
      "outputs": [],
      "source": [
        "AR_TWEETS_EXAMPLES = [\n",
        "    \"أريد أن يموت اللاجئون\",\n",
        "    \"فليمت جميع اللاجئين\",\n",
        "    \"كل اللاجئين لصوص\",\n",
        "    \"أكره اللاجئين\",\n",
        "    \"ارحلوا من بلدنا\",\n",
        "    \"هنا مرة أخرى، سلوك اللاجئين المتحرش والمغتصب للاجئين، والآن يجب إزالة القطيع المنحرف من بلادنا\",\n",
        "    \"النساء، اجلسن في البيت وإلا ستتعرضن للتحرش، لا تقولوا إنني لم أحذركن\",\n",
        "    \"مكان المرأة هو المطبخ\",\n",
        "    \"اليونان يجب أن تعرف حدودها، وإلا قد نأتي فجأة في ليلة\",\n",
        "    \"لا ينبغي للسوريين أن يصوتوا\",\n",
        "    \"ولا يجوز لغير المسلمين أن يكونوا أعضاء في البرلمان\",\n",
        "    \"لا ينبغي للاجئين التصويت\",\n",
        "    \"لا ينبغي للاجئين الاستفادة من مباحث أمن الدولة\",\n",
        "    \"التعليم باللغة الكردية غير مقبول\",\n",
        "    \"للاجئين أيضاً الحق في العيش\",\n",
        "    \"يتم اتخاذ تدابير شاملة ضمن الإطار القانوني لمنع أنشطة المنظمات الإرهابية\",\n",
        "    \"تم الإعلان عن مواعيد الفعاليات الثقافية لهذا العام في إسرائيل\",\n",
        "    \"تقوم رائدات الأعمال بافتتاح صالونات تجميل تلبي احتياجات الزبائن من الجاليات المسلمة وغيرها من الجاليات الأخرى\",\n",
        "    \"تجري الاستعدادات في الأسواق في سوريا قبل الأعياد الإسلامية والمسيحية\",\n",
        "    \"يجب على النساء دعم بعضهن البعض\",\n",
        "    \"اشتريت تفاحًا من السوق\",\n",
        "    \"طعام السوريين لذيذ جداً\",\n",
        "    \"الأكل السوري لذيذ\",\n",
        "    \"أحب السوريين كثيراً، فهم شعب متعاطف جداً\"\n",
        "]\n",
        "\n",
        "TR_TWEETS_EXAMPLES = [\n",
        "    \"Mülteciler ölsün istiyorum.\",\n",
        "    \"Bütün mülteciler hırsızdır.\",\n",
        "    \"Mültecilerden nefret ediyorum.\",\n",
        "    \"Defolun gidin bizim ülkemizden.\",\n",
        "    \"Suriyeli sürüleri ülkemize girmeye devam ediyor, bu işgalcilerin artık durması gerekiyor.\",\n",
        "    \"Kadınlar dizinizi kırıp evde oturun yoksa tacize uğrarsınız, demedi demeyein!\",\n",
        "    \"Kadının yeri mutfaktır.\",\n",
        "    \"Yunan haddını bilecek yoksa bir gece ansızın gelebiliriz!\",\n",
        "    \"Suriyeliler oy kullanmasın.\",\n",
        "    \"Gayrimüslimler milletvekili olmasın.\",\n",
        "    \"Mülteciler oy kullanmasın.\",\n",
        "    \"Mülteciler sgk'dan yararlanmasın.\",\n",
        "    \"Kürtçe eğitim kabul edilemez.\",\n",
        "    \"Mültecilerin de yaşamaya hakkı var.\",\n",
        "    \"Terör örgütlerinin faaliyetlerini önlemek için hukuki çerçevede kapsamlı önlemler alınmaktadır.\",\n",
        "    \"İsrail'de bu yıl düzenlenecek kültürel etkinliklerin tarihleri açıklandı\",\n",
        "    \"Kadın girişimciler, hem Müslüman hem de diğer topluluklardan müşterilere hitap eden güzellik salonları açıyor.\",\n",
        "    \"Suriye’deki pazarlarda Müslüman ve Hristiyan bayramları öncesinde hazırlıklar yapılıyor.\",\n",
        "    \"Bazı köpeklerin kuduz ya da saldırgan olması, tüm köpeklerin öldürülmesi için haklı bir sebep midir? Yasayı geri çek.\",\n",
        "    \"Kadınlar birbirlerine destek olmalı.\",\n",
        "    \"Pazardan elma aldim.\",\n",
        "    \"Suriyelilerin yemekleri çok güzel.\",\n",
        "    \"Suriye yemekleri çok lezzetli.\",\n",
        "    \"Süriyelileri çok seviyorum, çok sempatik insanlar.\"\n",
        "]\n",
        "\n",
        "if model_lang == 'ar':\n",
        "      TWEETS_EXAMPLES = AR_TWEETS_EXAMPLES\n",
        "\n",
        "if model_lang == 'tr':\n",
        "      TWEETS_EXAMPLES = TR_TWEETS_EXAMPLES\n",
        "\n",
        "for tweet in TWEETS_EXAMPLES:\n",
        "    preprocessed_tweet = remove_punctuation_marks(tweet.lower())\n",
        "    print(preprocessed_tweet)\n",
        "    sentiment = classify_tweet(preprocessed_tweet)\n",
        "    print(f\"Tweet:    {tweet}\")\n",
        "    print(f\"Category: {sentiment}\")\n",
        "    print(\"-\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyP/+m7gIfvCs9f/wAqBZ8j4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}