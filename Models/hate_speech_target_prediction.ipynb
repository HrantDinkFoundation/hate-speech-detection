{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wFXTGNTSP-K0"},"outputs":[],"source":["from google.colab import files\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","root_path = 'gdrive/My Drive/'\n","\n","import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","! nvcc --version\n","!nvidia-smi -L\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAAU3hinHKRQ"},"outputs":[],"source":["!pip install -U transformers\n","#!pip install transformers==4.28.0\n","!pip install datasets\n","!pip install snscrape\n","!pip install --upgrade accelerate\n","!pip install datasets evaluate\n","!pip install accelerate -U\n","\n","from scipy.spatial.distance import cosine\n","from transformers import AutoModel, AutoTokenizer\n","import numpy as np\n","import random\n","import csv\n","import pandas as pd\n","from sklearn.model_selection import StratifiedKFold\n","\n","import tensorflow as tf\n","import datasets\n","from datasets import Dataset, DatasetDict\n","import evaluate\n","from sklearn.metrics import classification_report, f1_score\n","\n","import sys, os\n","import torch\n","import json\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9vyLbRzuVICq"},"outputs":[],"source":["# preprocess functions\n","\n","def remove_url(text):\n","    return ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\",\" \", text).split())\n","\n","def remove_username(text):\n","    return ' '.join(re.sub(\"([@][A-Za-z0-9_]+)\",\" \", text).split())\n","\n","def remove_at_mark(text):\n","    return re.sub(r'[@]', ' ', text)\n","\n","def remove_tag_mark(text):\n","    return re.sub(r'[#]', ' ', text)\n","\n","def replace_hashtags_with_segments (text):\n","   regex = \"#(\\w+)\"\n","   text_split = text.split()\n","   sent= ''\n","   strip_word =' '\n","   for word in text_split:\n","     hashtag= re.findall(regex, word)\n","     #print(hashtag)\n","     #print(strip_word)\n","\n","     if (len(hashtag)) == 0 :\n","        sent = sent + ' ' + word\n","     if (len(hashtag))!=0:\n","        strip_word = word.replace(('#'+hashtag[0]), ' ')\n","        #print (hashtag[0])\n","        #print(strip_word)\n","        try :\n","           key = '#'+ hashtag[0]\n","           segmented = segmented_dict [key]\n","           if type(segmented) == str:\n","                    #segmented = re.sub(r'[|]', ' ', segmented)\n","                    #segmented = re.sub(r'[,]', ' ', segmented)\n","                    sent= sent + ' ' + segmented\n","           if type(segmented) == list:\n","                    for k in segmented:\n","                      sent = sent +' ' + k\n","        except:\n","           sent = sent + ' ' +  hashtag[0]\n","   return sent + ' ' + strip_word\n","\n","def control_tag_mark(text):\n","     s = ' '\n","     for word in text.split():\n","            word = re.sub(r'[#]', ' #', word)\n","            s = s + ' ' + word\n","     return s\n","\n","def remove_punctuation_marks(text):\n","   punc = '''!()-[]{};:'\"\\,<>./?$%^&*~'''\n","   for i in text:\n","       if i in punc:\n","         text = text.replace(i, \" \")\n","   return text\n","\n","def remove_underline(text):\n","  punc = '''_'''\n","  for i in text:\n","       if i in punc:\n","         text = text.replace(i, \"\")\n","  return text\n","\n","def remove_hashtag(text):\n","    return ' '.join(re.sub(\"([#][A-Za-z0-9_]+)\",\" \", text).split())\n","\n","def remove_turkish_hashtag(text):\n","    return ' '.join(re.sub(\"([#][\\w+_]+)\",\" \", text).split())\n","\n","def find_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    return re.findall(emoji_pattern, text) # no emoji\n","\n","def remove_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    return (emoji_pattern.sub(r'', text)) # no emoji\n","\n","def remove_arabic_hashtags(text):\n","   return ' '.join(re.sub('([#][\\u0600-\\u06ff_]+)', \" \", text).split())\n","\n","def remove_emoji_duplicate (text):\n","      emoji =[]\n","      rest = ''\n","      main_text = []\n","      for w in text :\n","         x = find_emoji(w)\n","         if len (x) != 0 :\n","            emoji.append (x[0])\n","      #print(emoji)\n","      rest = remove_emoji(text)\n","      emoji = list(set(emoji))\n","      for e in emoji :\n","        try:\n","          rest = rest + \" \" + e\n","        except:\n","          rest = rest\n","      main_text.append(rest)\n","      return main_text[0]\n","\n","def lower_case(text):\n","    text = text.replace(\"I\", \"ı\")\n","    text = text.replace(\"İ\", \"i\")\n","    return text.lower()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAAVod3AVjod"},"outputs":[],"source":["def seed_everything(seed):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","seed_everything(0)"]},{"cell_type":"code","source":["# Define data path\n","# Please replace the addresses with the address of dataset on your local drive\n","\n","path_turkish_data     =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/df_turkish_cats_targets_degrees_final_with_translated_all.csv'\n","path_arabic_data      =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/df_arabic_cats_targets_degrees_final.csv'\n","path_synthetic_data   =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/synthetic_data_all_new.xlsx'\n","\n","path_all_emoji =         '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/all_emoji_dict.json'\n","path_hashtag_segment  =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/all_hashtag_segmented_tr.json'"],"metadata":{"id":"ji-JuOQJvvw3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gn-0qMMmVyjS"},"outputs":[],"source":["# Laod emoji_dict and hashtag_dict\n","\n","f = open(path_all_emoji)\n","data = json.load(f)\n","emoji_list = data['emojis']\n","\n","f = open(path_hashtag_segment)\n","segmented_dict = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbINt_v7XOKE"},"outputs":[],"source":["# Select which preprocess steps must be apply on data\n","\n","def pre_process_tweets(text_list):\n","   cleaned_text =[]\n","   for s in text_list:\n","       #s = lower_case(s)\n","       s = remove_username(s)\n","       s = remove_at_mark(s)\n","       s = remove_url(s)\n","       s = remove_tag_mark(s)\n","       #s = control_tag_mark(s)\n","       s = remove_punctuation_marks(s)\n","       s = remove_underline(s)\n","       #s = replace_hashtags_with_segments(s)\n","       cleaned_text.append(s)\n","   return cleaned_text"]},{"cell_type":"code","source":["#  Main target groups in the dataset\n","\n","main_targets = {'Hedef grup belirgin değil veya yok.': 0,\n","                'Ülke/Milliyet': 1,\n","                'Irk/Etnik Köken': 2,\n","                'Din': 3,\n","                'Cinsiyet': 4,\n","                'Cinsel Yönelim': 5,\n","                'Belli Görüş/Statü/Uygulama; Mesleki Pozisyon Grubu': 6,\n","                'Hedef grup birden fazla.': 7}"],"metadata":{"id":"ffTlz8SeCJUF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Labels after merging :\n","\n","# 'Ülke/Milliyet' and 'Irk/Etnik Köken'  merged\n","# 'Cinsiyet' and 'Cinsel Yönelim' merged\n","\n","labels = ['Target group not specified or not present' ,\n","          'Country/Nationality/Race/Ethnicity' ,\n","          'Religion',\n","          'Gender/Sexual Orientation',\n","          'Specific Viewpoint/Status/Practice; Occupational Position Group']\n","\n","\n","map_7_to_5 = { 0:0, 1:1, 2:1, 3:2, 4:3, 5:3 , 6:4}\n","def extract_5class(inp):\n","    out = set()\n","    for i in inp:\n","      out.add(map_7_to_5[i])\n","\n","    return out\n","\n","\n","# remove 0 class (no-target), when there is other targets\n","\n","def conflicting_target_0(inp):\n","    out = inp.copy()\n","    if 0 in inp and len(inp) > 1:\n","        out.remove(0)\n","    return out"],"metadata":{"id":"r0dAP6ebTic_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wUp63TQOo8Ke"},"outputs":[],"source":["#Load synthetic data\n","\n","df_syn = pd.read_excel(path_synthetic_data)\n","df_syn = df_syn.iloc[:,[4,8,9]]\n","\n","\n","df_syn = df_syn[df_syn.TargetAll != set()].copy()\n","df_syn['TargetAll']= df_syn.TargetAll.apply(eval)\n","\n","print(df_syn['TargetAll'].value_counts())\n","\n","df_syn['TargetAll'] = df_syn.TargetAll.apply(extract_5class)\n","df_syn['TargetAll'] = df_syn.TargetAll.apply(conflicting_target_0)\n","\n","print(df_syn['TargetAll'].value_counts())\n","\n","df_syn= df_syn.sample(frac = 1)\n","df_syn\n"]},{"cell_type":"code","source":["#Load synthetic data\n","\n","df_turkish = pd.read_csv(path_turkish_data)\n","\n","df_turkish = df_turkish.iloc[:,[2, 36,38, 39]]\n","\n","label_all = df_turkish['TargetAll'].tolist()\n","label_all = [str({item}) for item in label_all]\n","\n","df_turkish = df_turkish[df_turkish.TargetAll != set()].copy()\n","df_turkish['TargetAll']= df_turkish.TargetAll.apply(eval)\n","\n","df_turkish['TargetAll'] = df_turkish.TargetAll.apply(extract_5class)\n","df_turkish['TargetAll'] = df_turkish['TargetAll'].apply(lambda x: x.discard(4) or x)    # ignor this class: 'Specific Viewpoint/Status/Practice; Occupational Position Group'\n","df_turkish['TargetAll'] = df_turkish.TargetAll.apply(conflicting_target_0)\n","df_turkish = df_turkish[df_turkish.TargetAll != set()].copy()\n","\n","print(df_turkish['TargetAll'].value_counts())\n","\n","\n","#df_all = df_all.reset_index()\n","df_turkish= df_turkish.sample(frac = 1)\n","\n","df_turkish"],"metadata":{"id":"MHfbcRLgxCTc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","df_arabic = pd.read_csv(path_arabic_data)\n","\n","df_arabic = df_arabic.iloc[:,[2,36,38]]\n","\n","\n","df_arabic = df_arabic[df_arabic.TargetAll != set()].copy()\n","df_arabic['TargetAll']= df_arabic.TargetAll.apply(eval)\n","\n","df_arabic['TargetAll'] = df_arabic.TargetAll.apply(extract_5class)\n","df_arabic['TargetAll'] = df_arabic['TargetAll'].apply(lambda x: x.discard(4) or x)  # ignor this class: 'Specific Viewpoint/Status/Practice; Occupational Position Group'\n","df_arabic['TargetAll'] = df_arabic.TargetAll.apply(conflicting_target_0)\n","df_arabic = df_arabic[df_arabic.TargetAll != set()].copy()\n","\n","print(df_arabic['TargetAll'].value_counts())\n","df_arabic= df_arabic.sample(frac = 1)\n","\n","df_arabic\n","\n"],"metadata":{"id":"iiP4O0k0xfYD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define the name and the adress of trained model on HuggingFace\n","\n","hub_id = 'HrantDinkFoundation/'\n","model_id = 'hs-target-prediction'"],"metadata":{"id":"ahU5jLPq-2UP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# select which data used in training\n","\n","# 'only-tr' : use only Turkish data\n","# 'only-ar' : use only arabic data\n","# 'tr-syn'  : use Turkish and synthetic data\n","# 'ar-syn'  : use arabic and synthetic data\n","# 'ar-translated-syn'  : use arabic, synthetic and the translated column of Turkish data\n","\n","mode_train = ['only-tr', 'only-ar', 'tr-syn', 'ar-syn', 'ar-translated-syn']\n","mode = mode_train[2]"],"metadata":{"id":"4NL7OpXAwbw5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if mode =='only-tr':\n","    model_lang = 'tr'\n","    model_name = 'dbmdz/bert-base-turkish-uncased'\n","    df_train = df_turkish[(df_turkish.Div=='train')]\n","    df_test  = df_turkish[(df_turkish.Div=='test')]\n","\n","\n","if mode =='only-ar' :\n","     model_lang = 'ar'\n","     model_name = 'aubmindlab/bert-base-arabert'\n","     df_train = df_arabic[(df_arabic.Div=='train')]\n","     df_test  = df_arabic[(df_arabic.Div=='test')]\n","\n","\n","if mode == 'tr-syn':\n","    model_lang = 'tr'\n","    model_name = 'dbmdz/bert-base-turkish-uncased'\n","    hub_id = hub_id + 'turkish-' + model_id\n","    df_train = df_turkish[(df_turkish.Div=='train')]\n","    df_test  = df_turkish[(df_turkish.Div=='test')]\n","\n","    df_train = pd.concat([df_train, df_syn], ignore_index=True)\n","\n","\n","if mode == 'ar-syn':\n","    model_lang = 'ar'\n","    model_name = 'aubmindlab/bert-base-arabert'\n","    df_train = df_arabic[(df_arabic.Div=='train')]\n","    df_test  = df_arabic[(df_arabic.Div=='test')]\n","\n","    df_syn = df_syn.drop (columns=['Text'])\n","    df_syn.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","    df_train = pd.concat([df_train, df_syn], ignore_index=True)\n","\n","\n","if mode == 'ar-translated-syn' :\n","     model_lang = 'ar'\n","     model_name = 'aubmindlab/bert-base-arabert'\n","     hub_id = hub_id + 'arabic-' + model_id\n","     df_turkish = df_turkish.drop (columns=['Text'])\n","     df_turkish.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","\n","     df_syn = df_syn.drop (columns=['Text'])\n","     df_syn.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","\n","     df_train = df_arabic[(df_arabic.Div=='train')]\n","     df_test  = df_arabic[(df_arabic.Div=='test')]\n","\n","     df_train = pd.concat([df_train, df_turkish, df_syn], ignore_index=True)\n","\n"],"metadata":{"id":"sF2M8ZN2-wtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train"],"metadata":{"id":"E06oD192YYL3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test\n"],"metadata":{"id":"msR_sbt1BSkF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_class = 4\n","column_select = 'TargetAll'\n","k_fold = 10\n"],"metadata":{"id":"H3qB2gQ_BFG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","df_train = df_train[df_train.TargetAll != set()].copy()\n","\n","text = df_train['Text'].tolist()\n","label = df_train[column_select].tolist()\n","label = [str(x) for x in label]\n","text = pre_process_tweets(text)\n","train_data = pd.DataFrame(list(zip(text, label)), columns =['text', 'label'])\n","\n","print(train_data['label'].value_counts())\n","train_data\n"],"metadata":{"id":"gU9qGrSUBYmv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import ast\n","\n","df_test = df_test[df_test.TargetAll != set()].copy()\n","print(df_test['TargetAll'].value_counts())\n","\n","text = df_test['Text'].tolist()\n","label = df_test[column_select].tolist()\n","label = [list(s) for s in label]\n","label = list(map(lambda l: list(np.where(np.isin(np.arange(num_class), l), float(1), float(0))), label))\n","\n","text = pre_process_tweets(text)\n","test_data = pd.DataFrame(list(zip(text, label)), columns =['text', 'label'])\n","#test_data= test_data.astype({'label':'int'})\n","\n","print(test_data['label'].value_counts())\n","test_data"],"metadata":{"id":"TdNbSju9C9uk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mjkcw1fDWToG"},"outputs":[],"source":["# Load BERT model\n","\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from transformers import ElectraForSequenceClassification,  ElectraTokenizerFast\n","from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaTokenizerFast, RobertaForSequenceClassification\n","from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments, GPT2Config\n","\n","labels = ['Target group not specified or not present' ,\n","          'Country/Nationality/Race/Ethnicity' ,\n","          'Religion',\n","          'Gender/Sexual Orientation']\n","\n","id2label = {idx:label for idx, label in enumerate(labels)}\n","label2id = {label:idx for idx, label in enumerate(labels)}\n","\n","pretrained_model_name = model_name\n","print(model_name)\n","\n","tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, do_lower_case=True, force_download=True)\n","model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name, num_labels = num_class, problem_type=\"multi_label_classification\",\n","                                                           force_download=True, id2label=id2label, label2id=label2id).to(device)\n","\n","model.resize_token_embeddings(len(tokenizer))\n","for item in emoji_list :\n","        tokenizer.add_tokens(item)\n","model.resize_token_embeddings(len(tokenizer))\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vh9FzUR427JW"},"outputs":[],"source":["def preprocess_function(examples):\n","    preprocessed = examples[\"text\"]\n","    return tokenizer(preprocessed, truncation=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZfw_lai35A4"},"outputs":[],"source":["from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n","from transformers import EvalPrediction\n","import torch\n","\n","# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n","def multi_label_metrics(predictions, labels, threshold=0.5):\n","    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n","    sigmoid = torch.nn.Sigmoid()\n","    probs = sigmoid(torch.Tensor(predictions))\n","    # next, use threshold to turn them into integer predictions\n","    y_pred = np.zeros(probs.shape)\n","    y_pred[np.where(probs >= threshold)] = 1\n","    # finally, compute metrics\n","    y_true = labels\n","    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n","    try:\n","      roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n","    except ValueError:\n","      pass\n","    accuracy = accuracy_score(y_true, y_pred)\n","    # return as dictionary\n","    metrics = {'f1': f1_micro_average,\n","               'roc_auc': roc_auc,\n","               'accuracy': accuracy}\n","    return metrics\n","\n","def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions[0] if isinstance(p.predictions,\n","            tuple) else p.predictions\n","    result = multi_label_metrics(\n","        predictions=preds,\n","        labels=p.label_ids)\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ntZERAEqvNwt"},"outputs":[],"source":["k_fold = 8\n","skf = StratifiedKFold(n_splits= k_fold, random_state = 31, shuffle=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pdYaTy8NoQi4"},"outputs":[],"source":["class CustomTrainer(Trainer):\n","    def save_model(self, output_dir=None, _internal_call=False):\n","        if output_dir is None:\n","            output_dir = self.args.output_dir\n","\n","        self.model = self.model.to('cuda')\n","\n","        for param in self.model.parameters():\n","            if not param.is_contiguous():\n","                param.data = param.data.contiguous()\n","\n","        super().save_model(output_dir, _internal_call)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c8vqUCVTvSQ0"},"outputs":[],"source":["# Let's start training model\n","\n","for i, (train_index, test_index) in enumerate(skf.split(train_data.text , train_data.label)):\n","\n","    print(\"number of samples in train folds: \", len(train_index))\n","    print(\"number of samples in test fold  : \", len(test_index))\n","\n","    test = train_data.iloc[test_index].set_index(\"text\")\n","    multi_label_test = test['label'].tolist()\n","    multi_label_test = [list(ast.literal_eval(item)) for item in multi_label_test]\n","    label_test = list(map(lambda l: list(np.where(np.isin(np.arange(num_class), l), float(1), float(0))), multi_label_test))\n","    test = test.drop (columns=['label'])\n","    test['label'] = label_test\n","    print(label_test)\n","    test = Dataset.from_pandas(test)\n","\n","\n","    train = train_data.iloc[train_index].set_index(\"text\")\n","    multi_label_train = train['label'].tolist()\n","    multi_label_train = [list(ast.literal_eval(item)) for item in multi_label_train]\n","    label_train = list(map(lambda l: list(np.where(np.isin(np.arange(num_class), l), float(1), float(0))), multi_label_train))\n","    trian = train.drop (columns=['label'])\n","    train['label'] = label_train\n","    train = Dataset.from_pandas(train)\n","\n","\n","    dataset_dict = DatasetDict({\"test\": test, \"train\": train})\n","    preprocessed_dataset_dict = dataset_dict.map(preprocess_function, batched=True)\n","\n","    training_args = TrainingArguments(\n","        output_dir= \"My Hate Speech Detection\",\n","        overwrite_output_dir=True,\n","        #report_to=\"none\",\n","        learning_rate=5e-6,\n","        per_device_train_batch_size=32,\n","        per_device_eval_batch_size=20,\n","        num_train_epochs=10,\n","        weight_decay=0.01,\n","        eval_strategy=\"steps\",\n","        metric_for_best_model = 'f1',\n","        logging_steps = 100,\n","        load_best_model_at_end=True,\n","        greater_is_better=True,\n","        #hub_strategy=\"end\",          # These parameter used when you want saved mmodels directly to HuggingFace,\n","        #push_to_hub=True,            # If you want to save model on huggingFace you must select \"True\"\n","        #hub_model_id= hub_id,        # Write your model address on HuggingFace\n","        #hub_private_repo= False,     # Determine whether the model on Hugging Face should be private or public.\n","        #hub_token= \"...........\",    # To save model into HuggingFace you need \"hub_token\" which can take from your own HuggingFace account\n","    )\n","\n","    trainer = CustomTrainer(\n","        model = model,\n","        args = training_args,\n","        train_dataset=preprocessed_dataset_dict[\"train\"],\n","        eval_dataset=preprocessed_dataset_dict[\"test\"],\n","        tokenizer = tokenizer,\n","        data_collator = data_collator,\n","        compute_metrics = compute_metrics,\n","\n","    )\n","    trainer.train()\n","    break\n","\n","#trainer.save_model()      # If you want save your trained model on HuggingFace\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSHmt9lFDzt9"},"outputs":[],"source":["#Let's start testing model\n","\n","test_ds = Dataset.from_pandas(test_data).map(preprocess_function, batched=True)\n","test_ds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fd_JrO8P4lkc"},"outputs":[],"source":["#Let's start testing model\n","\n","id2label = {idx:label for idx, label in enumerate(labels)}\n","label2id = {label:idx for idx, label in enumerate(labels)}\n","\n","model.eval()\n","\n","y_test = test_data['label'].tolist()\n","y_test = np.array(y_test)\n","y_pred =[]\n","\n","for t in test_data['text'].tolist() :\n","   encoding = tokenizer(t, return_tensors=\"pt\")\n","   encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n","\n","   outputs = trainer.model(**encoding)\n","   logits = outputs.logits\n","   logits.shape\n","   sigmoid = torch.nn.Sigmoid()\n","   probs = sigmoid(logits.squeeze().cpu())\n","   predictions = np.zeros(probs.shape)\n","   predictions[np.where(probs >= 0.3)] = 1\n","   y_pred.append(predictions)\n"]},{"cell_type":"code","source":["label_mapping = {0:'Target group not specified or not present' ,\n","                 1: 'Country/Nationality/Race/Ethnicity' ,\n","                 2: 'Religion',\n","                 3: 'Gender/Sexual Orientation'}"],"metadata":{"id":"uF61JDBQDa2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-fDE4iEj5b2U"},"outputs":[],"source":["from sklearn.metrics import classification_report, f1_score\n","from sklearn.metrics import confusion_matrix\n","\n","print ('-------------------------------------------------------')\n","print(classification_report(y_test, y_pred, digits=num_class, zero_division=0))\n"]},{"cell_type":"code","source":["from sklearn.metrics import classification_report, f1_score\n","from sklearn.metrics import confusion_matrix\n","\n","print ('-------------------------------------------------------')\n","print(classification_report(y_test, y_pred, target_names=labels, zero_division=0))\n"],"metadata":{"id":"W-9d161z1r4q"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGV9C4TRFs_g"},"outputs":[],"source":["# Saving trained model on local drive\n","# Please replace the pathes with your local address path\n","\n","\n","if model_lang == 'tr':\n","          path_save_model = '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/trained_models/hs-tr-target/'\n","\n","if model_lang == 'ar':\n","          path_save_model = '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/trained_models/hs-ar-strength/'\n","\n","trainer.save_model(path_save_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1z-apwd41ER"},"outputs":[],"source":["# Load trained model from local drive\n","# Please add the address where the models are saved\n","\n","from transformers import pipeline\n","\n","path_trained_model = path_save_model\n","model_pipline = pipeline(task = \"text-classification\", model = path_trained_model, top_k=None, return_all_scores=True, device=\"cuda\")"]},{"cell_type":"code","source":["def predict_target_tweet(review):\n","        prediction = []\n","        result = []\n","        predictions = model_pipline (review)\n","        prediction_result = predictions[0]\n","        result = []\n","        # Filter predictions based on the sigmoid threshold\n","        filtered_predictions = [pred for pred in prediction_result if pred['score'] > 0.2]\n","\n","        # Print all labels that meet the threshold\n","        if filtered_predictions:\n","            for item in filtered_predictions:\n","                   result.append ((item['label'], 'score:', item['score']))\n","            #else:\n","                   #result.append (\"No predictions meet the threshold.\")\n","\n","        return result\n"],"metadata":{"id":"BJVReiz4W0cb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["🔴 ▶ **Disclaimer:**\n","\n","This study (project) includes examples of offensive language, hate speech, and profanity due to the nature of the research. These examples do not represent the authors’ views (or developers' views). The goal of this work is to contribute to the detection and prevention of harmful content and violence directed toward refugees, LGBTI+ individuals, religious groups, nationalities, and other vulnerable communities."],"metadata":{"id":"VeDHKb20g8lj"}},{"cell_type":"code","source":["AR_TWEETS_EXAMPLES = [\n","    \"كل اللاجئين لصوص\",\n","    \"ارحلوا من بلدنا\",\n","    \"مكان المرأة هو المطبخ\",\n","    \"اليونان يجب أن تعرف حدودها، وإلا قد نأتي فجأة في ليلة\",\n","    \"لا ينبغي لغير المسلمين أن يصبحوا نوابًا.\",\n","    \"يجب ألا يُسمح للاجئين بالتصويت.\",\n","    \"يجب ألا يستفيد اللاجئون من التأمين الصحي.\",\n","    \"التعليم باللغة الكردية غير مقبول.\",\n","    \"للاجئين أيضًا الحق في الحياة.\",\n","    \"يتم اتخاذ تدابير شاملة ضمن الإطار القانوني لمنع أنشطة التنظيمات الإرهابية.\",\n","    \"تم الإعلان عن تواريخ الفعاليات الثقافية التي ستُقام هذا العام في إسرائيل.\",\n","    \"تفتتح رائدات الأعمال صالونات تجميل تستهدف العملاء من المسلمين وغير المسلمين.\",\n","    \"تُجرى الاستعدادات في الأسواق في سوريا قبل الأعياد الإسلامية والمسيحية.\",\n","    \"هل كون بعض الكلاب مسعورة أو عدوانية يبرر قتل جميع الكلاب؟ اسحبوا القانون.\",\n","     \"يجب على النساء دعم بعضهن البعض\",\n","    \"اشتريت تفاحًا من السوق.\",\n","    \"طعام السوريين لذيذ جدًا.\",\n","    \"الطعام السوري شهي للغاية.\",\n","    \"أنا أحب السوريين كثيرًا، إنهم أشخاص لطيفون جدًا.\",\n","    \"السنغافوريون يعملون بجد.\",\n","    \"أنا أحب الأمريكيين كثيرًا\",\n","    \"أنا لا أحب الملحدين على الإطلاق\",\n","    \"المسيحيين الأرثوذكس\",\n","    \"سنغافورة بلد جميل جدًا\"   ,\n","    \"ذهبت إلى المستشفى أمس، ثم تناولت وجبة طعام لذيذة، ثم عدت إلى المنزل، ولعبت مع ابنتي، وشاهدت مسلسلًا أمريكيًا\",\n","    \"أنا أحب الحيوانات كثيرًا\"\n","]\n","\n","TR_TWEETS_EXAMPLES = [\n","    \"mültecilerin hepsi hırsızdır\",\n","    \"defolun bizim ülkemizden\",\n","    \"kadının yeri mutfaktır\",\n","    \"Yunanistan haddini bilecek yoksa bir gece ansizin gelebiliriz\",\n","    \"Gayrimüslimler milletvekili olmasın.\",\n","    \"Mülteciler oy kullanmasın.\",\n","    \"Mültecilere oy hakki verilmemeli\",\n","    \"Mülteciler sgk'dan yararlanmasın.\",\n","    \"Kürtçe eğitim kabul edilemez.\",\n","    \"Mültecilerin de yaşamaya hakkı var.\",\n","    \"Terör örgütlerinin faaliyetlerini önlemek için hukuki çerçevede kapsamlı önlemler alınmaktadır.\",\n","    \"İsrail'de bu yıl düzenlenecek kültürel etkinliklerin tarihleri açıklandı\",\n","    \"Kadın girişimciler, hem Müslüman hem de diğer topluluklardan müşterilere hitap eden güzellik salonları açıyor.\",\n","    \"Suriye’deki pazarlarda Müslüman ve Hristiyan bayramları öncesinde hazırlıklar yapılıyor.\",\n","    \"Bazı köpeklerin kuduz ya da saldırgan olması, tüm köpeklerin öldürülmesi için haklı bir sebep midir? Yasayı geri çek.\",\n","    \"Kadınlar birbirlerine destek olmalı.\",\n","    \"Pazardan elma aldim.\",\n","    \"Suriyelilerin yemekleri çok güzel.\",\n","    \"Suriye yemekleri çok lezzetli.\",\n","    \"Süriyelileri çok seviyorum, çok sempatik insanlar.\",\n","    \"Singapurlular çok çalışkan\",\n","    \"Amerikalıları çok Seviyorum\",\n","    \"Ateistleri hiç sevmiyorum\",\n","    \"Ortodoksalar hrıstiyanlar\",\n","    \"Singapur çok güzel bir ülke dir\",\n","    \"Dün hastaneye gittim sonra da bir güzel yemek yedim, eve döndüm, kızımla oyun oynadım, Amerikan dizi izledim\",\n","    \"hayvanlari çok seviyorum\"\n","]\n","\n","if model_lang == 'ar':\n","      TWEETS_EXAMPLES = AR_TWEETS_EXAMPLES\n","\n","if model_lang == 'tr':\n","      TWEETS_EXAMPLES = TR_TWEETS_EXAMPLES\n","\n","\n","for tweet in TWEETS_EXAMPLES:\n","    target = predict_target_tweet(remove_punctuation_marks(tweet.lower()))\n","    print(f\"Tweet:    {tweet}\")\n","    print(f\"Target: {target}\")\n","    print(\"-\" * 60)\n"],"metadata":{"id":"wH0rs_PIWZJs"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyN4JNs6VEI02/1UlXpu5Fgi"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}