{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wFXTGNTSP-K0"},"outputs":[],"source":["from google.colab import files\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","root_path = 'gdrive/My Drive/'\n","\n","import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","! nvcc --version\n","!nvidia-smi -L\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAAU3hinHKRQ"},"outputs":[],"source":["!pip install -U transformers\n","#!pip install transformers==4.28.0\n","!pip install datasets\n","!pip install snscrape\n","!pip install --upgrade accelerate\n","!pip install datasets evaluate\n","!pip install accelerate -U\n","\n","from scipy.spatial.distance import cosine\n","from transformers import AutoModel, AutoTokenizer\n","import numpy as np\n","import random\n","import csv\n","import pandas as pd\n","from sklearn.model_selection import StratifiedKFold\n","import tensorflow as tf\n","import datasets\n","from datasets import Dataset, DatasetDict\n","import evaluate\n","from sklearn.metrics import classification_report, f1_score\n","\n","import sys, os\n","import torch\n","import json\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9vyLbRzuVICq"},"outputs":[],"source":["# preprocess functions\n","\n","def remove_url(text):\n","    return ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\",\" \", text).split())\n","\n","def remove_username(text):\n","    return ' '.join(re.sub(\"([@][A-Za-z0-9_]+)\",\" \", text).split())\n","\n","def remove_at_mark(text):\n","    return re.sub(r'[@]', ' ', text)\n","\n","def remove_tag_mark(text):\n","    return re.sub(r'[#]', ' ', text)\n","\n","def replace_hashtags_with_segments (text):\n","   regex = \"#(\\w+)\"\n","   text_split = text.split()\n","   sent= ''\n","   strip_word =' '\n","   for word in text_split:\n","     hashtag= re.findall(regex, word)\n","     #print(hashtag)\n","     #print(strip_word)\n","\n","     if (len(hashtag)) == 0 :\n","        sent = sent + ' ' + word\n","     if (len(hashtag))!=0:\n","        strip_word = word.replace(('#'+hashtag[0]), ' ')\n","        #print (hashtag[0])\n","        #print(strip_word)\n","        try :\n","           key = '#'+ hashtag[0]\n","           segmented = segmented_dict [key]\n","           if type(segmented) == str:\n","                    #segmented = re.sub(r'[|]', ' ', segmented)\n","                    #segmented = re.sub(r'[,]', ' ', segmented)\n","                    sent= sent + ' ' + segmented\n","           if type(segmented) == list:\n","                    for k in segmented:\n","                      sent = sent +' ' + k\n","        except:\n","           sent = sent + ' ' +  hashtag[0]\n","   return sent + ' ' + strip_word\n","\n","def control_tag_mark(text):\n","     s = ' '\n","     for word in text.split():\n","            word = re.sub(r'[#]', ' #', word)\n","            s = s + ' ' + word\n","     return s\n","\n","def remove_punctuation_marks(text):\n","   punc = '''!()-[]{};:'\"\\,<>./?$%^&*~'''\n","   for i in text:\n","       if i in punc:\n","         text = text.replace(i, \" \")\n","   return text\n","\n","\n","\n","def remove_underline(text):\n","  punc = '''_'''\n","  for i in text:\n","       if i in punc:\n","         text = text.replace(i, \"\")\n","  return text\n","\n","def remove_hashtag(text):\n","    return ' '.join(re.sub(\"([#][A-Za-z0-9_]+)\",\" \", text).split())\n","\n","def remove_turkish_hashtag(text):\n","    return ' '.join(re.sub(\"([#][\\w+_]+)\",\" \", text).split())\n","\n","def find_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    return re.findall(emoji_pattern, text) # no emoji\n","\n","def remove_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    return (emoji_pattern.sub(r'', text)) # no emoji\n","\n","def remove_arabic_hashtags(text):\n","   return ' '.join(re.sub('([#][\\u0600-\\u06ff_]+)', \" \", text).split())\n","\n","def remove_emoji_duplicate (text):\n","      emoji =[]\n","      rest = ''\n","      main_text = []\n","      for w in text :\n","         x = find_emoji(w)\n","         if len (x) != 0 :\n","            emoji.append (x[0])\n","      #print(emoji)\n","      rest = remove_emoji(text)\n","      emoji = list(set(emoji))\n","      for e in emoji :\n","        try:\n","          rest = rest + \" \" + e\n","        except:\n","          rest = rest\n","      main_text.append(rest)\n","      return main_text[0]\n","\n","def lower_case(text):\n","    text = text.replace(\"I\", \"Ä±\")\n","    text = text.replace(\"Ä°\", \"i\")\n","    return text.lower()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAAVod3AVjod"},"outputs":[],"source":["def seed_everything(seed):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","seed_everything(0)"]},{"cell_type":"code","source":["# define data path\n","# Please replace the addresses with the address of dataset on your local drive\n","\n","path_Turkish_data     =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/df_turkish_cats_targets_degrees_final_with_translated_all.csv'\n","path_arabic_data      =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/df_arabic_cats_targets_degrees_final.csv'\n","path_synthetic_data   =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/synthetic_data_all_new.xlsx'\n","\n","path_all_emoji =         '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/all_emoji_dict.json'\n","path_hashtag_segment  =  '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/data/all_hashtag_segmented_tr.json'"],"metadata":{"id":"KY6iREuAvDHU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gn-0qMMmVyjS"},"outputs":[],"source":["# Laod emoji_dict and hashtag_dict\n","\n","f = open(path_all_emoji)\n","data = json.load(f)\n","emoji_list = data['emojis']\n","\n","f = open(path_hashtag_segment)\n","segmented_dict = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbINt_v7XOKE"},"outputs":[],"source":["# Select which preprocess steps must be apply on data\n","\n","def pre_process_tweets(text_list):\n","   cleaned_text =[]\n","   for s in text_list:\n","       #s = lower_case(s)\n","       s = s.lower()\n","       s = remove_username(s)\n","       s = remove_at_mark(s)\n","       s = remove_url(s)\n","       s = remove_tag_mark(s)\n","       #s = control_tag_mark(s)\n","       s = remove_punctuation_marks(s)\n","       s = remove_underline(s)\n","       #s = replace_hashtags_with_segments(s)\n","       #s = remove_tag_mark(s)\n","       cleaned_text.append(s)\n","   return cleaned_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ZlH3j_avBwC"},"outputs":[],"source":["# Load synthetic data\n","\n","df_syn = pd.read_excel(path_synthetic_data)\n","\n","df_syn = df_syn.iloc[:,[4,5,6,7,9]]\n","\n","print(df_syn['CatComb6class'].value_counts())\n","print(df_syn['CatComb4class'].value_counts())\n","print(df_syn['CatComb2class'].value_counts())\n","\n","df_syn= df_syn.sample(frac = 1)\n","df_syn\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3HBA7XJvwx4"},"outputs":[],"source":["# Load Turkish data\n","\n","df_turkish = pd.read_csv(path_Turkish_data)\n","df_turkish = df_turkish.iloc[:,[2, 27, 29, 31, 32,38, 39]]\n","\n","print(df_turkish['CatComb6class'].value_counts())\n","print(df_turkish['CatComb4class'].value_counts())\n","print(df_turkish['CatComb2class'].value_counts())\n","\n","df_turkish"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOSLlHxvw1J1"},"outputs":[],"source":["# Load Arabic data\n","\n","df_arabic = pd.read_csv(path_arabic_data)\n","df_arabic = df_arabic.iloc[:,[2, 27, 29, 31, 32, 38]]\n","\n","df_arabic"]},{"cell_type":"code","source":["# select numumber of classes  [2, 4, 6]\n","# select k-fold for spliting train and validation data\n","\n","num_class = 2\n","k_fold = 11"],"metadata":{"id":"-UQAa1HPTezy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0-_R31GqByY"},"outputs":[],"source":["# based on number of classes : select the label column and the name and the adress of trained model on Huggingface\n","\n","if num_class == 2:\n","     column_select = 'CatComb2class'\n","     labels = ['No hate Speech', 'Hate Speech']\n","     sentiment_mapping =  {'No hate Speech', 'Hate Speech'}\n","     hub_id = 'HrantDinkFoundation/'\n","     model_id = 'hs-2class-prediction'\n","\n","\n","\n","if num_class == 4:\n","     column_select = 'CatComb4class'\n","\n","     labels = ['No Hate Detected',\n","               'Mild Level (Discriminatory Discourse)',\n","               'Moderate Level (Exaggeration, Generalization, Attribution, Distortion, Symbolization)',\n","               'Severe Level (Swearing, Insult, Defamation, Dehumanization, Threat of Enmity/War/Attack/Murder/Harm)']\n","\n","     hub_id = 'HrantDinkFoundation/'\n","     model_id = 'hs-4class-prediction'\n","\n","     sentiment_mapping = {'No Hate Detected',\n","                          'Mild Level (Discriminatory Discourse)',\n","                          'Moderate Level (Exaggeration, Generalization, Attribution, Distortion, Symbolization)',\n","                          'Severe Level (Swearing, Insult, Defamation, Dehumanization, Threat of Enmity/War/Attack/Murder/Harm)',\n","                          }\n","\n","\n","if num_class == 6:\n","     column_select = 'CatComb6class'\n","     labels = ['No Hate Detected',\n","               'Mild Level (Discriminatory Discourse)',\n","               'Moderate Level (Exaggeration, Generalization, Attribution, Distortion)',\n","               'Elevated Level (Symbolization)',\n","               'Severe Level (Swearing, Insult, Defamation, Dehumanization)',\n","               'Extreme Level (Threat of Enmity/War/Attack/Murder/Harm)']\n","\n","     hub_id = 'HrantDinkFoundation/'\n","     model_id = 'hs-6class-prediction'\n","\n","     sentiment_mapping = {'No Hate Detected',\n","                          'Mild Level (Discriminatory Discourse)',\n","                          'Moderate Level (Exaggeration, Generalization, Attribution, Distortion)',\n","                          'Elevated Level (Symbolization)',\n","                          'Severe Level (Swearing, Insult, Defamation, Dehumanization)',\n","                          'Extreme Level (Threat of Enmity/War/Attack/Murder/Harm)',\n","                          }\n","\n","\n"]},{"cell_type":"code","source":["# select which data used in training\n","\n","# 'only-tr' : use only Turkish data\n","# 'only-ar' : use only arabic data\n","# 'tr-syn'  : use Turkish and synthetic data\n","# 'ar-syn'  : use arabic and synthetic data\n","# 'ar-translated-syn'  : use arabic, synthetic and the translated column of Turkish data\n","\n","mode_train = ['only-tr', 'only-ar', 'tr-syn' , 'ar-syn', 'ar-translated-syn']\n","mode = mode_train [2]"],"metadata":{"id":"iJTR8qE90beO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z-pbvvDE1ZeJ"},"outputs":[],"source":["if mode =='only-tr':\n","    model_lang = 'tr'\n","    model_name = 'dbmdz/bert-base-turkish-uncased'\n","    hub_id = hub_id + 'turkish-' + model_id\n","\n","    df_train = df_turkish[(df_turkish.Div=='train')]\n","    df_test  = df_turkish[(df_turkish.Div=='test')]\n","\n","if mode =='only-ar' :\n","     model_lang = 'ar'\n","     model_name = 'aubmindlab/bert-base-arabert'\n","\n","     hub_id = hub_id + 'arabic-' + model_id\n","     df_train = df_arabic[(df_arabic.Div=='train')]\n","     df_test  = df_arabic[(df_arabic.Div=='test')]\n","\n","\n","if mode == 'tr-syn':\n","    model_lang = 'tr'\n","    model_name = 'dbmdz/bert-base-turkish-uncased'\n","    hub_id = hub_id + 'turkish-' + model_id\n","    df_train = df_turkish[(df_turkish.Div=='train')]\n","    df_test  = df_turkish[(df_turkish.Div=='test')]\n","\n","    df_train = pd.concat([df_train, df_syn], ignore_index=True)\n","\n","\n","if mode == 'ar-syn':\n","    model_lang = 'ar'\n","    model_name = 'aubmindlab/bert-base-arabert'\n","    hub_id = hub_id + 'arabic-' + model_id\n","\n","    df_train = df_arabic[(df_arabic.Div=='train')]\n","    df_test  = df_arabic[(df_arabic.Div=='test')]\n","\n","    df_syn = df_syn.drop (columns=['Text'])\n","    df_syn.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","    df_train = pd.concat([df_train, df_syn], ignore_index=True)\n","\n","\n","if mode == 'ar-translated-syn':\n","    model_lang = 'ar'\n","    model_name = 'aubmindlab/bert-base-arabert'\n","    hub_id = hub_id + 'arabic-' + model_id\n","\n","    df_turkish = df_turkish.drop (columns=['Text'])\n","    df_turkish.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","\n","    df_syn = df_syn.drop (columns=['Text'])\n","    df_syn.rename(columns={'translated_text_arabic': 'Text'}, inplace=True)\n","\n","    df_train_arabic = df_arabic[(df_arabic.Div=='train')]\n","    df_test  = df_arabic[(df_arabic.Div=='test')]\n","\n","    df_train = pd.concat([df_train_arabic , df_turkish, df_syn], ignore_index=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OtGrwB0qgsx"},"outputs":[],"source":["df_train\n"]},{"cell_type":"code","source":["df_test"],"metadata":{"id":"tuAHXCdl6nUB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q7v9UKUu_0SY"},"outputs":[],"source":["# Selecet the samples of training and testing data where the majority voting is clear\n","\n","if num_class == 2:\n","     column_select = 'CatComb2class'\n","     df_train = df_train[df_train.CatComb2class != set()].copy()\n","     df_test  = df_test[df_test.CatComb2class != set()].copy()\n","\n","     df_train = df_train[df_train.CatComb2class.apply(len)==3]\n","     df_test  = df_test[df_test.CatComb2class.apply(len)==3]\n","\n","\n","if num_class == 4:\n","     column_select = 'CatComb4class'\n","     df_train = df_train[df_train.CatComb4class != set()].copy()\n","     df_test  = df_test[df_test.CatComb4class != set()].copy()\n","\n","     df_train = df_train[df_train.CatComb4class.apply(len)==3]\n","     df_test  = df_test[df_test.CatComb4class.apply(len)==3]\n","\n","if num_class == 6:\n","     column_select = 'CatComb6class'\n","     df_train = df_train[df_train.CatComb6class != set()].copy()\n","     df_test  = df_test[df_test.CatComb6class != set()].copy()\n","\n","     df_train = df_train[df_train.CatComb6class.apply(len)==3]\n","     df_test  = df_test[df_test.CatComb6class.apply(len)==3]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pgeiz9nGnKQB"},"outputs":[],"source":["# preprocess train data\n","\n","import ast\n","\n","text = df_train['Text'].tolist()\n","label = df_train[column_select].tolist()\n","label = [int(i.strip('{}')) for i in label]\n","\n","text = pre_process_tweets(text)\n","train_data = pd.DataFrame(list(zip(text, label)), columns =['text', 'label'])\n","train_data= train_data.astype({'label':'int'})\n","\n","\n","print(train_data['label'].value_counts())\n","train_data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yoX6nmbsj0wB"},"outputs":[],"source":["# preprocess test data\n","\n","text = df_test['Text'].tolist()\n","label = df_test[column_select].tolist()\n","label = [int(i.strip('{}')) for i in label]\n","\n","text = pre_process_tweets(text)\n","test_data = pd.DataFrame(list(zip(text, label)), columns =['text', 'label'])\n","test_data= test_data.astype({'label':'int'})\n","\n","\n","\n","print(test_data['label'].value_counts())\n","test_data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mjkcw1fDWToG"},"outputs":[],"source":["# Load BERT model\n","\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from transformers import ElectraForSequenceClassification,  ElectraTokenizerFast\n","from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaTokenizerFast, RobertaForSequenceClassification\n","from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments, GPT2Config\n","\n","id2label = {idx:label for idx, label in enumerate(labels)}\n","label2id = {label:idx for idx, label in enumerate(labels)}\n","\n","pretrained_model_name = model_name\n","print(model_name)\n","\n","tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, do_lower_case=True, force_download=True)\n","model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name, num_labels = num_class, force_download=True, id2label=id2label, label2id=label2id ).to(device)   # id2label=id2label, label2id=label2id\n","model.resize_token_embeddings(len(tokenizer))\n","for item in emoji_list :\n","        tokenizer.add_tokens(item)\n","model.resize_token_embeddings(len(tokenizer))\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vh9FzUR427JW"},"outputs":[],"source":["def preprocess_function(examples):\n","    preprocessed = examples[\"text\"]\n","    return tokenizer(preprocessed, truncation=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZfw_lai35A4"},"outputs":[],"source":["def compute_metrics(eval_pred):\n","\n","    metric4 = evaluate.load(\"accuracy\")\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    accuracy = metric4.compute(predictions=predictions, references=labels)[\"accuracy\"]\n","    macro_f1 = (f1_score(labels, predictions, average =\"macro\"))\n","\n","    return {\"accuracy\":accuracy, \"macro_f1\":macro_f1}\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ntZERAEqvNwt"},"outputs":[],"source":["skf = StratifiedKFold(n_splits= k_fold, random_state = 31, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pdYaTy8NoQi4"},"outputs":[],"source":["class CustomTrainer(Trainer):\n","    def save_model(self, output_dir=None, _internal_call=False):\n","        if output_dir is None:\n","            output_dir = self.args.output_dir\n","\n","        self.model = self.model.to('cuda')\n","\n","        for param in self.model.parameters():\n","            if not param.is_contiguous():\n","                param.data = param.data.contiguous()\n","\n","        super().save_model(output_dir, _internal_call)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c8vqUCVTvSQ0"},"outputs":[],"source":["# Let's start training model\n","\n","for i, (train_index, test_index) in enumerate(skf.split(train_data.text , train_data.label)):\n","\n","    print(\"number of samples in train folds: \", len(train_index))\n","    print(\"number of samples in test fold  : \", len(test_index))\n","\n","    test = Dataset.from_pandas(train_data.iloc[test_index].set_index(\"text\"))\n","    train = Dataset.from_pandas(train_data.iloc[train_index].set_index(\"text\"))\n","    dataset_dict = DatasetDict({\"test\": test, \"train\": train})\n","    preprocessed_dataset_dict = dataset_dict.map(preprocess_function, batched=True)\n","\n","    training_args = TrainingArguments(\n","        \"My Hate Speech Detection\",\n","        overwrite_output_dir=True,\n","        learning_rate=5e-6,\n","        per_device_train_batch_size=16,\n","        per_device_eval_batch_size=20,\n","        num_train_epochs=10,\n","        weight_decay=0.01,\n","        eval_strategy=\"steps\",\n","        metric_for_best_model = 'accuracy',\n","        logging_steps = 100,\n","        load_best_model_at_end=True,\n","        greater_is_better=True,\n","        #hub_strategy=\"end\",          # These parameter used when you want saved mmodels directly to HuggingFace,\n","        #push_to_hub=True,            # If you want to save model on huggingFace you must select \"True\"\n","        #hub_model_id= hub_id,        # Write your model address on HuggingFace\n","        #hub_private_repo= False,     # Determine whether the model on Hugging Face should be private or public.\n","        #hub_token= \"...........\",    # To save model into HuggingFace you need \"hub_token\" which can take from your own HuggingFace account\n","    )\n","\n","    trainer = CustomTrainer(\n","        model = model,\n","        args = training_args,\n","        train_dataset=preprocessed_dataset_dict[\"train\"],\n","        eval_dataset=preprocessed_dataset_dict[\"test\"],\n","        tokenizer = tokenizer,\n","        data_collator = data_collator,\n","        compute_metrics = compute_metrics,\n","\n","    )\n","    trainer.train()\n","    break\n","\n","#trainer.save_model()              # If you want save your trained model on HuggingFace\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSHmt9lFDzt9"},"outputs":[],"source":["# Let's start testing model\n","\n","test_ds = Dataset.from_pandas(test_data).map(preprocess_function, batched=True)\n","test_ds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2O_oLx5c-vc-"},"outputs":[],"source":["predictions = trainer.predict(test_ds).predictions\n","predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGV9C4TRFs_g"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","\n","y_pred = tf.argmax(predictions, axis=1).numpy()\n","y_test= np.array(test_data['label'])\n","\n","#print(title)\n","print(confusion_matrix(y_test, y_pred))\n","print('-------------------------------------------------------')\n","print('macro-f1:', f1_score(y_test, y_pred, average =\"macro\"))\n","print('accuracy:', f1_score(y_test, y_pred, average =\"micro\"))"]},{"cell_type":"code","source":["from sklearn.metrics import classification_report, f1_score\n","from sklearn.metrics import confusion_matrix\n","\n","con_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred, num_classes=num_class)\n","#print(title)\n","print(con_mat)\n","\n","print('-------------------------------------------------------')\n","print(classification_report(y_test, y_pred, digits=num_class, zero_division=0))"],"metadata":{"id":"AQHOFgde8azg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1z-apwd41ER"},"outputs":[],"source":["# Saving trained model on local drive\n","# Please replace the pathes with your local address path\n","\n","\n","if model_lang == 'tr':\n","          path_save_model = '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/trained_models/hs-tr-' + str(num_class) + 'class/'\n","\n","if model_lang == 'ar':\n","          path_save_model = '/content/gdrive/My Drive/Colab Notebooks/my_hate_speech/all_data_work/final_all_data/HS_GitHub_data_codes/trained_models/hs-ar-' + str(num_class) + 'class/'\n","\n","\n","trainer.save_model(path_save_model)"]},{"cell_type":"code","source":["# Load trained model from local drive\n","\n","from transformers import pipeline\n","\n","path_trained_model = path_save_model\n","\n","model_pipline = pipeline(task = \"text-classification\", model =path_trained_model, device=\"cuda\")"],"metadata":{"id":"EK9WAwn1UfT4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2iMsNqe0bxzI"},"outputs":[],"source":["# Classify example teewts using trained model\n","\n","def classify_tweet(text):\n","        prediction = []\n","        prediction = (model_pipline (text))\n","        prediction = prediction[0][\"label\"]\n","        return prediction"]},{"cell_type":"markdown","source":["ğŸ”´ â–¶ **Disclaimer:**\n","\n","This study (project) includes examples of offensive language, hate speech, and profanity due to the nature of the research. These examples do not represent the authorsâ€™ views (or developers' views). The goal of this work is to contribute to the detection and prevention of harmful content and violence directed toward refugees, LGBTI+ individuals, religious groups, nationalities, and other vulnerable communities."],"metadata":{"id":"OLxPeLA4jl9K"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"w9xJCvj7b_ey"},"outputs":[],"source":["AR_TWEETS_EXAMPLES = [\n","    \"ÙƒÙ„ Ø§Ù„Ù„Ø§Ø¬Ø¦ÙŠÙ† Ù„ØµÙˆØµ\",\n","    \"Ø§Ø±Ø­Ù„ÙˆØ§ Ù…Ù† Ø¨Ù„Ø¯Ù†Ø§\",\n","    \"Ù…ÙƒØ§Ù† Ø§Ù„Ù…Ø±Ø£Ø© Ù‡Ùˆ Ø§Ù„Ù…Ø·Ø¨Ø®\",\n","    \"Ø§Ù„ÙŠÙˆÙ†Ø§Ù† ÙŠØ¬Ø¨ Ø£Ù† ØªØ¹Ø±Ù Ø­Ø¯ÙˆØ¯Ù‡Ø§ØŒ ÙˆØ¥Ù„Ø§ Ù‚Ø¯ Ù†Ø£ØªÙŠ ÙØ¬Ø£Ø© ÙÙŠ Ù„ÙŠÙ„Ø©\",\n","    \"Ù„Ø§ ÙŠÙ†Ø¨ØºÙŠ Ù„ØºÙŠØ± Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† Ø£Ù† ÙŠØµØ¨Ø­ÙˆØ§ Ù†ÙˆØ§Ø¨Ù‹Ø§.\",\n","    \"ÙŠØ¬Ø¨ Ø£Ù„Ø§ ÙŠÙØ³Ù…Ø­ Ù„Ù„Ø§Ø¬Ø¦ÙŠÙ† Ø¨Ø§Ù„ØªØµÙˆÙŠØª.\",\n","    \"ÙŠØ¬Ø¨ Ø£Ù„Ø§ ÙŠØ³ØªÙÙŠØ¯ Ø§Ù„Ù„Ø§Ø¬Ø¦ÙˆÙ† Ù…Ù† Ø§Ù„ØªØ£Ù…ÙŠÙ† Ø§Ù„ØµØ­ÙŠ.\",\n","    \"Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ±Ø¯ÙŠØ© ØºÙŠØ± Ù…Ù‚Ø¨ÙˆÙ„.\",\n","    \"Ù„Ù„Ø§Ø¬Ø¦ÙŠÙ† Ø£ÙŠØ¶Ù‹Ø§ Ø§Ù„Ø­Ù‚ ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø©.\",\n","    \"ÙŠØªÙ… Ø§ØªØ®Ø§Ø° ØªØ¯Ø§Ø¨ÙŠØ± Ø´Ø§Ù…Ù„Ø© Ø¶Ù…Ù† Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù„Ù…Ù†Ø¹ Ø£Ù†Ø´Ø·Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ…Ø§Øª Ø§Ù„Ø¥Ø±Ù‡Ø§Ø¨ÙŠØ©.\",\n","    \"ØªÙ… Ø§Ù„Ø¥Ø¹Ù„Ø§Ù† Ø¹Ù† ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„ÙØ¹Ø§Ù„ÙŠØ§Øª Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ© Ø§Ù„ØªÙŠ Ø³ØªÙÙ‚Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ø¹Ø§Ù… ÙÙŠ Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„.\",\n","    \"ØªÙØªØªØ­ Ø±Ø§Ø¦Ø¯Ø§Øª Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ ØµØ§Ù„ÙˆÙ†Ø§Øª ØªØ¬Ù…ÙŠÙ„ ØªØ³ØªÙ‡Ø¯Ù Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ù…Ù† Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ† ÙˆØºÙŠØ± Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ†.\",\n","    \"ØªÙØ¬Ø±Ù‰ Ø§Ù„Ø§Ø³ØªØ¹Ø¯Ø§Ø¯Ø§Øª ÙÙŠ Ø§Ù„Ø£Ø³ÙˆØ§Ù‚ ÙÙŠ Ø³ÙˆØ±ÙŠØ§ Ù‚Ø¨Ù„ Ø§Ù„Ø£Ø¹ÙŠØ§Ø¯ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© ÙˆØ§Ù„Ù…Ø³ÙŠØ­ÙŠØ©.\",\n","    \"Ù‡Ù„ ÙƒÙˆÙ† Ø¨Ø¹Ø¶ Ø§Ù„ÙƒÙ„Ø§Ø¨ Ù…Ø³Ø¹ÙˆØ±Ø© Ø£Ùˆ Ø¹Ø¯ÙˆØ§Ù†ÙŠØ© ÙŠØ¨Ø±Ø± Ù‚ØªÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ø§Ø¨ØŸ Ø§Ø³Ø­Ø¨ÙˆØ§ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†.\",\n","     \"ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø³Ø§Ø¡ Ø¯Ø¹Ù… Ø¨Ø¹Ø¶Ù‡Ù† Ø§Ù„Ø¨Ø¹Ø¶\",\n","    \"Ø§Ø´ØªØ±ÙŠØª ØªÙØ§Ø­Ù‹Ø§ Ù…Ù† Ø§Ù„Ø³ÙˆÙ‚.\",\n","    \"Ø·Ø¹Ø§Ù… Ø§Ù„Ø³ÙˆØ±ÙŠÙŠÙ† Ù„Ø°ÙŠØ° Ø¬Ø¯Ù‹Ø§.\",\n","    \"Ø§Ù„Ø·Ø¹Ø§Ù… Ø§Ù„Ø³ÙˆØ±ÙŠ Ø´Ù‡ÙŠ Ù„Ù„ØºØ§ÙŠØ©.\",\n","    \"Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ø³ÙˆØ±ÙŠÙŠÙ† ÙƒØ«ÙŠØ±Ù‹Ø§ØŒ Ø¥Ù†Ù‡Ù… Ø£Ø´Ø®Ø§Øµ Ù„Ø·ÙŠÙÙˆÙ† Ø¬Ø¯Ù‹Ø§.\",\n","    \"Ø§Ù„Ø³Ù†ØºØ§ÙÙˆØ±ÙŠÙˆÙ† ÙŠØ¹Ù…Ù„ÙˆÙ† Ø¨Ø¬Ø¯.\",\n","    \"Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠÙŠÙ† ÙƒØ«ÙŠØ±Ù‹Ø§\",\n","    \"Ø£Ù†Ø§ Ù„Ø§ Ø£Ø­Ø¨ Ø§Ù„Ù…Ù„Ø­Ø¯ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚\",\n","    \"Ø§Ù„Ù…Ø³ÙŠØ­ÙŠÙŠÙ† Ø§Ù„Ø£Ø±Ø«ÙˆØ°ÙƒØ³\",\n","    \"Ø³Ù†ØºØ§ÙÙˆØ±Ø© Ø¨Ù„Ø¯ Ø¬Ù…ÙŠÙ„ Ø¬Ø¯Ù‹Ø§\"   ,\n","    \"Ø°Ù‡Ø¨Øª Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ Ø£Ù…Ø³ØŒ Ø«Ù… ØªÙ†Ø§ÙˆÙ„Øª ÙˆØ¬Ø¨Ø© Ø·Ø¹Ø§Ù… Ù„Ø°ÙŠØ°Ø©ØŒ Ø«Ù… Ø¹Ø¯Øª Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù†Ø²Ù„ØŒ ÙˆÙ„Ø¹Ø¨Øª Ù…Ø¹ Ø§Ø¨Ù†ØªÙŠØŒ ÙˆØ´Ø§Ù‡Ø¯Øª Ù…Ø³Ù„Ø³Ù„Ù‹Ø§ Ø£Ù…Ø±ÙŠÙƒÙŠÙ‹Ø§\",\n","    \"Ø£Ù†Ø§ Ø£Ø­Ø¨ Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙƒØ«ÙŠØ±Ù‹Ø§\"\n","]\n","\n","TR_TWEETS_EXAMPLES = [\n","    \"mÃ¼ltecilerin hepsi hÄ±rsÄ±zdÄ±r\",\n","    \"defolun bizim Ã¼lkemizden\",\n","    \"kadÄ±nÄ±n yeri mutfaktÄ±r\",\n","    \"Yunanistan haddini bilecek yoksa bir gece ansizin gelebiliriz\",\n","    \"GayrimÃ¼slimler milletvekili olmasÄ±n.\",\n","    \"MÃ¼lteciler oy kullanmasÄ±n.\",\n","    \"MÃ¼ltecilere oy hakki verilmemeli\",\n","    \"MÃ¼lteciler sgk'dan yararlanmasÄ±n.\",\n","    \"KÃ¼rtÃ§e eÄŸitim kabul edilemez.\",\n","    \"MÃ¼ltecilerin de yaÅŸamaya hakkÄ± var.\",\n","    \"TerÃ¶r Ã¶rgÃ¼tlerinin faaliyetlerini Ã¶nlemek iÃ§in hukuki Ã§erÃ§evede kapsamlÄ± Ã¶nlemler alÄ±nmaktadÄ±r.\",\n","    \"Ä°srail'de bu yÄ±l dÃ¼zenlenecek kÃ¼ltÃ¼rel etkinliklerin tarihleri aÃ§Ä±klandÄ±\",\n","    \"KadÄ±n giriÅŸimciler, hem MÃ¼slÃ¼man hem de diÄŸer topluluklardan mÃ¼ÅŸterilere hitap eden gÃ¼zellik salonlarÄ± aÃ§Ä±yor.\",\n","    \"Suriyeâ€™deki pazarlarda MÃ¼slÃ¼man ve Hristiyan bayramlarÄ± Ã¶ncesinde hazÄ±rlÄ±klar yapÄ±lÄ±yor.\",\n","    \"BazÄ± kÃ¶peklerin kuduz ya da saldÄ±rgan olmasÄ±, tÃ¼m kÃ¶peklerin Ã¶ldÃ¼rÃ¼lmesi iÃ§in haklÄ± bir sebep midir? YasayÄ± geri Ã§ek.\",\n","    \"KadÄ±nlar birbirlerine destek olmalÄ±.\",\n","    \"Pazardan elma aldim.\",\n","    \"Suriyelilerin yemekleri Ã§ok gÃ¼zel.\",\n","    \"Suriye yemekleri Ã§ok lezzetli.\",\n","    \"SÃ¼riyelileri Ã§ok seviyorum, Ã§ok sempatik insanlar.\",\n","    \"Singapurlular Ã§ok Ã§alÄ±ÅŸkan\",\n","    \"AmerikalÄ±larÄ± Ã§ok Seviyorum\",\n","    \"Ateistleri hiÃ§ sevmiyorum\",\n","    \"Ortodoksalar hrÄ±stiyanlar\",\n","    \"Singapur Ã§ok gÃ¼zel bir Ã¼lke dir\",\n","    \"DÃ¼n hastaneye gittim sonra da bir gÃ¼zel yemek yedim, eve dÃ¶ndÃ¼m, kÄ±zÄ±mla oyun oynadÄ±m, Amerikan dizi izledim\",\n","    \"hayvanlari Ã§ok seviyorum\"\n","]\n","\n","if model_lang == 'ar':\n","      TWEETS_EXAMPLES = AR_TWEETS_EXAMPLES\n","\n","if model_lang == 'tr':\n","      TWEETS_EXAMPLES = TR_TWEETS_EXAMPLES\n","\n","\n","for tweet in TWEETS_EXAMPLES:\n","    preprocessed_tweet = remove_punctuation_marks(tweet.lower())\n","    sentiment = classify_tweet(preprocessed_tweet)\n","    print(f\"Tweet:    {tweet}\")\n","    print(f\"Category: {sentiment}\")\n","    print(\"-\" * 60)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyN3sOw2EhOYePdlGwgG/6Vq"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}